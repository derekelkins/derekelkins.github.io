<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Hedonistic Learning</title>
    <link href="https://derekelkins.github.io/haskell-atom.xml" rel="self" />
    <link href="https://derekelkins.github.io" />
    <id>https://derekelkins.github.io/haskell-atom.xml</id>
    <author>
        <name>Derek Elkins</name>
        <email>derek.a.elkins+blog@gmail.com</email>
    </author>
    <updated>2024-10-25T00:55:55Z</updated>
    <entry>
    <title>Classical First-Order Logic from the Perspective of Categorical Logic</title>
    <link href="https://derekelkins.github.io/posts/categorical-logic-and-fol.html" />
    <id>https://derekelkins.github.io/posts/categorical-logic-and-fol.html</id>
    <published>2024-10-24 17:55:55-07:00</published>
    <updated>2024-10-25T00:55:55Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Classical First-Order Logic (Classical FOL) has an absolutely central place in traditional
logic, model theory, and set theory. It is the foundation upon which <strong>ZF</strong>(<strong>C</strong>), which is itself
often taken as the foundation of mathematics, is built. When classical FOL was being established
there was a lot of study and debate around alternative options. There are a variety of philosophical
and metatheoretic reasons supporting classical FOL as The Right Choice.</p>
<p>This all happened, however, well before category theory was even a twinkle in Mac Lane’s and Eilenberg’s
eyes, and when type theory was taking its first stumbling steps.</p>
<p>My focus in this article is on what classical FOL looks like to a modern categorical logician.
This can be neatly summarized as “classical FOL is the internal logic of
a <a href="https://ncatlab.org/nlab/show/first-order+hyperdoctrine">Boolean First-Order Hyperdoctrine</a>.
Each of the three words in this term,”Boolean”, “First-Order”, and “Hyperdoctrine”, suggest
a distinct axis in which to vary the (class of categorical models of the) logic. <em>All</em> of them
have compelling categorical motivations to be varied.</p>
<!--more-->
<h2 id="boolean">Boolean</h2>
<p>The first and simplest is the term “Boolean”. This is what differentiates the categorical semantics
of classical (first-order) logic from constructive (first-order) logic. Considering arbitrary
first-order hyperdoctrines would give us a form of intuitionistic first-order logic.</p>
<p>It is fairly rare that the categories categorists are interested in are Boolean. For example, most
toposes, all of which give rise to first-order hyperdoctrines, are not Boolean. The assumption that they
are tends to correspond to a kind of “discreteness” that’s often at odds with the purpose of the
topos. For example, a category of sheaves on a topological space is Boolean if and only if that space is
a <a href="https://en.wikipedia.org/wiki/Stone_space">Stone space</a>. These are certainly interesting spaces,
but they are also totally disconnected unlike virtually every non-discrete topological space one
would typically mention.</p>
<h2 id="first-order">First-Order</h2>
<p>The next term is the term “first-order”. As the name suggests, a first-order hyperdoctrine has the
necessary structure to interpret first-order logic. The question, then, is what kind of categories
have this structure and only this structure. The answer, as far as I’m aware, is not many.</p>
<p>Many (classes of) categories have the structure to be first-order hyperdoctrines, but often they have
additional structure as well that it seems odd to ignore. The most notable and interesting example is
toposes. All elementary toposes (which includes all Grothendieck toposes) have the structure to give
rise to a first-order hyperdoctrine. But, famously, they also have the structure to give rise to a
higher order logic. Even more interesting, while Grothendieck toposes, being elementary toposes, technically
<em>do</em> support the necessary structure for first-order logic, the natural morphisms of Grothendieck
toposes, geometric morphisms, <em>do not preserve that structure</em>, unlike the logical functors between
elementary toposes.</p>
<p>The natural internal logic for Grothendieck toposes turns out to
be <a href="https://ncatlab.org/nlab/show/geometric+theory">geometric logic</a>. This is a logic that lacks
universal quantification and implication (and thus negation) but does have <em>infinitary</em> disjunction.
This leads to a logic that is, at least superficially, incomparable to first-order logic. Closely
related logics are regular logic and coherent logic which are sub-logics of both geometric
logic and first-order logic.</p>
<p>We see, then, just from the examples of the natural logics of toposes, none of them are first-order
logic, and we get examples that are more powerful, less powerful, and incomparable to first-order
logic. Other common classes of categories give other natural logics, such as the cartesian logic
from left exact categories, and monoidal categories give rise to (ordered) linear logics.
We get the simply typed lambda calculus from cartesian closed categories which leads
to the next topic.</p>
<h2 id="hyperdoctrine">Hyperdoctrine</h2>
<p>A (posetal) hyperdoctrine essentially takes a category and, for each object in that category, assigns to it
a poset of “predicates” on that object. In many cases, this takes the form of the <strong>Sub</strong> functor assigning
to each object its poset of subobjects. Various versions of hyperdoctrines will require additional structure
on the source category, these posets, and/or the functor itself to interpret various logical connectives.
For example, a <a href="https://ncatlab.org/nlab/show/regular+hyperdoctrine">regular hyperdoctrine</a> requires the
source category to have finite limits, the posets to be meet-semilattices, and the functor to give rise
to monotonic functions with left adjoints satisfying certain properties. This notion of hyperdoctrines is
suitable for regular logic.</p>
<p>It’s very easy to recognize that these functors are essentially <a href="https://ncatlab.org/nlab/show/indexed+category">indexed</a>
<a href="https://ncatlab.org/nlab/show/%280%2C1%29-category+theory">|(0,1)|-categories</a>. This immediately
suggests that we should consider higher categorical versions or at the very least normal indexed categories.</p>
<p>What this means for the logic is that we move from proof-irrelevant logic to proof-relevant logic. We now
have potentially multiple ways a “predicate” could “entail” another “predicate”. We can
<a href="https://doi.org/10.1017/CBO9780511525902.008">present the simply typed lambda calculus in this indexed category manner</a>.
This naturally leads/connects to the categorical semantics of type theories.</p>
<p>Pushing forward to |(\infty, 1)|-categories is also fairly natural, as it’s natural to want to
talk about an entailment holding for distinct but “equivalent” reasons.</p>
<h2 id="summary">Summary</h2>
<p>Moving in all three of these directions simultaneously leads pretty naturally to something like
Homotopy Type Theory (HoTT). HoTT is a naturally constructive (but not anti-classical) type theory
aimed at being an internal language for |(\infty, 1)|-toposes.</p>
<h2 id="why-classical-fol">Why Classical FOL?</h2>
<p>Okay, so why did people pick classical FOL in the first place? It’s not like the concept of, say, a
higher-order logic wasn’t considered at the time.</p>
<p>Classical versus Intuitionistic was debated at the time, but at that time it was primarily a philosophical
argument, and the defense of Intuitionism was not very compelling (to me and obviously people at the time).
The focus would probably have been more on (classical) FOL versus second- (or higher-)order logic.</p>
<p>Oversimplifying, the issue with second-order logic is fairly evident from the semantics. There are two
main approaches: Henkin-semantics and full (or standard) semantics. Henkin-semantics keeps the nice
properties of (classical) FOL but fails to get the nice properties, namely categoricity properties,
of second-order logic. This isn’t surprising as Henkin-semantics can be encoded into first-order logic.
It’s essentially syntactic sugar. Full semantics, however, states that the interpretation of predicate
sorts is power sets of (cartesian products of) the domain<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. This leads to massive completeness problems as our metalogical set theory has many, many
ways of building subsets of the domain. There are <a href="https://en.wikipedia.org/wiki/Second-order_logic#Metalogical_results">metatheoretic results</a>
that state that there is no computable set of <em>logical</em> axioms that would give us a sound and
complete theory for second-order logic with respect to full semantics. This aspect is also philosophically
problematic, because we don’t want to need set theory to understand the very formulation of set theory.
Thus Quine’s comment that “second-order logic [was] set theory in sheep’s clothing”.</p>
<p>On the more positive and (meta-)mathematical side, we have results like
<a href="https://ncatlab.org/nlab/show/Lindstr%C3%B6m%27s+theorem">Lindström’s theorem</a> which states that
classical FOL is the strongest logic that simultaneously satisfies (downward) Löwenheim-Skolem
and <a href="https://en.wikipedia.org/wiki/Compactness_theorem">compactness</a>. There’s also a syntactic
result by Lindström which characterizes first-order logic as the only logic having a recursively
enumerable set of tautologies and satisfying Löwenheim-Skolem<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<h3 id="the-catch">The Catch</h3>
<p>There’s one big caveat to the above. All of the above results are formulated in traditional model
theory which means there are various assumptions built in to their statements. In the language of
categorical logic, these assumptions can basically be summed up in the statement that the <em>only</em>
category of semantics that traditional model theory considers is <strong>Set</strong>.</p>
<p>This is an utterly bizarre thing to do from the standpoint of categorical logic.</p>
<p>The issues with full semantics follow directly from this choice. If, as categorical logic would
have us do, we considered <em>every</em> category with sufficient structure as a potential category of
semantics, then our theory would not be forced to follow every nook and cranny of <strong>Set</strong>’s
notion of subset to be complete. Valid formulas would need to be true not only in <strong>Set</strong> but
in wildly different categories, e.g. every (Boolean) topos.</p>
<p>These traditional results are also often very specific to <em>classical</em> FOL. Dropping this constraint
of classical logic would lead to an even broader class of models.</p>
<h3 id="categorical-perspective-on-classical-first-order-logic">Categorical Perspective on Classical First-Order Logic</h3>
<p>A <a href="https://ncatlab.org/nlab/show/Boolean+category">Boolean category</a> is just a <a href="https://ncatlab.org/nlab/show/coherent+category">coherent category</a>
where every object has a complement. Since <a href="https://ncatlab.org/nlab/show/coherent+functor">coherent functors</a>
preserve complements, we have that the category of Boolean categories is a <em>full</em> subcategory of the
category of coherent categories.</p>
<p>One nice thing about, specifically, <em>classical</em> first-order logic from the perspective of category
theory is the following. First, <a href="https://ncatlab.org/nlab/show/coherent+logic">coherent logic</a>
is a sub-logic of geometric logic restricted to finitary disjunction. Via <a href="posts/morleyization.html">Morleyization</a>, (TODO: Check link)
we can encode <em>classical</em> first-order logic into coherent logic such that the
categories of models of each are equivalent. This implies that a classical FOL formula is
valid if and only if its encoding is. Morleyization allows us to analyze classical FOL using
the tools of classifying toposes. On the one hand, this once again suggests the importance
of <em>coherent</em> logic, but it also means that we <em>can</em> use categorical tools with <em>classical</em> FOL.</p>
<h2 id="conclusion">Conclusion</h2>
<p>There are certain things that I and, I believe, most logicians take as table stakes for a
(foundational) logic<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.
For example, <em>checking</em> a proof should be computably decidable. For these reasons, I am in
complete accord with early (formal) logicians that classical second-order logic with full semantics
<em>is</em> an unacceptably worse alternative to classical first-order logic.</p>
<p>However, when it comes to statements about the specialness of FOL, a lot of them seem to be more
statements about traditional model theory than FOL itself, and also statements about the philosophical
predilections of the time. I feel that philosophical attitudes among logicians and mathematicians
have shifted a decent amount since the beginning of the 20th century. We have different philosophical
predilections today than then, but they are informed by another hundred years of thought, and they are
more relevant to what is being done today.</p>
<p>Martin-Löf type theory (MLTT) and its progeny also present an alternative path with their own philosophical
and metalogical justifications. I mention this to point out actual cases of foundational frameworks
that a (<em>very</em>) superficial reading of traditional model theory results would seem to have been “ruled
out”. Even if one thinks the FOL+<strong>ZFC</strong> (or whatever) is the better foundations, I think it is unreasonable
to assert that MLTT derivatives are unworkable as a foundations.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>It’s worth mentioning that this is exactly
what categorical logic would suggest: our syntactic power objects should be mapped to semantic power
objects.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>While nice, it’s not clear that
compactness and, especially, Löwenheim-Skolem are sacrosanct properties that we’d be unwilling
to do without. Lindström’s first theorem is thus a nice abstract characterization theorem for
classical FOL, but it doesn’t shut the door on considering alternatives even in the context of
traditional model theory.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>I’m totally fine thinking about logics that lack these properties, but
I would never put any of them forward as an acceptable foundational logic.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Global Rebuilding, Coroutines, and Defunctionalization</title>
    <link href="https://derekelkins.github.io/posts/global-rebuilding-coroutines-and-defunctionalization.html" />
    <id>https://derekelkins.github.io/posts/global-rebuilding-coroutines-and-defunctionalization.html</id>
    <published>2024-10-04 01:24:57-07:00</published>
    <updated>2024-10-04T08:24:57Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>In 1983, Mark Overmars described global rebuilding in <em>The Design of Dynamic Data Structures</em>.
The problem it was aimed at solving was turning the amortized time complexity bounds of
batched rebuilding into worst-case bounds. In <strong>batched rebuilding</strong> we perform a series of
updates to a data structure which may cause the performance of operations to degrade, but
occasionally we expensively rebuild the data structure back into an optimal arrangement.
If the updates don’t degrade performance too much before we rebuild, then we can achieve
our target time complexity bounds in an amortized sense. An update that doesn’t degrade
performance too much is called a <strong>weak update</strong>.</p>
<p>Taking an example from Okasaki’s <em>Purely Functional Data Structures</em>, we can consider a
binary search tree where deletions occur by simply marking the deleted nodes as deleted.
Then, once about half the tree is marked as deleted, we rebuild the tree into a balanced
binary search tree and clean out the nodes marked as deleted at that time. In this case,
the deletions count as weak updates because leaving the deleted nodes in the tree even
when it corresponds to up to half the tree can only mildly impact the time complexity of
other operations. Specifically, assuming the tree was balanced at the start, then deleting
half the nodes could only reduce the tree’s depth by about 1. On the other hand, naive inserts
are <em>not</em> weak updates as they can quickly increase the tree’s depth.</p>
<p>The idea of global rebuilding is relatively straightforward, though how you would actually
realize it in any particular example is not. The overall idea is simply that instead of
waiting until the last moment and then rebuilding the data structure all at once, we’ll start
the rebuild sooner and work at it incrementally as we perform other operations. If we
update the new version faster than we update the original version, we’ll finish it by the
time we would have wanted to perform a batch rebuild, and we can just switch to this new version.</p>
<p>More concretely, though still quite vaguely, <strong>global rebuilding</strong> involves, when a
threshold is reached, rebuilding by creating a new “empty” version of the data
structure called the <em>shadow copy</em>. The original version is the <em>working copy</em>. Work on
rebuilding happens incrementally as operations are performed on the data structure. During
this period, we service queries from the working copy and continue to update it as usual.
Each update needs to make more progress on building the shadow copy than it worsens the
working copy. For example, an insert should insert more nodes into the shadow copy than
the working copy. Once the shadow copy is built, we may still have more work to do to
incorporate changes that occurred after we started the rebuild. To this end, we can
maintain a queue of update operations performed on the working copy since the start of
a rebuild, and then apply these updates, also incrementally, to the shadow copy. Again,
we need to apply the updates from the queue at a fast enough rate so that we will
eventually catch up. Of course, all of this needs to happen fast enough so that 1)
the working copy doesn’t get too degraded before the shadow copy is ready, and 2)
we don’t end up needing to rebuild the shadow copy before it’s ready to do any work.</p>
<!--more-->
<h2 id="coroutines">Coroutines</h2>
<p>Okasaki passingly mentions that global rebuilding “can be usefully viewed as running
the rebuilding transformation as a coroutine”. Also, the situation described above is
quite reminiscent of garbage collection. There the classic half-space stop-the-world
copying collector is naturally the batched rebuilding version. More incremental versions
often have read or write barriers and break the garbage collection into incremental
steps. Garbage collection is also often viewed as two processes coroutining.</p>
<p>The goal of this article is to derive global rebuilding-based data structures from
an expression of them as two coroutining processes. Ideally, we should be able to
take a data structure implemented via batched rebuilding and simply run the batch
rebuilding step as a coroutine. Modifying the data structure’s operations and the
rebuilding step should, in theory, just be a matter of inserting appropriate <code>yield</code>
statements. Of course, it’s won’t be that easy since the batched version of rebuilding
doesn’t need to worry about concurrent updates to the original data structure.</p>
<p>In theory, such a representation would be a perfectly effective way of articulating
the global rebuilding version of the data structure. That said, I will be using the
standard power move of CPS transforming and defunctionalizing to get a more data
structure-like result.</p>
<p>I’ll implement coroutines as a very simplified case of modeling cooperative concurrency with
continuations. In that context, a “process” written in continuation-passing style
“yields” to the scheduler by passing its continuation to a scheduling function.
Normally, the scheduler would place that continuation at the end of a work queue
and then pick up a continuation from the front of the work queue and invoke it
resuming the previously suspended “process”. In our case, we only have two
“processes” so our “work queue” can just be a single mutable cell. When one
“process” yields, it just swaps its continuation into the cell and the other
“process’” out and invokes the continuation it read.</p>
<p>Since the rebuilding process is always driven by the main process, the pattern
is a bit more like generators. This has the benefit that only the rebuilding
process needs to be written in continuation-passing style. The following is
a very quick and dirty set of functions for this.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">module</span> <span class="dt">Coroutine</span> ( <span class="dt">YieldFn</span>, spawn ) <span class="kw">where</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="dt">Control.Monad</span> ( join )</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="dt">Data.IORef</span> ( <span class="dt">IORef</span>, newIORef, readIORef, writeIORef )</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">YieldFn</span> <span class="ot">=</span> <span class="dt">IO</span> () <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="ot">yield ::</span> <span class="dt">IORef</span> (<span class="dt">IO</span> ()) <span class="ot">-&gt;</span> <span class="dt">IO</span> () <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>yield <span class="ot">=</span> writeIORef</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="ot">resume ::</span> <span class="dt">IORef</span> (<span class="dt">IO</span> ()) <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>resume <span class="ot">=</span> join <span class="op">.</span> readIORef</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="ot">terminate ::</span> <span class="dt">IORef</span> (<span class="dt">IO</span> ()) <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>terminate yieldRef <span class="ot">=</span> writeIORef yieldRef (<span class="fu">ioError</span> <span class="op">$</span> <span class="fu">userError</span> <span class="st">&quot;Subprocess completed&quot;</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="ot">spawn ::</span> (<span class="dt">YieldFn</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> () <span class="ot">-&gt;</span> <span class="dt">IO</span> ()) <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">IO</span> ())</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>spawn process <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    yieldRef <span class="ot">&lt;-</span> newIORef <span class="fu">undefined</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    writeIORef yieldRef <span class="op">$</span> process (yield yieldRef) (terminate yieldRef)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span> (resume yieldRef)</span></code></pre></div>
<p>A simple example of usage is:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ot">process ::</span> <span class="dt">YieldFn</span> <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> () <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>process     _ <span class="dv">0</span> k <span class="ot">=</span> k</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>process yield i k <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">putStrLn</span> <span class="op">$</span> <span class="st">&quot;Subprocess: &quot;</span> <span class="op">++</span> <span class="fu">show</span> i</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    yield <span class="op">$</span> process yield (i<span class="op">-</span><span class="dv">1</span>) k</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="ot">example ::</span> <span class="dt">IO</span> ()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>example <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    resume <span class="ot">&lt;-</span> spawn <span class="op">$</span> \yield <span class="ot">-&gt;</span> process yield <span class="dv">10</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    forM_ [(<span class="dv">1</span><span class="ot"> ::</span> <span class="dt">Int</span>) <span class="op">..</span> <span class="dv">10</span>] <span class="op">$</span> \i <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="fu">putStrLn</span> <span class="op">$</span> <span class="st">&quot;Main process: &quot;</span> <span class="op">++</span> <span class="fu">show</span> i</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        resume</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">putStrLn</span> <span class="st">&quot;Main process done&quot;</span></span></code></pre></div>
<p>with output:</p>
<pre><code>Main process: 1
Subprocess: 10
Main process: 2
Subprocess: 9
Main process: 3
Subprocess: 8
Main process: 4
Subprocess: 7
Main process: 5
Subprocess: 6
Main process: 6
Subprocess: 5
Main process: 7
Subprocess: 4
Main process: 8
Subprocess: 3
Main process: 9
Subprocess: 2
Main process: 10
Subprocess: 1
Main process done</code></pre>
<h2 id="queues">Queues</h2>
<p>I’ll use queues since they are very simple and <em>Purely Functional Data Structures</em>
describes Hood-Melville Real-Time Queues in Figure 8.1 as an example of global
rebuilding. We’ll end up with something quite similar which could be made more similar
by changing the rebuilding code. Indeed, the differences are just an artifact of
specific, easily changed details of the rebuilding coroutine, as we’ll see.</p>
<p>The examples I’ll present are mostly imperative, <em>not</em> purely functional. There
are two reasons for this. First, I’m not focused on purely functional data structures
and the technique works fine for imperative data structures. Second, it is arguably
more natural to talk about coroutines in an imperative context. In this case,
it’s easy to adapt the code to a purely functional version since it’s not much
more than a purely functional data structure stuck in an <code>IORef</code>.</p>
<p>For a more imperative structure with mutable linked structure and/or in-place
array updates, it would be more challenging to produce a purely functional
version. The techniques here could still be used, though there are more
“concurrency” concerns. While I don’t include the code here, I did a similar
exercise for a random-access stack (a fancy way of saying a growable array).
There the “concurrency” concern is that the elements you are copying to the
new array may be popped and potentially overwritten before you switch to the
new array. In this case, it’s easy to solve, since if the head pointer of
the live version reaches the source offset for copy, you can just switch to
the new array immediately.</p>
<p>Nevertheless, I can easily imagine scenarios where it may be beneficial, if
not necessary, for the coroutines to communicate more and/or for there to be
multiple “rebuild” processes. The approach used here could be easily adapted
to that. It’s also worth mentioning that even in simpler cases, non-constant-time
operations will either need to invoke <code>resume</code> multiple times or need more
coordination with the “rebuild” process to know when it can do more than a
constant amount of work. This could be accomplished by “rebuild” process
simply recognizing this from the data structure state, or some state could
be explicitly set to indicate this, or the techniques described earlier
could be used, e.g. a different process for non-constant-time operations.</p>
<p>The code below uses the extensions <code>BangPatterns</code>, <code>RecordWildCards</code>, and <code>GADTs</code>.</p>
<h3 id="batched-rebuilding-implementation">Batched Rebuilding Implementation</h3>
<p>We start with the straightforward, amortized constant-time queues where
we push to a stack representing the back of the queue and pop from a stack
representing the front. When the front stack is empty, we need to expensively
reverse the back stack to make a new front stack.</p>
<p>I intentionally separate out the reverse step as an explicit <code>rebuild</code> function.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">module</span> <span class="dt">BatchedRebuildingQueue</span> ( <span class="dt">Queue</span>, new, enqueue, dequeue ) <span class="kw">where</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="dt">Data.IORef</span> ( <span class="dt">IORef</span>, newIORef, readIORef, writeIORef, modifyIORef )</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">Queue</span> a <span class="ot">=</span> <span class="dt">Queue</span> {</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="ot">    queueRef ::</span> <span class="dt">IORef</span> ([a], [a])</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="ot">new ::</span> <span class="dt">IO</span> (<span class="dt">Queue</span> a)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>new <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    queueRef <span class="ot">&lt;-</span> newIORef ([], [])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span> <span class="dt">Queue</span> { <span class="op">..</span> }</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="ot">dequeue ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">Maybe</span> a)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>dequeue q<span class="op">@</span>(<span class="dt">Queue</span> { <span class="op">..</span> }) <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    (front, back) <span class="ot">&lt;-</span> readIORef queueRef</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">case</span> front <span class="kw">of</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        (x<span class="op">:</span>front&#39;) <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            writeIORef queueRef (front&#39;, back)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            <span class="fu">return</span> (<span class="dt">Just</span> x)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        [] <span class="ot">-&gt;</span> <span class="kw">case</span> back <span class="kw">of</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>                [] <span class="ot">-&gt;</span> <span class="fu">return</span> <span class="dt">Nothing</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                _ <span class="ot">-&gt;</span> rebuild q <span class="op">&gt;&gt;</span> dequeue q</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="ot">enqueue ::</span> a <span class="ot">-&gt;</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>enqueue x (<span class="dt">Queue</span> { <span class="op">..</span> }) <span class="ot">=</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    modifyIORef queueRef (\(front, back) <span class="ot">-&gt;</span> (front, x<span class="op">:</span>back))</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="ot">rebuild ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>rebuild (<span class="dt">Queue</span> { <span class="op">..</span> }) <span class="ot">=</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    modifyIORef queueRef (\([], back) <span class="ot">-&gt;</span> (<span class="fu">reverse</span> back, []))</span></code></pre></div>
<h3 id="global-rebuilding-implementation">Global Rebuilding Implementation</h3>
<p>This step is where a modicum of thought is needed. We need to make the
<code>rebuild</code> step from the batched version incremental. This is straightforward,
if tedious, given the coroutine infrastructure. In this case, we incrementalize
the <code>reverse</code> by reimplementing <code>reverse</code> in CPS with some <code>yield</code> calls
inserted. Then we need to incrementalize append. Since we’re not waiting
until <code>front</code> is empty, we’re actually computing <code>front ++ reverse back</code>.
Incrementalizing append is hard, so we actually reverse <code>front</code> and then
use an incremental <code>reverseAppend</code> (which is basically what the incremental
reverse does anyway<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>).</p>
<p>One of first thing to note about this code is that the actual operations are
largely unchanged other than inserting calls to <code>resume</code>. In fact, <code>dequeue</code>
is even simpler than in the batched version as we can just assume that <code>front</code>
is always populated when the queue is not empty. <code>dequeue</code> is freed from the
responsibility of deciding when to trigger a rebuild. Most of the bulk of
this code is from reimplementing a <code>reverseAppend</code> function (twice).</p>
<p>The parts of this code that require some deeper though are 1) knowing when
a rebuild should begin, 2) knowing how “fast” the incremental operations
should go<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>
(e.g. <code>incrementalReverse</code> does two steps at a time and the
Hood-Melville implementation has an explicit <code>exec2</code> that does two steps
at a time), and 3) dealing with “concurrent” changes.</p>
<p>For the last, Overmars describes a queue of deferred operations to perform
on the shadow copy once it finishes rebuilding. This kind of suggests a
situation where the “rebuild” process can reference some “snapshot” of
the data structure. In our case, that is the situation we’re in, since
our data structures are essentially immutable data structures in an <code>IORef</code>.
However, it can easily not be the case, e.g. the random-access stack.
Also, this operation queue approach can easily be inefficient and inelegant.
None of the implementations below will have this queue of deferred operations.
It is easier, more efficient, and more elegant to just not copy over parts of
the queue that have been dequeued, rather than have an extra phase of the
rebuilding that just pops off the elements of the <code>front</code> stack that we just
pushed. A similar situation happens for the random-access stack.</p>
<p>The use of <code>drop</code> could probably be easily eliminated. (I’m not even sure it’s
still necessary.) It is mostly an artifact of (not) dealing with off-by-one issues.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">module</span> <span class="dt">GlobalRebuildingQueue</span> ( <span class="dt">Queue</span>, new, dequeue, enqueue ) <span class="kw">where</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="dt">Data.IORef</span> ( <span class="dt">IORef</span>, newIORef, readIORef, writeIORef, modifyIORef, modifyIORef&#39; )</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="dt">Coroutine</span> ( <span class="dt">YieldFn</span>, spawn )</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">Queue</span> a <span class="ot">=</span> <span class="dt">Queue</span> {</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="ot">    resume ::</span> <span class="dt">IO</span> (),</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="ot">    frontRef ::</span> <span class="dt">IORef</span> [a],</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="ot">    backRef ::</span> <span class="dt">IORef</span> [a],</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="ot">    frontCountRef ::</span> <span class="dt">IORef</span> <span class="dt">Int</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="ot">    backCountRef ::</span> <span class="dt">IORef</span> <span class="dt">Int</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="ot">new ::</span> <span class="dt">IO</span> (<span class="dt">Queue</span> a)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>new <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    frontRef <span class="ot">&lt;-</span> newIORef []</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    backRef <span class="ot">&lt;-</span> newIORef []</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    frontCountRef <span class="ot">&lt;-</span> newIORef <span class="dv">0</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    backCountRef <span class="ot">&lt;-</span> newIORef <span class="dv">0</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    resume <span class="ot">&lt;-</span> spawn <span class="op">$</span> <span class="fu">const</span> <span class="op">.</span> rebuild frontRef backRef frontCountRef backCountRef</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span> <span class="dt">Queue</span> { <span class="op">..</span> }</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="ot">dequeue ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">Maybe</span> a)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>dequeue q <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    resume q</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    front <span class="ot">&lt;-</span> readIORef (frontRef q)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">case</span> front <span class="kw">of</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        [] <span class="ot">-&gt;</span> <span class="fu">return</span> <span class="dt">Nothing</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        (x<span class="op">:</span>front&#39;) <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>            modifyIORef&#39; (frontCountRef q) <span class="fu">pred</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>            writeIORef (frontRef q) front&#39;</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            <span class="fu">return</span> (<span class="dt">Just</span> x)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="ot">enqueue ::</span> a <span class="ot">-&gt;</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>enqueue x q <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    modifyIORef (backRef q) (x<span class="op">:</span>)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    modifyIORef&#39; (backCountRef q) <span class="fu">succ</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    resume q</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="ot">rebuild ::</span> <span class="dt">IORef</span> [a] <span class="ot">-&gt;</span> <span class="dt">IORef</span> [a] <span class="ot">-&gt;</span> <span class="dt">IORef</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">IORef</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">YieldFn</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>rebuild frontRef backRef frontCountRef backCountRef yield <span class="ot">=</span> <span class="kw">let</span> k <span class="ot">=</span> go k <span class="kw">in</span> go k <span class="kw">where</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>  go k <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    frontCount <span class="ot">&lt;-</span> readIORef frontCountRef</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>    backCount <span class="ot">&lt;-</span> readIORef backCountRef</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">if</span> backCount <span class="op">&gt;</span> frontCount <span class="kw">then</span> <span class="kw">do</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        back <span class="ot">&lt;-</span> readIORef backRef</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>        front <span class="ot">&lt;-</span> readIORef frontRef</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        writeIORef backRef []</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>        writeIORef backCountRef <span class="dv">0</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        incrementalReverse back [] <span class="op">$</span> \rback <span class="ot">-&gt;</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>            incrementalReverse front [] <span class="op">$</span> \rfront <span class="ot">-&gt;</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>                incrementalRevAppend rfront rback <span class="dv">0</span> backCount k</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>      <span class="kw">else</span> <span class="kw">do</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>        yield k</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>  incrementalReverse [] acc k <span class="ot">=</span> k acc</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>  incrementalReverse [x] acc k <span class="ot">=</span> k (x<span class="op">:</span>acc)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>  incrementalReverse (x<span class="op">:</span>y<span class="op">:</span>xs) acc k <span class="ot">=</span> yield <span class="op">$</span> incrementalReverse xs (y<span class="op">:</span>x<span class="op">:</span>acc) k</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>  incrementalRevAppend [] front <span class="op">!</span>movedCount backCount&#39; k <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>    writeIORef frontRef front</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>    writeIORef frontCountRef <span class="op">$!</span> movedCount <span class="op">+</span> backCount&#39;</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>    yield k</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>  incrementalRevAppend (x<span class="op">:</span>rfront) acc <span class="op">!</span>movedCount backCount&#39; k <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>    currentFrontCount <span class="ot">&lt;-</span> readIORef frontCountRef</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>    <span class="kw">if</span> currentFrontCount <span class="op">&lt;=</span> movedCount <span class="kw">then</span> <span class="kw">do</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>        <span class="co">-- This drop count should be bounded by a constant.</span></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>        writeIORef frontRef <span class="op">$!</span> <span class="fu">drop</span> (movedCount <span class="op">-</span> currentFrontCount) acc</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>        writeIORef frontCountRef <span class="op">$!</span> currentFrontCount <span class="op">+</span> backCount&#39;</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>        yield k</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>      <span class="kw">else</span> <span class="kw">if</span> <span class="fu">null</span> rfront <span class="kw">then</span></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>        incrementalRevAppend [] (x<span class="op">:</span>acc) (movedCount <span class="op">+</span> <span class="dv">1</span>) backCount&#39; k</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>      <span class="kw">else</span></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>        yield <span class="op">$!</span> incrementalRevAppend rfront (x<span class="op">:</span>acc) (movedCount <span class="op">+</span> <span class="dv">1</span>) backCount&#39; k</span></code></pre></div>
<h3 id="defunctionalized-global-rebuilding-implementation">Defunctionalized Global Rebuilding Implementation</h3>
<p>This step is completely mechanical.</p>
<p>There’s arguably no reason to defunctionalize. It produces a result that
is more data-structure-like, but, unless you need the code to work in a
first-order language, there’s nothing really gained by doing this. It does
lead to a result that is more directly comparable to other implementations.</p>
<p>For some data structures, having the continuation be analyzable would
provide a simple means for the coroutines to communicate. The main process
could directly look at the continuation to determine its state, e.g. if
a rebuild is in-progress at all. The main process could also directly
manipulate the stored continutation to change the “rebuild” process’
behavior. That said, doing this would mean that we’re not <em>deriving</em>
the implementation. Still, the opportunity for additional optimizations
and simplifications is nice.</p>
<p>As a minor aside, while it is, of course, obvious from looking at the
previous version of the code, it’s neat how the <code>Kont</code> data type
implies that the call stack is bounded and that most calls are tail calls.
<code>REVERSE_STEP</code> is the only constructor that contains a <code>Kont</code> argument,
but its type means that that argument can’t itself be a <code>REVERSE_STEP</code>.
Again, I just find it neat how defunctionalization makes this concrete
and explicit.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">module</span> <span class="dt">DefunctionalizedQueue</span> ( <span class="dt">Queue</span>, new, dequeue, enqueue ) <span class="kw">where</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="dt">Data.IORef</span> ( <span class="dt">IORef</span>, newIORef, readIORef, writeIORef, modifyIORef, modifyIORef&#39; )</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">Kont</span> a r <span class="kw">where</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">IDLE</span><span class="ot"> ::</span> <span class="dt">Kont</span> a ()</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">REVERSE_STEP</span><span class="ot"> ::</span> [a] <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="dt">Kont</span> a [a] <span class="ot">-&gt;</span> <span class="dt">Kont</span> a ()</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">REVERSE_FRONT</span><span class="ot"> ::</span> [a] <span class="ot">-&gt;</span> <span class="op">!</span><span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Kont</span> a [a]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">REV_APPEND_START</span><span class="ot"> ::</span> [a] <span class="ot">-&gt;</span> <span class="op">!</span><span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Kont</span> a [a]</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">REV_APPEND_STEP</span><span class="ot"> ::</span> [a] <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="op">!</span><span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="op">!</span><span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Kont</span> a ()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="ot">applyKont ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">Kont</span> a r <span class="ot">-&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>applyKont q <span class="dt">IDLE</span> _ <span class="ot">=</span> rebuildLoop q</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>applyKont q (<span class="dt">REVERSE_STEP</span> xs acc k) _ <span class="ot">=</span> incrementalReverse q xs acc k</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>applyKont q (<span class="dt">REVERSE_FRONT</span> front backCount) rback <span class="ot">=</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    incrementalReverse q front [] <span class="op">$</span> <span class="dt">REV_APPEND_START</span> rback backCount</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>applyKont q (<span class="dt">REV_APPEND_START</span> rback backCount) rfront <span class="ot">=</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    incrementalRevAppend q rfront rback <span class="dv">0</span> backCount</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>applyKont q (<span class="dt">REV_APPEND_STEP</span> rfront acc movedCount backCount) _ <span class="ot">=</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    incrementalRevAppend q rfront acc movedCount backCount</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="ot">rebuildLoop ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>rebuildLoop q<span class="op">@</span>(<span class="dt">Queue</span> { <span class="op">..</span> }) <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    frontCount <span class="ot">&lt;-</span> readIORef frontCountRef</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    backCount <span class="ot">&lt;-</span> readIORef backCountRef</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">if</span> backCount <span class="op">&gt;</span> frontCount <span class="kw">then</span> <span class="kw">do</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        back <span class="ot">&lt;-</span> readIORef backRef</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        front <span class="ot">&lt;-</span> readIORef frontRef</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>        writeIORef backRef []</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        writeIORef backCountRef <span class="dv">0</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        incrementalReverse q back [] <span class="op">$</span> <span class="dt">REVERSE_FRONT</span> front backCount</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>      <span class="kw">else</span> <span class="kw">do</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>        writeIORef resumeRef <span class="dt">IDLE</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="ot">incrementalReverse ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="dt">Kont</span> a [a] <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>incrementalReverse q [] acc k <span class="ot">=</span> applyKont q k acc</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>incrementalReverse q [x] acc k <span class="ot">=</span> applyKont q k (x<span class="op">:</span>acc)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>incrementalReverse q (x<span class="op">:</span>y<span class="op">:</span>xs) acc k <span class="ot">=</span> writeIORef (resumeRef q) <span class="op">$</span> <span class="dt">REVERSE_STEP</span> xs (y<span class="op">:</span>x<span class="op">:</span>acc) k</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="ot">incrementalRevAppend ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>incrementalRevAppend (<span class="dt">Queue</span> { <span class="op">..</span> }) [] front <span class="op">!</span>movedCount backCount&#39; <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    writeIORef frontRef front</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    writeIORef frontCountRef <span class="op">$!</span> movedCount <span class="op">+</span> backCount&#39;</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    writeIORef resumeRef <span class="dt">IDLE</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>incrementalRevAppend q<span class="op">@</span>(<span class="dt">Queue</span> { <span class="op">..</span> }) (x<span class="op">:</span>rfront) acc <span class="op">!</span>movedCount backCount&#39; <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    currentFrontCount <span class="ot">&lt;-</span> readIORef frontCountRef</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">if</span> currentFrontCount <span class="op">&lt;=</span> movedCount <span class="kw">then</span> <span class="kw">do</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>        <span class="co">-- This drop count should be bounded by a constant.</span></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>        writeIORef frontRef <span class="op">$!</span> <span class="fu">drop</span> (movedCount <span class="op">-</span> currentFrontCount) acc</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>        writeIORef frontCountRef <span class="op">$!</span> currentFrontCount <span class="op">+</span> backCount&#39;</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>        writeIORef resumeRef <span class="dt">IDLE</span></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>      <span class="kw">else</span> <span class="kw">if</span> <span class="fu">null</span> rfront <span class="kw">then</span></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>        incrementalRevAppend q [] (x<span class="op">:</span>acc) (movedCount <span class="op">+</span> <span class="dv">1</span>) backCount&#39;</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>      <span class="kw">else</span></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>        writeIORef resumeRef <span class="op">$!</span> <span class="dt">REV_APPEND_STEP</span> rfront (x<span class="op">:</span>acc) (movedCount <span class="op">+</span> <span class="dv">1</span>) backCount&#39;</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a><span class="ot">resume ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>resume q <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>    kont <span class="ot">&lt;-</span> readIORef (resumeRef q)</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>    applyKont q kont ()</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">Queue</span> a <span class="ot">=</span> <span class="dt">Queue</span> {</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a><span class="ot">    resumeRef ::</span> <span class="dt">IORef</span> (<span class="dt">Kont</span> a ()),</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a><span class="ot">    frontRef ::</span> <span class="dt">IORef</span> [a],</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a><span class="ot">    backRef ::</span> <span class="dt">IORef</span> [a],</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a><span class="ot">    frontCountRef ::</span> <span class="dt">IORef</span> <span class="dt">Int</span>,</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a><span class="ot">    backCountRef ::</span> <span class="dt">IORef</span> <span class="dt">Int</span></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a><span class="ot">new ::</span> <span class="dt">IO</span> (<span class="dt">Queue</span> a)</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>new <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>    frontRef <span class="ot">&lt;-</span> newIORef []</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>    backRef <span class="ot">&lt;-</span> newIORef []</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>    frontCountRef <span class="ot">&lt;-</span> newIORef <span class="dv">0</span></span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>    backCountRef <span class="ot">&lt;-</span> newIORef <span class="dv">0</span></span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>    resumeRef <span class="ot">&lt;-</span> newIORef <span class="dt">IDLE</span></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span> <span class="dt">Queue</span> { <span class="op">..</span> }</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a><span class="ot">dequeue ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">Maybe</span> a)</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>dequeue q  <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>    resume q</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>    front <span class="ot">&lt;-</span> readIORef (frontRef q)</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>    <span class="kw">case</span> front <span class="kw">of</span></span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>        [] <span class="ot">-&gt;</span> <span class="fu">return</span> <span class="dt">Nothing</span></span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>        (x<span class="op">:</span>front&#39;) <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>            modifyIORef&#39; (frontCountRef q) <span class="fu">pred</span></span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>            writeIORef (frontRef q) front&#39;</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>            <span class="fu">return</span> (<span class="dt">Just</span> x)</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a><span class="ot">enqueue ::</span> a <span class="ot">-&gt;</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>enqueue x q <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a>    modifyIORef (backRef q) (x<span class="op">:</span>)</span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>    modifyIORef&#39; (backCountRef q) <span class="fu">succ</span></span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>    resume q</span></code></pre></div>
<h3 id="functional-defunctionalized-global-rebuilding-implementation">Functional Defunctionalized Global Rebuilding Implementation</h3>
<p>This is just a straightforward reorganization of the previous code into purely
functional code. This produces a persistent queue with worst-case constant
time operations.</p>
<p>It is, of course, far uglier and more ad-hoc than Okasaki’s
extremely elegant real-time queues, but the methodology to derive it was
simple-minded. The result is also quite similar to the <a href="#hood-melville-implementation">Hood-Melville Queues</a>
even though I did not set out to achieve that. That said, I’m pretty
confident you could derive pretty much <em>exactly</em> the Hood-Melville queues
with just minor modifications to <a href="#global-rebuilding-implementation">Global Rebuilding Implementation</a>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">module</span> <span class="dt">FunctionalQueue</span> ( <span class="dt">Queue</span>, empty, dequeue, enqueue ) <span class="kw">where</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">Kont</span> a r <span class="kw">where</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">IDLE</span><span class="ot"> ::</span> <span class="dt">Kont</span> a ()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">REVERSE_STEP</span><span class="ot"> ::</span> [a] <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="dt">Kont</span> a [a] <span class="ot">-&gt;</span> <span class="dt">Kont</span> a ()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">REVERSE_FRONT</span><span class="ot"> ::</span> [a] <span class="ot">-&gt;</span> <span class="op">!</span><span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Kont</span> a [a]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">REV_APPEND_START</span><span class="ot"> ::</span> [a] <span class="ot">-&gt;</span> <span class="op">!</span><span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Kont</span> a [a]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">REV_APPEND_STEP</span><span class="ot"> ::</span> [a] <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="op">!</span><span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="op">!</span><span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Kont</span> a ()</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="ot">applyKont ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">Kont</span> a r <span class="ot">-&gt;</span> r <span class="ot">-&gt;</span> <span class="dt">Queue</span> a</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>applyKont q <span class="dt">IDLE</span> _ <span class="ot">=</span> rebuildLoop q</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>applyKont q (<span class="dt">REVERSE_STEP</span> xs acc k) _ <span class="ot">=</span> incrementalReverse q xs acc k</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>applyKont q (<span class="dt">REVERSE_FRONT</span> front backCount) rback <span class="ot">=</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    incrementalReverse q front [] <span class="op">$</span> <span class="dt">REV_APPEND_START</span> rback backCount</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>applyKont q (<span class="dt">REV_APPEND_START</span> rback backCount) rfront <span class="ot">=</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    incrementalRevAppend q rfront rback <span class="dv">0</span> backCount</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>applyKont q (<span class="dt">REV_APPEND_STEP</span> rfront acc movedCount backCount) _ <span class="ot">=</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    incrementalRevAppend q rfront acc movedCount backCount</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="ot">rebuildLoop ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">Queue</span> a</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>rebuildLoop q<span class="op">@</span>(<span class="dt">Queue</span> { <span class="op">..</span> }) <span class="ot">=</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">if</span> backCount <span class="op">&gt;</span> frontCount <span class="kw">then</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> q&#39; <span class="ot">=</span> q { back <span class="ot">=</span> [], backCount <span class="ot">=</span> <span class="dv">0</span> } <span class="kw">in</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        incrementalReverse q&#39; back [] <span class="op">$</span> <span class="dt">REVERSE_FRONT</span> front backCount</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>      <span class="kw">else</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        q { resumeKont <span class="ot">=</span> <span class="dt">IDLE</span> }</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="ot">incrementalReverse ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="dt">Kont</span> a [a] <span class="ot">-&gt;</span> <span class="dt">Queue</span> a</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>incrementalReverse q [] acc k <span class="ot">=</span> applyKont q k acc</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>incrementalReverse q [x] acc k <span class="ot">=</span> applyKont q k (x<span class="op">:</span>acc)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>incrementalReverse q (x<span class="op">:</span>y<span class="op">:</span>xs) acc k <span class="ot">=</span> q { resumeKont <span class="ot">=</span> <span class="dt">REVERSE_STEP</span> xs (y<span class="op">:</span>x<span class="op">:</span>acc) k }</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a><span class="ot">incrementalRevAppend ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Queue</span> a</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>incrementalRevAppend q [] front&#39; <span class="op">!</span>movedCount backCount&#39; <span class="ot">=</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    q { front <span class="ot">=</span> front&#39;, frontCount <span class="ot">=</span> movedCount <span class="op">+</span> backCount&#39;, resumeKont <span class="ot">=</span> <span class="dt">IDLE</span> }</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>incrementalRevAppend q (x<span class="op">:</span>rfront) acc <span class="op">!</span>movedCount backCount&#39; <span class="ot">=</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">if</span> frontCount q <span class="op">&lt;=</span> movedCount <span class="kw">then</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">-- This drop count should be bounded by a constant.</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> <span class="op">!</span>front <span class="ot">=</span> <span class="fu">drop</span> (movedCount <span class="op">-</span> frontCount q) acc <span class="kw">in</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        q { front <span class="ot">=</span> front, frontCount <span class="ot">=</span> frontCount q <span class="op">+</span> backCount&#39;, resumeKont <span class="ot">=</span> <span class="dt">IDLE</span> }</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>      <span class="kw">else</span> <span class="kw">if</span> <span class="fu">null</span> rfront <span class="kw">then</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        incrementalRevAppend q [] (x<span class="op">:</span>acc) (movedCount <span class="op">+</span> <span class="dv">1</span>) backCount&#39;</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>      <span class="kw">else</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>        q { resumeKont <span class="ot">=</span> <span class="dt">REV_APPEND_STEP</span> rfront (x<span class="op">:</span>acc) (movedCount <span class="op">+</span> <span class="dv">1</span>) backCount&#39; }</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="ot">resume ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">Queue</span> a</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>resume q <span class="ot">=</span> applyKont q (resumeKont q) ()</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">Queue</span> a <span class="ot">=</span> <span class="dt">Queue</span> {</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a><span class="ot">    resumeKont ::</span> <span class="op">!</span>(<span class="dt">Kont</span> a ()),</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a><span class="ot">    front ::</span> [a],</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a><span class="ot">    back ::</span> [a],</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a><span class="ot">    frontCount ::</span> <span class="op">!</span><span class="dt">Int</span>,</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a><span class="ot">    backCount ::</span> <span class="op">!</span><span class="dt">Int</span></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a><span class="ot">empty ::</span> <span class="dt">Queue</span> a</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>empty <span class="ot">=</span> <span class="dt">Queue</span> { resumeKont <span class="ot">=</span> <span class="dt">IDLE</span>, front <span class="ot">=</span> [], back <span class="ot">=</span> [], frontCount <span class="ot">=</span> <span class="dv">0</span>, backCount <span class="ot">=</span> <span class="dv">0</span> }</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="ot">dequeue ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> (<span class="dt">Maybe</span> a, <span class="dt">Queue</span> a)</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>dequeue q <span class="ot">=</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>    <span class="kw">case</span> front <span class="kw">of</span></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>        [] <span class="ot">-&gt;</span> (<span class="dt">Nothing</span>, q)</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>        (x<span class="op">:</span>front&#39;) <span class="ot">-&gt;</span></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>            (<span class="dt">Just</span> x, q&#39; { front <span class="ot">=</span> front&#39;, frontCount <span class="ot">=</span> frontCount <span class="op">-</span> <span class="dv">1</span> })</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span> q&#39;<span class="op">@</span>(<span class="dt">Queue</span> { <span class="op">..</span> }) <span class="ot">=</span> resume q</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a><span class="ot">enqueue ::</span> a <span class="ot">-&gt;</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">Queue</span> a</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>enqueue x q<span class="op">@</span>(<span class="dt">Queue</span> { <span class="op">..</span> }) <span class="ot">=</span> resume (q { back <span class="ot">=</span> x<span class="op">:</span>back, backCount <span class="ot">=</span> backCount <span class="op">+</span> <span class="dv">1</span> })</span></code></pre></div>
<h3 id="hood-melville-implementation">Hood-Melville Implementation</h3>
<p>This is just the Haskell code from <em>Purely Functional Data Structures</em> adapted
to the interface of the other examples.</p>
<p>This code is mostly to compare. The biggest difference, other than some code
structuring differences, is the front and back lists are reversed in parallel
while my code does them sequentially. As mentioned before, to get a structure
like that would simply be a matter of defining a parallel incremental reverse
back in the <a href="#global-rebuilding-implementation">Global Rebuilding Implementation</a>.</p>
<p>Again, Okasaki’s real-time queue that can be seen as an application of the
<em>lazy</em> rebuilding and scheduling techniques, described in his thesis and book,
is a better implementation than this in pretty much every way.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">module</span> <span class="dt">HoodMelvilleQueue</span> (<span class="dt">Queue</span>, empty, dequeue, enqueue) <span class="kw">where</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">RotationState</span> a</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="ot">=</span> <span class="dt">Idle</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="op">|</span> <span class="dt">Reversing</span> <span class="op">!</span><span class="dt">Int</span> [a] [a] [a] [a]</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">|</span> <span class="dt">Appending</span> <span class="op">!</span><span class="dt">Int</span> [a] [a]</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="op">|</span> <span class="dt">Done</span> [a]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">Queue</span> a <span class="ot">=</span> <span class="dt">Queue</span> <span class="op">!</span><span class="dt">Int</span> [a] (<span class="dt">RotationState</span> a) <span class="op">!</span><span class="dt">Int</span> [a]</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="ot">exec ::</span> <span class="dt">RotationState</span> a <span class="ot">-&gt;</span> <span class="dt">RotationState</span> a</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>exec (<span class="dt">Reversing</span> ok (x<span class="op">:</span>f) f&#39; (y<span class="op">:</span>r) r&#39;) <span class="ot">=</span> <span class="dt">Reversing</span> (ok<span class="op">+</span><span class="dv">1</span>) f (x<span class="op">:</span>f&#39;) r (y<span class="op">:</span>r&#39;)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>exec (<span class="dt">Reversing</span> ok [] f&#39; [y] r&#39;) <span class="ot">=</span> <span class="dt">Appending</span> ok f&#39; (y<span class="op">:</span>r&#39;)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>exec (<span class="dt">Appending</span> <span class="dv">0</span> f&#39; r&#39;) <span class="ot">=</span> <span class="dt">Done</span> r&#39;</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>exec (<span class="dt">Appending</span> ok (x<span class="op">:</span>f&#39;) r&#39;) <span class="ot">=</span> <span class="dt">Appending</span> (ok<span class="op">-</span><span class="dv">1</span>) f&#39; (x<span class="op">:</span>r&#39;)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>exec state <span class="ot">=</span> state</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="ot">invalidate ::</span> <span class="dt">RotationState</span> a <span class="ot">-&gt;</span> <span class="dt">RotationState</span> a</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>invalidate (<span class="dt">Reversing</span> ok f f&#39; r r&#39;) <span class="ot">=</span> <span class="dt">Reversing</span> (ok<span class="op">-</span><span class="dv">1</span>) f f&#39; r r&#39;</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>invalidate (<span class="dt">Appending</span> <span class="dv">0</span> f&#39; (x<span class="op">:</span>r&#39;)) <span class="ot">=</span> <span class="dt">Done</span> r&#39;</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>invalidate (<span class="dt">Appending</span> ok f&#39; r&#39;) <span class="ot">=</span> <span class="dt">Appending</span> (ok<span class="op">-</span><span class="dv">1</span>) f&#39; r&#39;</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>invalidate state <span class="ot">=</span> state</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="ot">exec2 ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="dt">RotationState</span> a <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="dt">Queue</span> a</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>exec2 <span class="op">!</span>lenf f state lenr r <span class="ot">=</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">case</span> exec (exec state) <span class="kw">of</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        <span class="dt">Done</span> newf <span class="ot">-&gt;</span> <span class="dt">Queue</span> lenf newf <span class="dt">Idle</span> lenr r</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        newstate <span class="ot">-&gt;</span> <span class="dt">Queue</span> lenf f newstate lenr r</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="ot">check ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="dt">RotationState</span> a <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="dt">Queue</span> a</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>check <span class="op">!</span>lenf f state <span class="op">!</span>lenr r <span class="ot">=</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">if</span> lenr <span class="op">&lt;=</span> lenf <span class="kw">then</span> exec2 lenf f state lenr r</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">else</span> <span class="kw">let</span> newstate <span class="ot">=</span> <span class="dt">Reversing</span> <span class="dv">0</span> f [] r []</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>         <span class="kw">in</span> exec2 (lenf<span class="op">+</span>lenr) f newstate <span class="dv">0</span> []</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="ot">empty ::</span> <span class="dt">Queue</span> a</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>empty <span class="ot">=</span> <span class="dt">Queue</span> <span class="dv">0</span> [] <span class="dt">Idle</span> <span class="dv">0</span> []</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a><span class="ot">dequeue ::</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> (<span class="dt">Maybe</span> a, <span class="dt">Queue</span> a)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>dequeue q<span class="op">@</span>(<span class="dt">Queue</span> _ [] _ _ _) <span class="ot">=</span> (<span class="dt">Nothing</span>, q)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>dequeue (<span class="dt">Queue</span> lenf (x<span class="op">:</span>f&#39;) state lenr r) <span class="ot">=</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> <span class="op">!</span>q&#39; <span class="ot">=</span> check (lenf<span class="op">-</span><span class="dv">1</span>) f&#39; (invalidate state) lenr r <span class="kw">in</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    (<span class="dt">Just</span> x, q&#39;)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a><span class="ot">enqueue ::</span> a <span class="ot">-&gt;</span> <span class="dt">Queue</span> a <span class="ot">-&gt;</span> <span class="dt">Queue</span> a</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>enqueue x (<span class="dt">Queue</span> lenf f state lenr r) <span class="ot">=</span> check lenf f state (lenr<span class="op">+</span><span class="dv">1</span>) (x<span class="op">:</span>r)</span></code></pre></div>
<h3 id="empirical-evaluation">Empirical Evaluation</h3>
<p>I won’t reproduce the evaluation code as it’s not very sophisticated or interesting.
It randomly generated a sequence of enqueues and dequeues with an 80% chance to produce
an enqueue over a dequeue so that the queues would grow. It measured the average
time of an enqueue and a dequeue, as well as the maximum time of any single dequeue.</p>
<p>The main thing I wanted to see was relatively stable average enqueue and dequeue
times with only the batched implementation having a growing maximum dequeue time.
This is indeed what I saw, though it took about 1,000,000 operations (or really
a queue of a couple hundred thousand elements) for the numbers to stabilize.</p>
<p>The results were mostly unsurprising. Unsurprisingly, in overall time, the batched
implementation won. Its <code>enqueue</code> is also, obviously, the fastest. (Indeed, there’s
a good chance my measurement of its average enqueue time was largely a measurement
of the timer’s resolution.) The operations’ average times were stable illustrating their
constant (amortized) time. At large enough sizes, the ratio of the maximum dequeue
time versus the average stabilized around 7000 to 1, except, of course, for the
batched version which grew linearly to millions to 1 ratios at queue sizes of tens
of millions of elements. This illustrates the worst-case time complexity of all the
other implementations, and the merely amortized time complexity of the batched one.</p>
<p>While the batched version was best in overall time, the difference wasn’t that great.
The worst implementations were still less 1.4x slower. All the worst-case optimal
implementations performed roughly the same, but there were still some clear winners
and losers. Okasaki’s real-time queue (not listed) is almost on-par with the batched
implementation in overall time and handily beats the other implementations in average
enqueue and dequeue times. The main surprise for me was that the loser was the
Hood-Melville queue. My guess is this is due to <code>invalidate</code> which seems like it
would do more work and produce more garbage than the approach taken in my functional
version.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The point of this article was to illustrate the process of deriving a deamortized
data structure from an amortized one utilizing batch rebuilding by explicitly
modeling global rebuilding as a coroutine.</p>
<p>The point wasn’t to produce the fastest queue implementation, though I am pretty
happy with the results. While this is an extremely simple example, it was still
nice that each step was very easy and natural. It’s especially nice that this
derivation approach produced a better result than the Hood-Melville queue.</p>
<p>Of course, my advice is to use Okasaki’s real-time queue if you need a purely functional queue.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This code could definitely be refactored to leverage
this similarity to reduce code. Alternatively, one could refunctionalize
the <a href="#hood-melville-implementation">Hood-Melville implementation</a> at the end.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Going “too fast”, so long as it’s still a constant amount of
work for each step, isn’t really an issue asymptotically, so you can just
crank the knobs if you don’t want to think too hard about it. That said,
going faster than you need to will likely give you worse worst-case
constant factors. In some cases, going faster than necessary could reduce
constant factors, e.g. by better utilizing caches and disk I/O buffers.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Morleyization</title>
    <link href="https://derekelkins.github.io/posts/morleyization.html" />
    <id>https://derekelkins.github.io/posts/morleyization.html</id>
    <published>2024-07-18 19:35:18-07:00</published>
    <updated>2024-07-19T02:35:18Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Morleyization is a fairly important operation in categorical logic for which it is hard to find readily
accessible references to a statement and proof. Most refer to D1.5.13 of “Sketches of an Elephant” which is
not an accessible text. 3.2.8 of “Accessible Categories” by Makkai and Paré is another reference, and
“Accessible Categories” is more accessible but still a big ask for just a single theorem.</p>
<p>Here I reproduce the statement and proof from “Accessible Categories” albeit with some notational and
conceptual adaptations as well as some commentary. This assumes some basic familiarity with the ideas
and notions of traditional model theory, e.g. what structures, models, and |\vDash| are.</p>
<!--more-->
<h2 id="preliminaries">Preliminaries</h2>
<p>The context of the theorem is <a href="https://en.wikipedia.org/wiki/Infinitary_logic">infinitary, classical (multi-sorted) first-order logic</a>.
|L| will stand for a <strong>language</strong> aka a <strong>signature</strong>, i.e. sorts, function symbols, predicate symbols as usual,
except if we’re allowing infinitary quantification we may have function or predicate symbols of infinite
arity. We write |L_{\kappa,\lambda}| for the corresponding classical first-order logic where we
allow conjunctions and disjunctions indexed by sets of cardinality less than the regular (infinite) cardinal
|\kappa| while allowing quantification over sets of variables of (infinite) cardinality less than
|\lambda \leq \kappa|. |\lambda=\varnothing| is also allowed to indicate a propositional logic.
If |\kappa| or |\lambda| are |\infty|, that means conjunctions/disjunctions or quantifications over
arbitrary sets. |L_{\omega,\omega}| would be normal finitary, classical first-order logic. Geometric
logic would be a fragment of |L_{\infty,\omega}|. The theorem will focus on |L_{\infty,\infty}|,
but inspection of the proof shows that theorem would hold for any reasonable choice for |\kappa|
and |\lambda|.</p>
<p>As a note, infinitary logics can easily have a <em>proper class</em> of formulas. Thus, it will make sense
to talk about <em>small</em> subclasses of formulas, i.e. ones which are sets.</p>
<p>Instead of considering logics with different sets of connectives Makkai and Paré, introduces the
fairly standard notion of a <strong>positive existential</strong> formula which is a formula that uses only
atomic formulas, conjunctions, disjunctions, and existential quantification. That is, no implication,
negation, or universal quantification. They then define a <strong>basic sentence</strong> as “a conjunction of
a set of sentences, i.e. closed formulas, each of which is of the form |\forall\vec x(\phi\to\psi)|
where |\phi| and |\psi| are [positive existential] formulas”.</p>
<p>It’s clear the component formulas of a basic sentences correspond to sequents of the form
|\phi\vdash\psi| for open positive existential formulas. A basic sentence corresponds to what
is often called a theory, i.e. a set of sequents. Infinitary logic lets us smash a theory down
to a single formula, but I think the theory concept is clearer though I’m sure there are benefits
to having a single formula. Instead of talking about basic sentences, we can talk about a theory
in the positive existential fragment of the relevant logic. This has the benefit that we don’t
need to introduce connectives or infinitary versions of connectives just for structural reasons.
I’ll call a theory that corresponds to a basic sentence a <strong>positive existential theory</strong> for
conciseness.</p>
<p>Makkai and Paré also define |L_{\kappa,\lambda}^*| “for the class of formulas |L_{\kappa,\lambda}|
which are conjunctions of formulas in each of which the only conjunctions occurring are
of cardinality |&lt; \lambda|”. For us, the main significance of this is that geometric theories
correspond to basic sentences in |L_{\infty,\omega}^*| as this limits the conjunctions to the
finitary case. Indeed, Makkai and Paré include the somewhat awkward sentence: “Thus, a <em>geometric
theory</em> is the same as a basic sentence in |L_{\infty,\omega}^*|, and a <em>coherent theory</em> is
a conjunction of basic sentences in |L_{\omega,\omega}|.” Presumably, the ambiguous meaning of
“conjunction” leads to the differences in how these are stated, i.e. a basic sentence is already
a “conjunction” of formulas.</p>
<p>The standard notion of an |L|-structure and model are used, and I won’t give a precise definition
here. An <strong>|L|-structure</strong> assigns meaning (sets, functions, and relations) to all the sorts and
symbols of |L|, and a model of a formula (or theory) is an |L|-structure which satisfies the
formula (or all the formulas of the theory). We’ll write |Str(L)| for the category of |L|-structures
and homomorphisms. In categorical logic, an |L|-structure would usually
be some kind of structure preserving (fibred) functor usually into |\mathbf{Set}|, and a homomorphism
is a natural transformation. A formula would be mapped to a subobject, and a model would require
these subobjects to satisfy certain factoring properties specified by the theory. A sequent
|\varphi \vdash \psi| in the theory would require a model to have the interpretation of
|\varphi| factor through the interpretation of |\psi|, i.e. for the former to be a subset
of the latter when interpreting into |\mathbf{Set}|.</p>
<h2 id="theorem-statement">Theorem Statement</h2>
<p>|\mathcal F \subseteq L_{\infty,\infty}| is called a <strong>fragment of |L_{\infty,\infty}|</strong> if:</p>
<ol type="i">
<li>it contains all atomic formulas of |L|,</li>
<li>it is closed under substitution,</li>
<li>if a formula is in |\mathcal F| then so are all its subformulas,</li>
<li>if |\forall\vec x\varphi \in \mathcal F|, then so is |\neg\exists\vec x\neg\varphi|, and</li>
<li>if |\varphi\to\psi \in \mathcal F|, then so is |\neg\varphi\lor\psi|.</li>
</ol>
<p>Basically, and the motivation for this will become clear shortly, formulas in |\mathcal F| are
like “compound atomic formulas” with the caveat that we must include the classically equivalent
versions of |\forall| and |\to| in terms of |\neg| and |\exists| or |\lor| respectively.</p>
<p>Given |\mathcal F|, we define an <strong>|\mathcal F|-basic sentence</strong> exactly like a basic sentence
except that we allow formulas from |\mathcal F| instead of just atomic formulas as the base
case. In theory language, an |\mathcal F|-basic sentence is a theory, i.e. set of sequents,
using only the connectives |\bigwedge|, |\bigvee|, and |\exists|, except within subformulas
contained in |\mathcal F| which may use any (first-order) connective. We’ll call such a
theory a <strong>positive existential |\mathcal F|-theory</strong>. Much of the following will be double-barrelled
as I try to capture the proof as stated in “Accessible Categories” and my slight reformulation
using positive existential theories.</p>
<p>|\mathrm{Mod}^{(\mathcal F)}(\mathbb T)| for a theory |\mathbb T| (or
|\mathrm{Mod}^{(\mathcal F)}(\sigma)| for a basic sentence |\sigma|) is the category
whose objects are |L|-structures that are models of |\mathbb T| (or |\sigma|), and whose arrows are the
|\mathcal F|-elementary mappings. An <strong>|\mathcal F|-elementary mapping</strong> |h : M \to N|,
for <em>any</em> subset of formulas of |L_{\infty,\infty}|, |\mathcal F|, is a mapping of |L|-structures
which preserves the meaning of all formulas in |\mathcal F|.
That is, |M \vDash \varphi(\vec a)| implies |N \vDash \varphi(h(\vec a))| for all
formulas, |\varphi \in \mathcal F| and appropriate sequences |\vec a|. We can define
the <strong>elementary mappings for a language |L’|</strong> as the |\mathcal F’|-elementary mappings
where |\mathcal F’| consists of (only) the atomic formulas of |L’|. |\mathrm{Mod}^{(L’)}(\mathbb T’)|
(or |\mathrm{Mod}^{(L’)}(\sigma’)|) can be defined by |\mathrm{Mod}^{(\mathcal F’)}(\mathbb T’)|
(or |\mathrm{Mod}^{(L’)}(\sigma’)|) for the |\mathcal F’| determined this way.</p>
<p>Here’s the theorem as stated in “Accessible Categories”.</p>
<blockquote>
<p><strong>Theorem</strong> (Proposition 3.2.8): Given any small fragment |\mathcal F| and an |\mathcal F|-basic
sentence |\sigma|, the category of |\mathrm{Mod}^{(\mathcal F)}(\sigma)| is equivalent to
|\mathrm{Mod}^{(L’)}(\sigma’)| for some other language |L’| and basic sentence |\sigma’| over
|L’|, hence by 3.2.1, to the category of models of a small sketch as well.</p>
</blockquote>
<p>We’ll replace the |\mathcal F|-basic sentences |\sigma| and |\sigma’| with positive existential
|\mathcal F|-theories |\mathbb T| and |\mathbb T’|.</p>
<p>Implied is that |\mathcal F \subseteq L_{\infty,\infty}|, i.e. that |L| and |L’| may be distinct
and usually will be. As the proof will show, they agree on sorts and function symbols, but we have
different predicate symbols in |L’|.</p>
<p>I’ll be ignoring the final comment referencing Theorem 3.2.1. Theorem 3.2.1 is the main theorem of
the section and states that every small sketch gives rise to a language |L| and theory |\mathbb T|
(or basic sentence |\sigma|) and vice versa such that the category of models of the sketch are
equivalent to models of |\mathbb T| (or |\sigma|). Thus, the final comment is an immediate corollary.</p>
<p>For us, the interesting part of 3.2.8 is that it takes a classical first-order theory, |\mathbb T|,
and produces a positive existential theory, as represented by |\mathbb T’|, that has
an equivalent, in fact isomorphic, category of models. This positive existential theory is called
the <strong>Morleyization</strong> of the first-order theory.</p>
<p>In particular, if we have a <em>finitary</em> classical first-order theory, then we get a coherent theory
with the same models. This means to study models of classical first-order theories, it’s enough
to study models of coherent theories via the Morleyization of the classical first-order theories.
This allows many techniques for geometric and coherent theories to be applied, e.g. (pre)topos theory
and classifying toposes. As stated before, the theorem statement doesn’t actually make it clear that
the result holds for a restricted degree of “infinitariness”, but this is obvious from the proof.</p>
<h2 id="proof">Proof</h2>
<p>I’ll quote the first few sentences of the proof to which I have nothing to add.</p>
<blockquote>
<p>The idea is to replace each formula in |\mathcal F| by a new predicate. Let the
sorts of the language |L’| be the same as those of |L|, and similarly for the [function]
symbols.</p>
</blockquote>
<p>The description of the predicate symbols is complicated by their (potential) infinitary nature.
I’ll quote the proof here as well as I have nothing to add and am not as interested in this case.
The finitary quantifiers case would be similar, just slightly less technical. It would be even
simpler if we defined formulas in a given (ordered) variable context as is typical in categorical
logic.</p>
<blockquote>
<p>With any formula |\phi(\vec x)| in |\mathcal F|, with |\vec x| the repetition free sequence
|\langle x_\beta\rangle_{\beta&lt;\alpha}| of exactly the free variables of |\phi| in a
once and for all fixed order of variables, let us associate the new [predicate] symbol |P_\phi|
of arity |a : \alpha \to \mathrm{Sorts}| such that |a(\beta) = x_\beta|. The [predicate]
symbols of |L’| are the |P_\phi| for all |\phi\in\mathcal F|.</p>
</blockquote>
<p>The motivation of |\mathcal F|-basic sentences / positive existential |\mathcal F|-theories should
now be totally clear. The |\mathcal F|-basic sentences / positive existential |\mathcal F|-theories
are literally basic sentences / positive existential theories in the language of |L’| if we
replace all occurrences of subformulas in |\mathcal F| with their corresponding predicate symbol in |L’|.</p>
<p>We can extend any |L|-structure |M| to an |L’|-structure |M^\sharp| such that they agree on all the sorts
and function symbols of |L|, and |M^\sharp| satisfies |M^\sharp \vDash P_\varphi(\vec a)| if and only if
|M \vDash \varphi(\vec a)|. Which is to say, we <em>define</em> the interpretation of |P_\varphi| to
be the subset of the interpretation of its domain specified by |M \vDash \varphi(\vec a)| for all
|\vec a| in the domain. In more categorical language, we define the subobject that |P_\varphi|
gets sent to to be the subobject |\varphi|.</p>
<p>We can define an |L|-structure, |N^\flat|, for |N| an |L’|-structure by, again, requiring it to do the
same thing to sorts and function symbols as |N|, and defining the interpretation of the predicate
symbols as |N^\flat \vDash R(\vec a)| if and only if |N \vDash P_{R(\vec x)}(\vec a)|.</p>
<p>We immediately have |(M^\sharp)^\flat = M|.</p>
<p>We can extend this to |L’|-formulas. Let |\psi| be an |L’|-formula, then |\psi^\flat| is defined
by a connective-preserving operation for which we only need to specify the action on predicate
symbols. We define that by declaring |P_\varphi(\vec t)^\flat| gets mapped to |\varphi(\vec t)|.
We extend |\flat| to theories via |\mathbb T’^\flat \equiv \{ \varphi^\flat \vdash \psi^\flat \mid (\varphi\vdash\psi) \in \mathbb T’\}|.
A similar induction allows us to prove \[M\vDash\psi^\flat(\vec a)\iff M^\sharp\vDash\psi(\vec a)\]
for all |L|-structures |M| and appropriate |\vec a|.</p>
<p>We have |\mathbb T = \mathbb T’^\flat| for a positive existential theory |\mathbb T’| over |L’|
(or |\sigma = \rho^\flat| for a basic |L’|-sentence |\rho|)
and thus |\varphi^\flat \vDash_M \psi^\flat \iff \varphi \vDash_{M^\sharp}\psi|
for all |\varphi\vdash\psi \in \mathbb T’| (or |M \vDash\sigma \iff M^\sharp\vDash\rho|).
We want to make it so that any |L’|-structure |N| interpreting |\mathbb T’| (or |\rho|) as |\mathbb T|
(or |\sigma|) is of the form |N = M^\sharp| for some |M|. Right now that doesn’t happen because, while
the definition of |M^\sharp| forces it to respect the logical connectives in the formula |\varphi|
associated to the |L’| predicate symbol |P_\varphi|, this isn’t required for an arbitrary model |N|.
For example, nothing requires |N \vDash P_\top| to hold.</p>
<p>The solution is straightforward. In addition to |\mathbb T’| (or |\rho|) representing
the theory |\mathbb T| (or |\sigma|), we add in an additional set of axioms |\Phi|
that capture the behavior of the (encoded) logical connectives of the formulas associated to the
predicate symbols.</p>
<p>These axioms are largely structural with a few exceptions that I’ll address separately. I’ll present
this as a collection of sequents for a theory, but we can replace |\vdash| and |\dashv \vdash| with
|\to| and |\leftrightarrow| for the basic sentence version. |\varphi \dashv\vdash \psi| stands
for two sequents going opposite directions.</p>
<p>\[\begin{align}
\varphi(\vec x) &amp; \dashv\vdash P_\varphi(\vec x) \tag{for atomic $\varphi$} \\
P_{R(\vec x)}(\vec t) &amp; \dashv\vdash P_{R(\vec t)}(\vec y) \tag{for terms $\vec t$ with free variables $\vec y$} \\
P_{\bigwedge\Sigma}(\vec x) &amp; \dashv\vdash \bigwedge_{\varphi \in \Sigma} P_\varphi(\vec x_\varphi) \tag{$\vec x_\varphi$ are the free variables of $\varphi$} \\
P_{\bigvee\Sigma}(\vec x) &amp; \dashv\vdash \bigvee_{\varphi \in \Sigma} P_\varphi(\vec x_\varphi) \tag{$\vec x_\varphi$ are the free variables of $\varphi$} \\
P_{\exists\vec y.\varphi(\vec x,\vec y)}(\vec x) &amp; \dashv\vdash \exists\vec y.P_{\varphi(\vec x,\vec y)}(\vec x,\vec y)
\end{align}\]</p>
<p>We then have two axiom schemas that eliminate the |\forall| and |\to| by leveraging the defining
property of |\mathcal F| being a fragment.</p>
<p>\[\begin{align}
P_{\forall\vec y.\varphi(\vec x,\vec y)}(\vec x) &amp; \dashv\vdash P_{\neg\exists\vec y.\neg\varphi(\vec x,\vec y)}(\vec x) \\
P_{\varphi\to\psi}(\vec x) &amp; \dashv\vdash P_{\neg\varphi}(\vec x) \lor P_\psi(\vec x)
\end{align}\]</p>
<p>We avoid needing negation by axiomatizing that |P_{\neg\varphi}| is the complement to |P_\varphi|. <strong>This
is arguably the key idea.</strong> Once we can simulate the behavior of negation without actually needing it, then
it is clear that we can embed all the other non-positive-existential connectives.</p>
<p>\[\begin{align}
&amp; \vdash P_{\neg\varphi}(\vec x) \lor P_\varphi(\vec x) \\
P_{\neg\varphi}(\vec x) \land P_\varphi(\vec x) &amp; \vdash \bot
\end{align}\]</p>
<p>|\Phi| is the set of all these sequents. (For the basic sentence version, |\Phi| is the set of universal
closures of all these formulas for all |\varphi,\psi \in \mathcal F|.)</p>
<p>Another straightforward structural induction over the subformulas of |\varphi\in\mathcal F| shows that
\[N^\flat \vDash \varphi(\vec a) \iff N \vDash P_\varphi(\vec a)\] for any |L’|-structure |N|
which is a model of |\Phi|. The only interesting case is the negation case. Here, the induction hypothesis
states that |N^\flat\vDash\varphi(\vec a)| agrees with |N\vDash P_\varphi(\vec a)| and the axioms
state that |N\vDash P_{\neg\varphi}(\vec a)| is the complement of the latter which thus agrees with the
complement of the former which is |N^\flat\vDash\neg\varphi(\vec a)|.</p>
<p>From this, it follows that |N = M^\sharp| for |M = N^\flat| or, equivalently, |N = (N^\flat)^\sharp|.</p>
<p>|({-})^\sharp| and |({-})^\flat| thus establish a bijection between the objects of
|\mathrm{Mod}^{(\mathcal F)}(\mathbb T)| (or |\mathrm{Mod}^{(\mathcal F)}(\sigma)|) and
|\mathrm{Mod}^{(L’)}(\mathbb T’\cup\Phi))| (or |\mathrm{Mod}^{(L’)}(\bigwedge(\{\rho\}\cup\Phi))|).
The morphisms of these two categories would each be subclasses of the morphisms of |Str(L_0)| where |L_0| is
the language consisting of only the sorts and function symbols of |L| and thus |L’|. We can show that they
are identical subclasses which basically comes down to showing that an elementary mapping of
|\mathrm{Mod}^{(L’)}(\mathbb T’\cup\Phi))| (or |\mathrm{Mod}^{(L’)}(\bigwedge(\{\rho\}\cup\Phi))|)
is an |\mathcal F|-elementary mapping.</p>
<p>The idea is that such a morphism is a map |h : N \to N’| in |Str(L_0)| which must satisfy
\[N \vDash P_\varphi(\vec a) \implies N’ \vDash P_\varphi(h(\vec a))\] for
all |\varphi \in \mathcal F| and appropriate |\vec a|. However, since |N = (N^\flat)^\sharp|
and |P_\varphi(\vec a)^\flat = \varphi(\vec a)|, we have |N^\flat \vDash \varphi(\vec a) \iff N \vDash P_\varphi(\vec a)|
and similarly for |N’|. Thus
\[N^\flat \vDash \varphi(\vec a) \implies N’^\flat \vDash \varphi(h(\vec a))\] for all
|\varphi \in \mathcal F|, and every such |h| corresponds to an |\mathcal F|-elementary mapping.
Choosing |N = M^\sharp| allows us to show the converse for any |\mathcal F|-elementary
mapping |g : M \to M’|. |\square|</p>
<h3 id="commentary">Commentary</h3>
<p>The proof doesn’t particularly care that we’re interpreting the models into |\mathbf{Set}| and would
work just as well if we interpreted into some other category with the necessary structure. The amount
of structure required would vary with how much “infinitariness” we actually used, though it would need
to be a Boolean category. In particular, the proof works as stated (in its theory form) without
any infinitary connectives being implied for mapping finitary classical first-order logic to coherent logic.</p>
<p>We could simplify the statement and the proof by first eliminating |\forall| and |\to| and then
considering the proof over classical first-order logic with the connectives
|\{\bigwedge,\bigvee,\exists,\neg\}|. This would simplify the definition of fragment and
remove some cases in the proof.</p>
<p>To reiterate, <strong>the key is how we handle negation</strong>.</p>
<h2 id="defunctionalization">Defunctionalization</h2>
<p>Morleyization is related to <a href="https://en.wikipedia.org/wiki/Defunctionalization">defunctionalization</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.
For simplicity, I’ll only consider the finitary, propositional case, i.e. |L_{\omega,\varnothing}|.</p>
<p>In this case, we can consider each |P_\varphi| to be a new data type. In most cases, it would be
a <code>newtype</code> to use Haskell terminology. The only non-trivial case is |P_{\neg\varphi}|. Now, the
computational interpretation of classical propositional logic would use control operators to handle
negation. Propositional coherent logic, however, has a straightforward (first-order) functional
interpretation. Here, a negated formula, |\neg\varphi|, is represented by an primitive type
|P_{\neg\varphi}|.</p>
<p>The |P_{\neg\varphi} \land P_\varphi \vdash \bot| sequent is the <code>apply</code>
function for the defunctionalized continuation (of type |\varphi|). Even more clearly, this
is interderivable with |P_{\neg\varphi} \land \varphi’ \vdash \bot| where |\varphi’| is
the same as |\varphi| except the most shallow negated subformulas are replaced with the corresponding
predicate symbols. In particular, if |\varphi| contains no negated subformulas, then |\varphi’=\varphi|.
We have no way of creating new values of |P_{\neg\varphi}| other than via whatever sequents have been given.
We can, potentially, get a value of |P_{\neg\varphi}| by case analyzing on |\vdash \mathsf{lem}_\varphi : P_{\neg\varphi}\lor P_\varphi|.</p>
<p>What this corresponds to is a first-order functional language with a primitive type for each negated formula.
Any semantics/implementation for this, will need to decide if the primitive type |P_{\neg\varphi}| is
empty or not, and then implement |\mathsf{lem}_\varphi| appropriately (or allow inconsistency). A
programmer writing a program in this signature, however, cannot assume either way whether |P_{\neg\varphi}|
is empty unless they can create a program with that type.</p>
<p>As a very slightly non-trivial example, let’s consider implementing |A \to P_{\neg\neg A}|
corresponding to double negating. Using Haskell-like syntax, the program looks like:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ot">proof ::</span> <span class="dt">A</span> <span class="ot">-&gt;</span> <span class="dt">NotNotA</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>proof a <span class="ot">=</span> <span class="kw">case</span> lem_NotA <span class="kw">of</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>            <span class="dt">Left</span> notNotA <span class="ot">-&gt;</span> notNotA</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>            <span class="dt">Right</span> notA <span class="ot">-&gt;</span> absurd (apply_NotA (notA, a))</span></code></pre></div>
<p>where <code>lem_NotA :: Either NotNotA NotA</code>, <code>apply_NotA :: (NotA, A) -&gt; Void</code>, and <code>absurd :: Void -&gt; a</code>
is the eliminator for |\bot| where |\bot| is represented by <code>Void</code>.</p>
<p>Normally in defunctionalization we’d also be adding constructors to our new types for all the
occurrences of lambdas (or maybe |\mu|s would be better in this case). However, since the only
thing we can do (in general) with <code>NotA</code> is use <code>apply_A</code> on it, no information can be extracted
from it. Either it’s inhabited and behaves like <code>()</code>, i.e. |\top|, or it’s not inhabited and
behaves like <code>Void</code>, i.e. |\bot|. We can even test for this by case analyzing on <code>lem_A</code> which
makes sense because in the classical logic this formula was decidable.</p>
<h2 id="bonus-grothendieck-toposes-as-categories-of-models-of-sketches">Bonus: Grothendieck toposes as categories of models of sketches</h2>
<p>The main point of this section of “Accessible Categories” is to show that we can equivalently
view categories of models of <a href="https://ncatlab.org/nlab/show/sketch">sketches</a>
as categories of models of theories. In particular, models of <strong>geometric sketches</strong>, those whose
cone diagrams are finite but cocone diagrams are arbitrary, correspond to models of <a href="https://ncatlab.org/nlab/show/geometric+theory">geometric theories</a>.</p>
<p>We can view a <a href="https://ncatlab.org/nlab/show/site">site</a>, |(\mathcal C, J)|, for a Grothendieck topos as the
data of a geometric sketch. In particular, |\mathcal C| becomes the underlying category of the sketch, we
add cones to capture all finite limits, and the coverage, |J|, specifies the cocones. These cocones
have a particular form as the <a href="https://ncatlab.org/nlab/show/%CE%BA-ary+exact+category#sinks_and_relations">quotient of the kernel of a sink</a>
as specified by the sieves in |J|. (We need to use the apex of the cones representing pullbacks instead
of actual pullbacks.)</p>
<p>Lemma 3.2.2 shows the sketch-to-theory implication. The main thing I want to note about its proof is that
it illustrates how infinitely large cones would require infinitary (universal) quantification (in addition
to the unsurprising need for infinitary conjunction), but infinitely large cocones <em>do not</em> (but they do
require infinitary disjunction). I’ll not reproduce it here, but it comes down to writing out the normal
set-theoretic constructions of limits and colimits (in |\mathbf{Set}|), but instead of using some first-order
theory of sets, like <strong>ZFC</strong>, uses of sets would be replaced with (infinitary) logical operations. The
“infinite tuples” of an infinite limit become universal quantification over an infinitely large number of
free variables. For the colimits, though, the most complex use of quantifiers is an infinite disjunction of
increasingly deeply nested quantifiers to represent the transitive closure of a relation, but no single
disjunct is infinitary. Figuring out the infinitary formulas is a good exercise.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>An
even more direct connection to defunctionalization is the fact that geometric logic is the internal logic
of Grothendieck toposes, but Grothendieck toposes are elementary toposes and so have the structure to model
implication and universal quantification. It’s just that those connectives aren’t preserved by geometric
morphisms. For implication, the idea is that |A \to B| is represented by
|\bigvee\{\bigwedge\Gamma\mid \Gamma,A\vdash B\}| where |\Gamma| is finite. We can even see how
a homomorphism that preserved geometric logic structure will fail to preserve this definition of |\to|.
Specifically, there could be additional contexts not in the image of the homomorphism that <em>should</em> be
included in the image of the disjunction for it to lead to |\to| in the target but won’t be.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>The Pullback Lemma in Gory Detail (Redux)</title>
    <link href="https://derekelkins.github.io/posts/the-pullback-lemma-in-gory-detail-redux.html" />
    <id>https://derekelkins.github.io/posts/the-pullback-lemma-in-gory-detail-redux.html</id>
    <published>2024-01-14 17:33:54-08:00</published>
    <updated>2024-01-15T01:33:54Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Andrej Bauer has a paper titled <a href="http://math.andrej.com/wp-content/uploads/2012/05/pullback.pdf">The pullback lemma in gory detail</a>
that goes over the proof of the <a href="https://ncatlab.org/nlab/show/pasting+law+for+pullbacks">pullback lemma</a>
in full detail. This is a basic result of category theory and most introductions leave it as an exercise.
It is a good exercise, and you should prove it yourself before reading this article or Andrej Bauer’s.</p>
<p>Andrej Bauer’s proof is what most introductions are expecting you to produce.
I very much like the <a href="styles-of-category-theory.html">representability perspective</a> on category theory
and like to see what proofs look like using this perspective.</p>
<p>So this is a proof of the pullback lemma from the perspective of representability.</p>
<h2 id="preliminaries">Preliminaries</h2>
<p>The key thing we need here is a characterization of pullbacks in terms of representability. To
just jump to the end, we have for |f : A \to C| and |g : B \to C|, |A \times_{f,g} B| is <strong>the
pullback of |f| and |g|</strong> if and only if it represents the functor
\[\{(h, k) \in \mathrm{Hom}({-}, A) \times \mathrm{Hom}({-}, B) \mid f \circ h = g \circ k \}\]</p>
<p>That is to say we have the natural isomorphism \[
\mathrm{Hom}({-}, A \times_{f,g} B) \cong
\{(h, k) \in \mathrm{Hom}({-}, A) \times \mathrm{Hom}({-}, B) \mid f \circ h = g \circ k \}
\]</p>
<p>We’ll write the left to right direction of the isomorphism as |\langle u,v\rangle : U \to A \times_{f,g} B|
where |u : U \to A| and |v : U \to B| and they satisfy |f \circ u = g \circ v|. Applying
the isomorphism right to left on the identity arrow gives us two arrows |p_1 : A \times_{f,g} B \to A|
and |p_2 : A \times_{f,g} B \to B| satisfying |p_1 \circ \langle u, v\rangle = u| and
|p_2 \circ \langle u,v \rangle = v|. (Exercise: Show that this follows from being a <em>natural</em> isomorphism.)</p>
<p>One nice thing about representability is that it reduces categorical reasoning to set-theoretic
reasoning that you are probably already used to, as we’ll see. You can connect this definition
to a typical universal property based definition used in Andrej Bauer’s article. Here we’re taking
it as the definition of the pullback.</p>
<h2 id="proof">Proof</h2>
<p>The claim to be proven is if the right square in the below diagram is a pullback square, then the left
square is a pullback square if and only if the whole rectangle is a pullback square.
\[
\xymatrix {
A \ar[d]_{q_1} \ar[r]^{q_2} &amp; B \ar[d]_{p_1} \ar[r]^{p_2} &amp; C \ar[d]^{h} \\
X \ar[r]_{f} &amp; Y \ar[r]_{g} &amp; Z
}\]</p>
<p>Rewriting the diagram as equations, we have:</p>
<p><strong>Theorem</strong>: If |f \circ q_1 = p_1 \circ q_2|, |g \circ p_1 = h \circ p_2|, and |(B, p_1, p_2)| is a
pullback of |g| and |h|, then |(A, q_1, q_2)| is a pullback of |f| and |p_1| if and only if
|(A, q_1, p_2 \circ q_2)| is a pullback of |g \circ f| and |h|.</p>
<p><strong>Proof</strong>: If |(A, q_1, q_2)| was a pullback of |f| and |p_1| then we’d have the following.</p>
<p>\[\begin{align}
\mathrm{Hom}({-}, A)
&amp; \cong \{(u_1, u_2) \in \mathrm{Hom}({-}, X)\times\mathrm{Hom}({-}, B) \mid f \circ u_1 = p_1 \circ u_2 \} \\
&amp; \cong \{(u_1, (v_1, v_2)) \in \mathrm{Hom}({-}, X)\times\mathrm{Hom}({-}, Y)\times\mathrm{Hom}({-}, C) \mid f \circ u_1 = p_1 \circ \langle v_1, v_2\rangle \land g \circ v_1 = h \circ v_2 \} \\
&amp; = \{(u_1, (v_1, v_2)) \in \mathrm{Hom}({-}, X)\times\mathrm{Hom}({-}, Y)\times\mathrm{Hom}({-}, C) \mid f \circ u_1 = v_1 \land g \circ v_1 = h \circ v_2 \} \\
&amp; = \{(u_1, v_2) \in \mathrm{Hom}({-}, X)\times\mathrm{Hom}({-}, C) \mid g \circ f \circ u_1 = h \circ v_2 \}
\end{align}\]</p>
<p>The second isomorphism is |B| being a pullback and |u_2| is an arrow into |B| so it’s necessarily
of the form |\langle v_1, v_2\rangle|. The first equality is just |p_1 \circ \langle v_1, v_2\rangle = v_1|
mentioned earlier. The second equality merely eliminates the use of |v_1| using the equation |f \circ u_1 = v_1|.</p>
<p>This overall natural isomorphism, however, is exactly what it means for |A| to be a pullback
of |g \circ f| and |h|. We verify the projections are what we expect by pushing |id_A| through
the isomorphism. By assumption, |u_1| and |u_2| will be |q_1| and |q_2| respectively in the first isomorphism.
We see that |v_2 = p_2 \circ \langle v_1, v_2\rangle = p_2 \circ q_2|.</p>
<p>We simply run the isomorphism backwards to get the other direction of the if and only if. |\square|</p>
<p>The simplicity and compactness of this proof demonstrates why I like representability.</p>]]></summary>
</entry>
<entry>
    <title>Universal Quantification and Infinite Conjunction</title>
    <link href="https://derekelkins.github.io/posts/universal-quantification-and-infinite-conjunction.html" />
    <id>https://derekelkins.github.io/posts/universal-quantification-and-infinite-conjunction.html</id>
    <published>2024-01-02 22:00:41-08:00</published>
    <updated>2024-01-03T06:00:41Z</updated>
    <summary type="html"><![CDATA[<script>
extraMacros = {
  den: ["{[\\\![#1]\\\!]}", 1],
  bigden: ["{\\left[\\\!\\\!\\left[#1\\right]\\\!\\\!\\right]}", 1]
};
</script>
<h3 id="introduction">Introduction</h3>
<p>It is not uncommon for universal quantification to be described as
(potentially) infinite conjunction<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.
Quoting Wikipedia’s <a href="https://en.wikipedia.org/w/index.php?title=Quantifier_(logic)&amp;oldid=1060503100#Relations_to_logical_conjunction_and_disjunction">Quantifier_(logic)</a>
page (my emphasis):</p>
<blockquote>
<p>For a finite domain of discourse |D = \{a_1,\dots,a_n\}|, the universal quantifier is equivalent to a logical conjunction of propositions with singular terms |a_i| (having the form |Pa_i| for monadic predicates).</p>
<p>The existential quantifier is equivalent to a logical disjunction of propositions having the same structure as before. <strong>For infinite domains of discourse, the equivalences are similar.</strong></p>
</blockquote>
<p>While there’s a small grain of truth
to this, I think it is wrong and/or misleading far more often than
it’s useful or correct. Indeed, it takes a bit of effort to even
get a statement that makes sense at all. There’s a bit of conflation
between syntax and semantics that’s required to have it naively
make sense, unless you’re working (quite unusually) in an infinitary
logic where it is typically outright false.</p>
<p>What harm does this confusion do? The most obvious harm is that
this view does not generalize to non-classical logics. I’ll focus
on constructive logics, in particular. Besides causing problems in
these contexts, which maybe you think you don’t care about, it betrays
a significant gap in understanding of what universal quantification
actually is. Even in purely classical contexts, this confusion often
manifests, e.g., in <a href="https://math.stackexchange.com/questions/110635/how-it-is-posible-that-omega-inconsistency-does-not-lead-to-inconsistency">confusion about |\omega|-inconsistency</a>.</p>
<p>So what is the difference between universal quantification and
infinite conjunction? Well, the most obvious difference is that
infinite conjunction is indexed by some (meta-theoretic) set that
doesn’t have anything to do with the domain the universal quantifier
quantifies over. However, even if these sets happened to coincide<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> there are
still differences between universal quantification and infinite
conjunction. The key is that universal quantification
requires the predicate being quantified over to hold <em>uniformly</em>,
while infinite conjunction does not. It just so happens that for
the standard set-theoretic semantics of classical first-order logic
this “uniformity” constraint is degenerate. However, even for
classical first-order logic, this notion of uniformity will be
relevant.</p>
<!--more-->
<h3 id="classical-semantic-view">Classical Semantic View</h3>
<p>I want to start in the context where this identification is closest
to being true, so I can show where the idea comes from. The summary
of this section is that the standard, classical, set-theoretic
semantics of universal quantification is equivalent to an infinitary
generalization of the semantics of conjunction. The issue is
“infinitary generalization of the semantics of conjunction” isn’t
the same as “semantics of infinitary conjunction”.</p>
<p>The standard set-theoretic semantics of classical first-order logic
interprets each formula, |\varphi|, as a subset of |D^{\mathsf{fv}(\varphi)}|
where |D| is a given domain set and |\mathsf{fv}| computes the (necessarily finite)
set of free variables of |\varphi|. Traditionally, |D^{\mathsf{fv}(\varphi)}|
would be identified with |D^n| where |n| is the cardinality of |\mathsf{fv}(\varphi)|.
This involves an arbitrary mapping of the free variables of |\varphi|
to the numbers |1| to |n|. The semantics of a formula then becomes an |n|-ary
set-theoretic relation.</p>
<p>The interpretation of binary conjunction is straightforward:</p>
<p>\[\den{\varphi \land \psi} = \den{\varphi} \cap \den{\psi}\]</p>
<p>where |\den{\varphi}| stands for the interpretation of the
formula |\varphi|. To be even
more explicit, I should index this notation by a structure which specifies
the domain, |D|, as well as the interpretations of any predicate or function
symbols, but we’ll just consider this fixed but unspecified.</p>
<p>The interpretation of universal quantification is more complicated
but still fairly straightforward:</p>
<p>\[\den{\forall x.\varphi} = \bigcap_{d \in D}\left\{\bar y|_{\mathsf{fv}(\varphi) \setminus \{x\}} \mid \bar y \in \den{\varphi} \land \bar y(x) = d\right\}\]</p>
<p>Set-theoretically, we have:</p>
<p>\[\begin{align}
\bar z \in \bigcap_{d \in D}\left\{\bar y|_{\mathsf{fv}(\varphi) \setminus \{x\}} \mid \bar y \in \den{\varphi} \land \bar y(x) = d\right\}
\iff &amp; \forall d \in D. \bar z \in \left\{\bar y|_{\mathsf{fv}(\varphi) \setminus \{x\}} \mid \bar y \in \den{\varphi} \land \bar y(x) = d\right\} \\
\iff &amp; \forall d \in D. \exists \bar y \in \den{\varphi}. \bar z = \bar y|_{\mathsf{fv}(\varphi) \setminus \{x\}} \land \bar y(x) = d \\
\iff &amp; \forall d \in D. \bar z[x \mapsto d] \in \den{\varphi}
\end{align}\]</p>
<p>where |f[x \mapsto c]| extends a function |f \in D^{S}| to a function in |D^{S \cup \{x\}}|
via |f[x \mapsto c](v) = \begin{cases}c, &amp;\textrm{ if }v = x \\ f(v), &amp;\textrm{ if }v \neq x\end{cases}|.
The final |\iff| arises because |\bar z[x \mapsto d]| is the <em>unique</em> function which
extends |\bar z| to the desired domain such that |x| is mapped to |d|. Altogether, this
illustrates our desired semantics of the interpretation of |\forall x.\varphi| being the
interpretations of |\varphi| which hold when |x| is interpreted as any element of the domain.</p>
<p>This demonstrates the summary that the semantics of quantification is an infinitary
version of the semantics of conjunction, as |\bigcap| is an infinitary version of |\cap|.
But even here there are substantial cracks in this perspective.</p>
<h3 id="infinitary-logic">Infinitary Logic</h3>
<p>The first problem is that we don’t have an infinitary conjunction so saying
universal quantification is essentially infinitary conjunction doesn’t make sense.
However, it’s easy enough to formulate the syntax and semantics of infinitary
conjunction (assuming we have a meta-theoretic notion of sets).</p>
<p>Syntactically, for a (meta-theoretic) set |I| and an |I|-indexed family of formulas
|\{\varphi_i\}_{i \in I}|, we have the infinitary conjunction |\bigwedge_{i \in I} \varphi_i|.</p>
<p>The set-theoretic semantics of this connective <em>is</em> a direct generalization of the
binary conjunction case:</p>
<p>\[\bigden{\bigwedge_{i \in I}\varphi_i} = \bigcap_{i \in I}\den{\varphi_i}\]</p>
<p>If |I = \{1,2\}|, we recover exactly the binary conjunction case.</p>
<p>Equipped with a semantics of <em>actual</em> infinite conjunction, we can compare
to the semantics of universal quantification case and see where things go wrong.</p>
<p>The first problem is that it makes no sense to choose |I| to be |D|. The formula
|\bigwedge_{i \in I} \varphi_i| can be interpreted with respect to many
different domains. So any particular choice of |D| would be wrong for most semantics.
This is assuming that our syntax’s meta-theoretic sets were the same as our
semantics’ meta-theoretic sets, which need not be the case at all<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>An even bigger problem is that infinitary conjunction expects a family of formulas
while with universal quantification has just one. This is one facet of the uniformity
I mentioned. Universal quantification has one formula that is interpreted a single
way (with respect to the given structure). The infinitary intersection expression
is computing a set out of this singular interpretation. Infinitary conjunction, on
the other hand, has a family of formulas which need have no relation to each other.
Each of these formulas is independently interpreted and then all those separate
interpretations are combined with an infinitary intersection. The problem we have
is that there’s generally no way to take a formula |\varphi| with free variable |x|
and an element |d \in D| and make a formula |\varphi_d| with |x| not free such that
|\bar y[x \mapsto d] \in \den{\varphi} \iff \bar y \in \den{\varphi_d}|. A simple
cardinality argument shows that: there are only countably many (finitary) formulas,
but there are plenty of uncountable domains. This is why |\omega|-inconsistency is
possible. We can easily have elements in the domain which cannot be captured by any
formula.</p>
<h3 id="syntactic-view">Syntactic View</h3>
<p>Instead of taking a semantic view, let’s take a syntactic view of universal
quantification and infinitary conjunction, i.e. let’s compare the rules that
characterize them. As before, the first problem we have is that traditional
first-order logic does not have infinitary conjunction, but we can easily
formulate what the rules would be.</p>
<p>The elimination rules are superficially similar but have subtle but important
distinctions:</p>
<p>\[\frac{\Gamma \vdash \forall x.\varphi}{\Gamma \vdash \varphi[x \mapsto t]}\forall E,t
\qquad \frac{\Gamma \vdash \bigwedge_{i \in I} \varphi_i}{\Gamma \vdash \varphi_j}{\wedge}E,j\]
where |t| is a term, |j| is an element of |I|, and |\varphi[x \mapsto t]| corresponds
to syntactically substituting |t| for |x| in |\varphi| in a capture-avoiding way. A first,
not-so-subtle distinction is if |I| is an infinite set, then |\bigwedge_{i \in I}\varphi_i|
is an infinitely large formula. Another pretty obvious issue is universal quantification is
restricted to instantiating terms while |I| stands for either an <em>arbitrary</em> (meta-theoretic)
set or it may stand for some particular (meta-theoretic) set, e.g. |\mathbb N|. Either
way, it is typically not the set of terms of the logic.</p>
<p>Arguably, this isn’t an issue since the claim isn’t that every infinite conjunction
corresponds to a universal quantification, but only that universal quantification
corresponds to some infinite conjunction. The set of terms is a possible choice for
|I|, so that shouldn’t be a problem. Well, whether it’s a problem or not depends on
how you set up the syntax of the language. In my preferred way of handling the syntax
of logical formulas, I index each formula by the set of free variables that may occur
in that formula. This means the set of terms varies with the set of possible free
variables. Writing |\vdash_V \varphi| to mean |\varphi| is well-formed and provable
in a context with free variables |V|, then we would want the following rule:</p>
<p>\[\frac{\vdash_V \varphi}{\vdash_U \varphi}\] where |V \subseteq U|. This
simply states that if a formula is provable, it should remain provable even if we
add more (unused) free variables. This causes a problem with having an infinitary
conjunction indexed by terms. Writing |\mathsf{Term}(V)| for the set of terms
with (potential) free variables in |V|, then while |\vdash_V \bigwedge_{t \in \mathsf{Term}(V)}\varphi_t|
might be okay, this would also lead to |\vdash_U \bigwedge_{t \in \mathsf{Term}(V)}\varphi_t| which
would also hold but would no longer correspond to universal quantification in
a context with free variables in |U|. This really makes a difference. For example,
for many theories, such as the usual presentation of <strong>ZFC</strong>, |\mathsf{Term}(\varnothing) = \varnothing|,
i.e. there are no closed terms. As such, |\vdash_\varnothing \forall x.\bot|
is neither provable (which we wouldn’t expect it to be) <em>nor refutable</em> without additional axioms. On the
other hand, |\bigwedge_{i \in \varnothing}\bot| is |\top| and thus trivially
provable. If we consider |\vdash_{\{y\}} \forall x.\bot| next, it becomes
refutable. This doesn’t contradict our earlier rule about adding free variables
because |\vdash_\varnothing \forall x.\bot| wasn’t provable and so the rule
says nothing. On the other hand, that rule does require |\vdash_{\{y\}} \bigwedge_{i \in \varnothing}\bot|
to be provable, and it is. Of course, it no longer corresponds to |\forall x.\bot|
with this set of free variables. The putative corresponding formula would be
|\bigwedge_{i \in \{y\}}\bot| which is indeed refutable.</p>
<p>With the setup above, we can’t get the elimination rule for |\bigwedge| to
correspond to the elimination rule for |\forall|, because there isn’t a
singular set of terms. However, a more common if less clean approach is to allow
all free variables all the time, i.e. to fix a single countably infinite
set of variables once and for all. This would “resolve” this problem.</p>
<p>The differences in the introduction rules are more stark. The rules are:</p>
<p>\[\frac{\Gamma \vdash \varphi \quad x\textrm{ not free in }\Gamma}{\Gamma \vdash \forall x.\varphi}\forall I
\qquad \frac{\left\{\Gamma \vdash \varphi_i \right\}_{i \in I}}{\Gamma \vdash \bigwedge_{i \in I}\varphi_i}{\wedge}I\]</p>
<p>Again, the most blatant difference is that (when |I| is infinite) |{\wedge}I|
corresponds to an infinitely large derivation. Again, the uniformity aspects
show through. |\forall I| requires a single derivation that will handle all
terms, whereas |{\wedge}I| allows a different derivation for each |i \in I|.</p>
<p>We don’t run into the same issue as in the semantic view with needing to turn
elements of the domain into terms/formulas. Given a formula |\varphi| with
free variable |x|, we can easily make a formula |\varphi_t| for every
term |t|, namely |\varphi_t = \varphi[x \mapsto t]|. We won’t have the
issue that leads to |\omega|-inconsistency because |\forall x.\varphi|
is derivable from |\bigwedge_{t \in \mathsf{Term}(V)}\varphi[x \mapsto t]|.
Of course, the reason this is true is because one of the terms in |\mathsf{Term}(V)|
will be a variable not occurring in |\Gamma| allowing us to derive the
premise of |\forall I|. On the other hand, if we choose |I = \mathsf{Term}(\varnothing)|,
i.e. only consider closed terms, which is what <a href="https://en.wikipedia.org/wiki/%CE%A9-consistent_theory#%CF%89-logic">the |\omega| rule in arithmetic</a>
is doing, then we definitely can get |\omega|-inconsistency-like situations.
Most notably, in the case of theories, like <strong>ZFC</strong>, which have <em>no</em> closed
terms.</p>
<h3 id="constructive-view">Constructive View</h3>
<p>A constructive perspective allows us to accentuate the contrast between
universal quantification and infinitary conjunction even more as well
as bring more clarity to the notion of uniformity.</p>
<p>We’ll start with the <a href="https://en.wikipedia.org/wiki/Brouwer%E2%80%93Heyting%E2%80%93Kolmogorov_interpretation">BHK interpretation</a>
of Intuitionistic logic and specifically a <a href="https://en.wikipedia.org/wiki/Realizability">realizabilty</a>
interpretation. For this, we’ll allow infinitary conjunction only for |I = \mathbb N|.</p>
<p>I’ll write |n\textbf{ realizes }\varphi| for the statement that the natural number |n|
realizes the formula |\varphi|. As in the linked articles, we’ll need a computable <a href="https://en.wikipedia.org/wiki/Pairing_function">pairing function</a>
which computably encodes a pair of natural numbers as a natural number. I’ll just write
this using normal pairing notation, i.e. |(n,m)|. We’ll also need Gödel numbering
to computably map a natural number |n| to a computable function |f_n|.</p>
<p>\[\begin{align}
(n_0, n_1)\textbf{ realizes }\varphi_1 \land \varphi_2
\quad &amp; \textrm{if and only if} \quad n_0\textbf{ realizes }\varphi_0\textrm{ and }
n_1\textbf{ realizes }\varphi_1 \\
n\textbf{ realizes }\forall x.\varphi
\quad &amp; \textrm{if and only if}\quad \textrm{for all }m, f_n(m)\textbf{ realizes }\varphi[x \mapsto m] \\
(k, n_k)\textbf{ realizes }\varphi_1 \lor \varphi_2
\quad &amp; \textrm{if and only if} \quad k \in \{0, 1\}\textrm{ and }n_k\textbf{ realizes }\varphi_k \\
n\textbf{ realizes }\neg\varphi
\quad &amp; \textrm{if and only if} \quad\textrm{there is no }m\textrm{ such that }m\textbf{ realizes }\varphi
\end{align}\]</p>
<p>I included disjunction and negation in the above so I could talk about the Law of the Excluded Middle.
Via the above interpretation, given any formula |\varphi| with free variable |x|,
the meaning of |\forall x.\varphi\lor\neg\varphi| would be a computable function
which for each natural number |m| produces a bit indicating whether or not
|\varphi[x \mapsto m]| holds. The Law of Excluded Middle holding would thus mean
every such formula is computationally decidable which we know isn’t the case. For example,
choose |\varphi| as the formula which asserts that the |x|-th Turing machine halts.</p>
<p>This example illustrates the uniformity constraint. Assuming a traditional, classical
meta-language, e.g. <strong>ZFC</strong>, then it is the case that |(\varphi\lor\neg\varphi)[x \mapsto m]| is
realized for each |m| in the case where |\varphi| is asserting the halting of the |x|-th
Turing machine<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. But this interpretation of universal quantification
requires not only that the quantified formula holds for all naturals, but also that
we can computably find this out.</p>
<p>It’s clear that trying to formulate a notion of infinitary conjunction with regards
to realizability would require using something other than natural numbers as realizers
if we just directly generalize the finite conjunction case. For example, we might
use potentially infinite sequences of natural numbers as realizers. Regardless,
the discussion of the previous example makes it clear an interpretation of infinitary
conjunction can’t be done in standard computability<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>,
while, obviously, universal quantification can.</p>
<h3 id="categorical-view">Categorical View</h3>
<p>The categorical semantics of universal quantification and conjunction are quite different
which also suggests that they are not related, at least not in some straightforward way.</p>
<p>One way to get to categorical semantics is to restate traditional, set-theoretic semantics
in categorical terms. Traditionally, the semantics of a formula is a subset
of some product of the domain set, one for each free variable. Categorically, that
suggests we want finite products and the categorical semantics of a formula should
be a subobject of a product of some object representing the domain.</p>
<p>Conjunction is traditionally represented via intersection of subsets, and categorically
we form the intersection of subobjects via pulling back. So to support finite conjunctions,
we need our category to additionally have finite pullbacks of monomorphisms. Infinitary
conjunctions simply require infinitely wide pullbacks of monomorphisms. However, we
can start to see some cracks here. What does it mean for a pullback to be infinitely wide?
It means the obvious thing; namely, that we have an infinite set of monomorphisms sharing
a codomain, and we’ll take the limit of this diagram. The key here, though, is “<em>set</em>”.
Regardless of whatever the objects of our semantic category are, the infinitary conjunctions
are indexed by a set.</p>
<p>To talk about the categorical semantics of universal quantification, we need to bring to
the foreground some structure that we have been leaving – and traditionally accounts do leave
– in the background. Before, I said the semantics of a formula, |\varphi|, depends on the
free variables in that formula, e.g. if |D| is our domain object, then the semantics of
a formula with three free variables would be a subobject of |\prod_{v \in \mathsf{fv}(\varphi)}D \cong D\times D \times D|
which I’ll continue to write as |D^{\mathsf{fv}(\varphi)}| though now it will be
interpreted as a product rather than a function space. For |\mathbf{Set}|,
this makes no difference.
It would be more accurate to say that a formula can be given semantics in any product
of the domain object indexed by any <em>superset</em> of the free variables. This is just
to say that a formula doesn’t need to use every free variable that is available. Nevertheless,
even if it is induced by the same formula, a subobject of |D^{\mathsf{fv}(\varphi)}| is
a different subobject than a subobject of |D^{\mathsf{fv}(\varphi) \cup \{u\}}|
where |u| is a variable not free in |\varphi|, so we need a way of relating the semantics
of formulas considered with respect to different sets of free variables.</p>
<p>To do this, we will formulate a category of contexts and index our semantics by it.
Fix a category |\mathcal C| and an object |D| of |\mathcal C|. Our category
of contexts, |\mathsf{Ctx}|, will be the full subcategory of |\mathcal C| with
objects of the form |D^S| where |S| is a finite subset of |V|, a fixed set of
variables. We’ll assume these products exist, though typically we’ll just assume
that |\mathcal C| has <em>all</em> finite products. From here, we use the |\mathsf{Sub}| functor.
|\mathsf{Sub} : \mathsf{Ctx}^{op} \to \mathbf{Pos}| maps an object
of |\mathsf{Ctx}| to the poset of its subobjects as objects of |\mathcal C|<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.
Now an arrow |f : D^{\{x,y,z,w\}} \to D^{\{x,y,z\}}| would induce a monotonic function
|\mathsf{Sub}(f) : \mathsf{Sub}(D^{\{x,y,z\}}) \to \mathsf{Sub}(D^{\{x,y,z,w\}})|.
This is defined for each subobject by pulling back a representative monomorphism of that subobject along |f|.
Arrows of |\mathsf{Ctx}| are the semantic analogues of substitutions, and |\mathsf{Sub}(f)|
applies these “substitutions” to the semantics of formulas.</p>
<p>Universal quantification is then characterized as the (indexed) right adjoint (Galois
connection in this context) of |\mathsf{Sub}(\pi^x)| where |\pi^x : D^S \to D^{S \setminus \{x\}}|
is just projection. The indexed nature of this adjoint leads to <a href="/posts/beck-chevalley.html">Beck-Chevalley conditions</a>
reflecting the fact universal quantification should respect substitution.
|\mathsf{Sub}(\pi^x)| corresponds to adding |x| as a new, unused free variable to a formula.
Let |U| be a subobject of |D^{S \setminus \{x\}}| and |V| a subobject of |D^S|. Furthermore, write
|U \sqsubseteq U’| to indicate that |U| is a subobject of the subobject |U’|, i.e.
that the monos that represent |U| factor through the monos that represent |U’|. The
adjunction then states: \[\mathsf{Sub}(\pi^x)(U) \sqsubseteq V\quad \textrm{if and only if}\quad U \sqsubseteq \forall_x(V)\]
The |\implies| direction is a fairly direct semantic analogue of the |\forall I| rule:
\[\frac{\Gamma \vdash \varphi\quad x\textrm{ not free in }\Gamma}{\Gamma \vdash \forall x.\varphi}\]
Indeed, it is easy to show that the converse of this rule is derivable with |\forall E|
validating the semantic “if and only if”. To be clear, the full adjunction is natural in
|U| and |V| and indexed, effectively, in |S|.</p>
<p>Incidentally, we’d also want the semantics of infinite conjunctions to respect substitution,
so they too have a Beck-Chevalley condition they satisfy and give rise to an indexed right adjoint.</p>
<p>It’s hard to even compare the categorical semantics of infinitary conjunction and universal
quantification, let alone conflate them, <em>even when |\mathcal C = \mathbf{Set}|</em>. This isn’t
too surprising as these semantics work just fine for constructive logics where, as illustrated
earlier, these can be semantically distinct. As mentioned, both of these constructs can be
described by indexed right adjoints. However, they are adjoints between very different indexed
categories. If |\mathcal M| is our indexed category (above it was |\mathsf{Sub}|), then we’ll
have |I|-indexed products if |\Delta_{\mathcal M} : \mathcal M \to [DI, -] \circ \mathcal M|
has an indexed right adjoint where |D : \mathbf{Set} \to \mathbf{cat}| is the discrete (small)
category functor. For |\mathcal M| to have universal quantification, we need an indexed right
adjoint to an indexed functor |\mathcal M \circ \mathsf{cod} \circ \iota \to \mathcal M \circ \mathsf{dom} \circ \iota|
where |\iota : s(\mathsf{Ctx}) \hookrightarrow \mathsf{Ctx}^{\to}| is the full subcategory
of the arrow category |\mathsf{Ctx}^{\to}| consisting of just the projections.</p>
<h3 id="conclusion">Conclusion</h3>
<p>My hope is that the preceding makes it abundantly clear that viewing universal quantification
as some kind of special “infinite conjunction” is not sensible even approximately. To do so is
to seriously misunderstand universal quantification. Most discussions “equating” them involve
significant conflations of syntax and semantics where a specific choice of domain is fixed and elements
of that specific domain are used as terms.</p>
<p>A secondary goal was to illustrate an aspect of logic from a variety of perspectives and illustrate
some of the concerns in meta-logical reasoning. For example, quantifiers and connectives are
syntactical concepts and thus can’t depend on the details of the semantic domain. As another
example, better perspectives on quantifiers and connectives are more robust to weakening the
logic. I’d say this is especially true when going from classical to constructive logic.
Structural proof theory and categorical semantics are good at formulating logical concepts
modularly so that they still make sense in very weak logics.</p>
<p>Unfortunately, the traditional trend towards minimalism strongly pushes in the
other direction leading to the exploiting of every symmetry and coincidence a
stronger logic (namely classical logic) provides producing definitions that
don’t survive even mild weakening of the logic<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. The attempt to identify
universal quantification with infinite conjunction here takes that impulse too
far and doesn’t even work in classical logic as demonstrated. While there’s
certainly value in recognizing redundancy, I personally find minimizing logical
assumptions far more important and valuable than minimizing (primitive) logical
connectives.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>
“Universal statements are true if they are true for every individual in the world. They can be thought of as an infinite conjunction,” from <a href="https://www.massey.ac.nz/~mjjohnso/notes/59302/l07.html">some random AI lecture notes</a>. You can find many others.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The domain
doesn’t even need to be a set.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>For example,
we may formulate our syntax in a second-order arithmetic identifying our syntax’s
meta-theoretic sets with unary predicates, while our semantics is in <strong>ZFC</strong>. Just
from cardinality concerns, we know that there’s no way of injectively mapping
every <strong>ZFC</strong> set to a set of natural numbers.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>It’s probably worth pointing out that not only will this classical meta-language
not tell us whether it’s |\varphi[x \mapsto m]| or |\neg\varphi[x \mapsto m]| that
holds for every specific |m|, but it’s easy to show (assuming consistency of <strong>ZFC</strong>)
that |\varphi[x \mapsto m]| is independent of <strong>ZFC</strong> for specific values of |m|.
For example, it’s easy to make a Turing machine that halts if and only if it finds
a contradiction in the theory of <strong>ZFC</strong>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Interestingly, for some models
of computation, e.g. ones based on Turing machines, infinitary disjunction, or, specifically,
|\mathbb N|-ary disjunction is <em>not</em> problematic. Given an infinite sequence of halting
Turing machines, we can interleave their execution such that every Turing machine in the
sequence will halt at some finite time. Accordingly, extending the definition of
disjunction in realizability to the |\mathbb N|-ary case does not run into any
of the issues that |\mathbb N|-ary conjunction has and is completely unproblematic.
We just let |k| be an arbitrary natural instead of just |\{0, 1\}|.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This is a place we could generalize the
categorical semantics further. There’s no reason we need to consider this particular functor.
We could consider other functors from |\mathsf{Ctx}^{op} \to \mathbf{Pos}|, i.e. other
<a href="https://ncatlab.org/nlab/show/indexed+category">indexed</a> <a href="https://ncatlab.org/nlab/show/%280%2C1%29-category+theory">|(0,1)|-categories</a>.
This setup is called a <a href="https://ncatlab.org/nlab/show/hyperdoctrine">hyperdoctrine</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The most obvious example of
this is defining quantifiers and connectives in terms of other connectives
particularly when negation is involved. A less obvious example is the overwhelming
focus on |\mathbf 2|-valued semantics when classical logic naturally allows
arbitrary Boolean-algebra-valued semantics.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>What is the coproduct of two groups?</title>
    <link href="https://derekelkins.github.io/posts/what-is-the-coproduct-of-two-groups.html" />
    <id>https://derekelkins.github.io/posts/what-is-the-coproduct-of-two-groups.html</id>
    <published>2023-12-21 18:47:57-08:00</published>
    <updated>2023-12-22T02:47:57Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The purpose of this article is to answer the question: what is the coproduct
of two groups? The approach, however, will be somewhat absurd. Instead of
simply presenting a construction and proving that it satisfies the appropriate
universal property, I want to find the general answer and simply instantiate
it for the case of groups.</p>
<p>Specifically, this will be a path through the theory of Lawvere theories and
their models with the goal of motivating some of the theory around it in
pursuit of the answer to this relatively simple question.</p>
<p>If you really just want to know the answer to the title question, then the
construction is usually called the <a href="https://en.wikipedia.org/wiki/Free_product">free product</a>
and is described on the linked Wikipedia page.</p>
<!--more-->
<h2 id="groups-as-models-of-a-lawvere-theory">Groups as Models of a Lawvere Theory</h2>
<p>A group is a model of an equational theory. This means a group is described by a set
equipped with a collection of operations that must satisfy some equations. So we’d
have a set, |G|, and operations |\mathtt{e} : () \to G|, |\mathtt{i} : G \to G|,
and |\mathtt{m} : G \times G \to G|. These operations satisfy the equations,
\[
\begin{align}
\mathtt{m}(\mathtt{m}(x, y), z) = \mathtt{m}(x, \mathtt{m}(y, z)) \\
\mathtt{m}(\mathtt{e}(), x) = x = \mathtt{m}(x, \mathtt{e}()) \\
\mathtt{m}(\mathtt{i}(x), x) = \mathtt{e}() = \mathtt{m}(x, \mathtt{i}(x))
\end{align}
\]
universally quantified over |x|, |y|, and |z|.</p>
<p>These equations can easily be represented by commutative diagrams, i.e. equations
of compositions of arrows, in any category with finite products of an object, |G|,
with itself. For example, the left inverse law becomes:
\[
\mathtt{m} \circ (\mathtt{i} \times id_G) = \mathtt{e} \circ {!}_G
\]
where |{!}_G : G \to 1| is the unique arrow into the terminal object corresponding
to the |0|-ary product of copies of |G|.</p>
<p>One nice thing about this categorical description is that we can now talk about
a group object in any category with finite products. Even better, we can make this
pattern describing what a group is first-class. The (<strong>Lawvere</strong>) <strong>theory of a
group</strong> is a (small) category, |\mathcal{T}_{\mathbf{Grp}}| whose objects are an object |\mathsf{G}| and
all its powers, |\mathsf{G}^n|, where |\mathsf{G}^0 = 1|
and |\mathsf{G}^{n+1} = \mathsf{G} \times \mathsf{G}^n|. The arrows consist
of the relevant projection and tupling operations, the three arrows above,
|\mathsf{m} : \mathsf{G}^2 \to \mathsf{G}^1|, |\mathsf{i} : \mathsf{G}^1 \to \mathsf{G}^1|,
|\mathsf{e} : \mathsf{G}^0 \to \mathsf{G}^1|, and all composites that could
be made with these arrows. See my <a href="/posts/category-theory-syntactically.html#finite-product-theories">previous article</a>
for a more explicit description of this, but it should be fairly intuitive.</p>
<p>An actual group is then, simply, a finite-product-preserving functor
|\mathcal{T}_{\mathbf{Grp}} \to \mathbf{Set}|. It must be finite-product-preserving
so the image of |\mathsf{m}| actually gets sent to a binary function
and not some function with some arbitrary domain. The category, |\mathbf{Grp}|, of
groups and group homomorphisms is equivalent to the category |\mathbf{Mod}_{\mathcal{T}_{\mathbf{Grp}}}|
which is defined to be the full subcategory of the category of functors from |\mathcal{T}_{\mathbf{Grp}} \to \mathbf{Set}|
consisting of the functors which preserve finite products. While we’ll not explore it more
here, we could use <em>any</em> category with finite products as the target, not just |\mathbf{Set}|.
For example, we’ll show that |\mathbf{Grp}| has finite products, and in fact all limits
and colimits, so we can talk about the models of the theory of groups in the
category of groups. This turns out to be equivalent to the category of Abelian
groups via the well-known <a href="https://en.wikipedia.org/wiki/Eckmann%E2%80%93Hilton_argument">Eckmann-Hilton argument</a>.</p>
<h2 id="a-bit-of-organization">A Bit of Organization</h2>
<p>First, a construction that will become even more useful later. Given any category, |\mathcal{C}|,
we define |\mathcal{C}^{\times}|, or, more precisely, an inclusion |\sigma : \mathcal{C} \hookrightarrow \mathcal{C}^{\times}|
to be the <strong>free category-with-finite-products generated from</strong> |\mathcal{C}|. Its universal property is:
given any functor |F : \mathcal{C} \to \mathcal{E}| into a category-with-finite-products |\mathcal E|,
there exists a unique finite-product-preserving functor |\bar{F} : \mathcal{C}^{\times} \to \mathcal E|
such that |F = \bar{F} \circ \sigma|.</p>
<p>An explicit construction of |\mathcal{C}^{\times}| is the following. Its objects consist
of (finite) lists of objects of |\mathcal{C}| with concatenation as the categorical product
and the empty list as the terminal object. The arrows are tuples with a component for each
object in the codomain list. Each component is a pair of an index into the domain list and
an arrow from the corresponding object in the domain list to the object in the codomain list
for this component. For example, the arrow |[A, B] \to [B, A]| would be |((1, id_B), (0, id_A))|.
Identity and composition is straightforward. |\sigma| then maps each object to a singleton
list and each arrow |f| to |((0, f))|.</p>
<p>Like most free constructions, this construction completely ignores any finite products the
original category may have had. In particular, we want the category
|\mathcal{T}_{\mathbf{Set}} = \mathbf{1}^{\times}|, called <strong>the theory of a set</strong>.
The fact that the one object of the category |\mathbf{1}| is terminal has nothing to do
with its image via |\sigma| which is not the terminal object.</p>
<p>We now define the general notion of a (<strong>Lawvere</strong>) <strong>theory</strong> as a small category
with finite products, |\mathcal{T}|, equipped with a finite-product-preserving, identity-on-objects
functor |\mathcal{T}_{\mathbf{Set}} \to \mathcal{T}|. A <strong>morphism of</strong> (<strong>Lawvere</strong>) <strong>theories</strong>
is a finite-product-preserving functor that preserves these inclusions a la:
\[
\xymatrix {
&amp; \mathcal{T}_{\mathbf{Set}} \ar[dl] \ar[dr] &amp; \\
\mathcal{T}_1 \ar[rr] &amp; &amp; \mathcal{T}_2
}
\]</p>
<p>The identity-on-objects aspect of the inclusion of |\mathcal{T}_{\mathbf{Set}}|
along with finite-product-preservation ensures that the only objects in |\mathcal{T}|
are powers of a single object which we’ll generically call |\mathsf{G}|. This is
sometimes called the “generic object”, though the term “generic object” has other
meanings in category theory.</p>
<p>A <strong>model of a theory</strong> (in |\mathbf{Set}|) is then simply a finite-product-preserving
functor into |\mathbf{Set}|. |\mathbf{Mod}_{\mathcal{T}}| is the full subcategory
of functors from |\mathcal{T} \to \mathbf{Set}| which preserve finite products.
The <strong>morphisms of models</strong> are simply the natural transformations. As an exercise,
you should show that for a natural transformation |\tau : M \to N| where |M|
and |N| are two models of the same theory, |\tau_{\mathsf{G}^n} = \tau_{\mathsf{G}}^n|.</p>
<h2 id="the-easy-categorical-constructions">The Easy Categorical Constructions</h2>
<p>This relatively simple definition of model already gives us a large swathe of results.
An easy result in basic category theory is that (co)limits in functor
categories are computed pointwise whenever the corresponding (co)limits exist
in the codomain category. In our case, |\mathbf{Set}| has all (co)limits, so
all categories of |\mathbf{Set}|-valued functors have all (co)limits and they
are computed pointwise.</p>
<p>However, the (co)limit of finite-product-preserving functors into |\mathbf{Set}|
may not be finite-product-preserving, so we don’t immediately get that |\mathbf{Mod}_{\mathcal{T}}|
has all (co)limits (and they are computed pointwise). That said, finite products
are limits and limits commute with each other, so we <em>do</em> get that |\mathbf{Mod}_{\mathcal{T}}|
has all limits and they are computed pointwise. Similarly, sifted colimits, which are
colimits that commute with finite products in |\mathbf{Set}| also exist and are
computed pointwise in |\mathbf{Mod}_{\mathcal{T}}|. Sifted colimits include the
better known filtered colimits which commute with all finite limits.</p>
<p>I’ll not elaborate on sifted colimits.
We’re here for (finite) coproducts, and, as you’ve probably already guessed,
coproducts are not sifted colimits.</p>
<h2 id="when-the-coproduct-of-groups-is-easy">When the Coproduct of Groups is Easy</h2>
<p>There is one class of groups whose coproduct is easy to compute for
general reasons: the free groups. The free group construction, like
most “free constructions”, is a left adjoint and left adjoints
preserve colimits, so the coproduct of two free groups is just the
free group on the coproduct, i.e. disjoint union, of their generating
sets. We haven’t defined the free group yet, though.</p>
<p>Normally, the free group construction would be defined as left
adjoint to the underlying set functor. We have a very straightforward
way to define the underlying set functor. Define |U : \mathbf{Mod}_{\mathcal T} \to \mathbf{Set}|
as |U(M) = M(\mathsf{G}^1)| and |U(\tau) = \tau_{\mathsf{G}^1}|.
Identifying |\mathsf{G}^1| with the functor |\mathsf G : \mathbf{1} \to \mathcal{T}|
we have |U(M) = M \circ \mathsf{G}| giving a functor |\mathbf{1} \to \mathbf{Set}|
which we identify with a set. The left adjoint to precomposition
by |\mathsf{G}| is the left Kan extension along |\mathsf{G}|.</p>
<p>We then compute |F(S) = \mathrm{Lan}_{\mathsf{G}}(S)
\cong \int^{{*} : \mathbf{1}} \mathcal{T}(\mathsf{G}({*}), {-}) \times S({*})
\cong \mathcal{T}(\mathsf{G}^1, {-}) \times S|. This is the left
Kan extension and does form an adjunction but <em>not</em> with the category
of models because the functor produced by |F(S)| does not preserve finite products.
We should have |F(S)(\mathsf{G}^n) \cong F(S)(\mathsf{G})^n|, but substituting
in the definition of |F(S)| clearly does not satisfy this. For example,
consider |F(\varnothing)(\mathsf{G}^0)|.</p>
<p>We can and will show that the left Kan extension of a functor into |\mathbf{Set}|
preserves finite products when the original functor did. Once
we have that result we can correct our definition of the free construction.
We simply replace |S : \mathbf{1} \to \mathbf{Set}| with a functor that
<em>does</em> preserve finite products, namely |\bar{S} : \mathbf{1}^{\times} \to \mathbf{Set}|.
Of course, |\mathbf{1}^{\times}| is exactly our definition of |\mathcal{T}_{\mathbf{Set}}|.
We see now that a model of |\mathcal{T}_{\mathbf{Set}}| is the same thing as having
a set, hence the name. Indeed, we have an equivalence of categories between |\mathbf{Set}|
and |\mathbf{Mod}_{\mathcal{T}_{\mathbf{Set}}}|. (More generally, this theory is called
“the theory of an object” as we may consider models in categories other than |\mathbf{Set}|,
and we’ll still have this relation.)</p>
<p>The correct definition of |F| is |F(S) = \mathrm{Lan}_{\iota}(\bar S)
\cong \int^{\mathsf{G}^n:\mathcal{T}_{\mathbf{Set}}} \mathcal{T}(\iota(\mathsf{G}^n), {-}) \times \bar{S}(\mathsf{G}^n)
\cong \int^{\mathsf{G}^n:\mathcal{T}_{\mathbf{Set}}} \mathcal{T}(\iota(\mathsf{G}^n), {-}) \times S^n|
where |\iota : \mathcal{T}_{\mathbf{Set}} \to \mathcal{T}| is the inclusion
we give as part of the definition of a theory. We can also see |\iota| as |\bar{\mathsf{G}}|.</p>
<p>We can start to see the term algebra in this definition. An element of |F(S)| is
a choice of |n|, an |n|-tuple of elements of |S|, and a (potentially compound) |n|-ary operation.
We can think of an element of |\mathcal{T}(\mathsf{G}^n, {-})| as a term with |n|
free variables which we’ll label with the elements of |S^n| in |F(S)|. The equivalence
relation in the explicit construction of the coend allows us to swap projections and
tupling morphisms from the term to the tuple of labels. For example, it equates a
unary term paired with one label with a binary term paired with two labels but where
the binary term immediately discards one of its inputs. Essentially, if you are
given a unary term and two labels, you can either discard one of the labels or you
can make the unary term binary by precomposing with a projection. Similarly for
tupling.</p>
<p>It’s still not obvious this definition produces a functor which preserves finite products.
As a lemma to help in the proof of that fact, we have a bit of <a href="https://arxiv.org/abs/1501.02503">coend calculus</a>.</p>
<p><strong>Lemma 1</strong>: Let |F \dashv U : \mathcal{D} \to \mathcal{C}| and |H : \mathcal D^{op} \times \mathcal{C} \to \mathcal{E}|.
Then, |\int^C H(FC, C) \cong \int^D H(D, UD)| when one, and thus both, exist. <strong>Proof</strong>:
\[
\begin{align}
\mathcal{E}\left(\int^C H(FC, C), {-}\right)
&amp; \cong \int_C \mathcal{E}(H(FC, C), {-}) \tag{continuity} \\
&amp; \cong \int_C \int_D [\mathcal{D}(FC, D), \mathcal{E}(H(D, C), {-})] \tag{Yoneda} \\
&amp; \cong \int_C \int_D [\mathcal{C}(C, UD), \mathcal{E}(H(D, C), {-})] \tag{adjunction} \\
&amp; \cong \int_D \int_C [\mathcal{C}(C, UD), \mathcal{E}(H(D, C), {-})] \tag{Fubini} \\
&amp; \cong \int_D \mathcal{E}(H(D, UD), {-}) \tag{Yoneda} \\
&amp; \cong \mathcal{E}\left(\int^D H(D, UD), {-}\right) \tag{continuity} \\
&amp; \square
\end{align}
\]</p>
<p>Using the adjunction |\Delta \dashv \times : \mathcal{C} \times \mathcal{C}\to \mathcal{C}|
gives the following corollary.</p>
<p><strong>Corollary 2</strong>: For any |H : \mathcal{C}^{op} \times \mathcal{C}^{op} \times \mathcal{C} \to \mathcal{E}|,
\[\int^{C} H(C, C, C) \cong \int^{C_1}\int^{C_2} H(C_1, C_2, C_1 \times C_2)\] when
both exists. This allows us to combine two (co)ends into one.</p>
<p>Now our theorem.</p>
<p><strong>Theorem 3</strong>: Let |F : \mathcal{T}_1 \to \mathbf{Set}| and |J : \mathcal{T}_1 \to \mathcal{T}_2|
where |\mathcal{T}_1| and |\mathcal{T}_2| have finite products. Then |\mathrm{Lan}_J(F)|
preserves finite products if |F| does.</p>
<p><strong>Proof</strong>:
\[
\begin{flalign}
\mathrm{Lan}_J(F)(X \times Y)
&amp; \cong \int^A \mathcal{T}_2(J(A), X \times Y) \times F(A) \tag{coend formula for left Kan extension} \\
&amp; \cong \int^A \mathcal{T}_2(J(A), X) \times \mathcal{T}_2(J(A), Y) \times F(A) \tag{continuity} \\
&amp; \cong \int^{A_1}\int^{A_2}\mathcal{T}_2(J(A_1), X) \times \mathcal{T}_2(J(A_2), Y) \times F(A_1 \times A_2) \tag{Corollary 2} \\
&amp; \cong \int^{A_1}\int^{A_2}\mathcal{T}_2(J(A_1), X) \times \mathcal{T}_2(J(A_2), Y) \times F(A_1) \times F(A_2) \tag{finite product preservation} \\
&amp; \cong \left(\int^{A_1}\mathcal{T}_2(J(A_1), X) \times F(A_1) \right) \times \left(\int^{A_2}\mathcal{T}_2(J(A_2), Y) \times F(A_2)\right) \tag{commutativity and cocontinuity of $\times$} \\
&amp; \cong \mathrm{Lan}_J(F)(X) \times \mathrm{Lan}_J(F)(Y) \tag{coend formula for left Kan extension} \\
&amp; \square
\end{flalign}
\]</p>
<h2 id="the-coproduct-of-groups">The Coproduct of Groups</h2>
<p>To get general coproducts (and all colimits), we’ll show that |\mathbf{Mod}_{\mathcal{T}}|
is a <em>reflective</em> subcategory of |[\mathcal{T}, \mathbf{Set}]|. Write
|\iota : \mathbf{Mod}_{\mathcal{T}} \hookrightarrow [\mathcal{T}, \mathbf{Set}]|.
If we had a functor |R| such that |R \dashv \iota|, then we have
|R \circ \iota = Id| which allows us to quickly produce colimits
in the subcategory via |\int^I D(I) \cong R\int^I \iota D(I)|.
It’s easy to verify that |R\int^I \iota D(I)| has the appropriate universal property
to be |\int^I D(I)|.</p>
<p>We’ll compute |R| by composing two adjunctions. First,
we have |\bar{({-})} \dashv \iota({-}) \circ \sigma : \mathbf{Mod}_{\mathcal{T}^{\times}} \to [\mathcal T, \mathbf{Set}]|.
This is essentially the universal property of |\mathcal{T}^{\times}|.
When |\mathcal{T}| has finite products, which, of course, we’re assuming,
then we can use the universal property of |\mathcal{T}^{\times}| to factor |Id_{\mathcal{T}}|
into |Id = \bar{Id} \circ \sigma|. The second adjunction is then |\mathrm{Lan}_{\bar{Id}} \dashv {-} \circ \bar{Id} : \mathbf{Mod}_{\mathcal{T}} \to \mathbf{Mod}_{\mathcal{T}^{\times}}|.
The left adjoint sends finite-product-preserving functors to finite-product-preserving functors
via Theorem 3. The right adjoint is the composition of finite-product-preserving functors.</p>
<p>The composite of the left adjoints is |\iota({-} \circ \bar{Id}) \circ \sigma = \iota({-}) \circ \bar{Id} \circ \sigma = \iota({-})|.
The composite of the right adjoint is
\[
\begin{align}
R(F)
&amp; = \mathrm{Lan}_{\bar{Id}}(\bar{F}) \\
&amp; \cong \int^X \mathcal{T}(\bar{Id}(X), {-}) \times \bar{F}(X) \\
&amp; \cong \int^X \mathcal{T}\left(\prod_{i=1}^{\lvert X\rvert} X_i, {-}\right) \times \prod_{i=1}^{\lvert X \rvert} F(X_i)
\end{align}
\] where we view the list |X : \mathcal{T}^{\times}| as a |\lvert X\rvert|-tuple with components |X_i|.</p>
<p>This construction of the reflector, |R|, is quite similar to the free construction.
The main difference is that here we factor |Id| via |\mathcal{T}^{\times}| where there we factored
|\mathsf{G} : \mathbf{1} \to \mathcal{T}| via |\mathbf{1}^{\times} = \mathcal{T}_{\mathbf{Set}}|.</p>
<p>Let’s now explicitly describe the coproducts via |R|. As a warm-up, we’ll consider the initial object, i.e.
nullary coproducts. We consider |R(\Delta 0)|. Because |0 \times S = 0|, the only case in the coend
that isn’t |0| is when |\lvert X \rvert = 0| so the underlying set of the coend reduces to
|\mathcal{T}(\mathsf{G}^0, \mathsf{G}^1)|, i.e. the nullary terms. For groups, this is just the unit
element. For bounded lattices, it would be the two element set consisting of the top and bottom elements.
For lattices without bounds, it would be the empty set. Of course, |R(\Delta 0)| matches |F(0)|, i.e.
the free model on |0|.</p>
<p>Next, we consider two models |G| and |H|. First, we compute to the coproduct of |G| and |H| as (plain)
functors which is just computed pointwise, i.e. |(G+H)(\mathsf{G}^n) = G(\mathsf{G}^n)+H(\mathsf{G}^n) \cong G(\mathsf{G^1})^n + H(\mathsf{G^1})^n|.
Considering the case where |X_i = \mathsf{G}^1| for all |i| and where |\lvert X \rvert = n|, which
subsumes all the other cases, we see we have a term with |n| free variables each labelled by either
an element of |G| or an element of |H|. If we normalized the term into a list of variables representing
a product of variables, then we’d have a essentially a <strong>word</strong> as described on <a href="https://en.wikipedia.org/wiki/Free_product#Construction">the Wikipedia page
for the free product</a>. If we then only considered
quotienting by the equivalences induced by projection and tupling, we’d have the free group on the
disjoint union of the underlying sets of the |G| and |H|. However, for |R|, we quotient also by
the action of the other operations. The lists of objects with |X_i \neq \mathsf{G}^1| come in here
to support equating non-unary ops. For example, a pair of the binary term |\mathsf{m}| and
the 2-tuple of elements |(g_1, g_2)| for |g_1, g_2 \in U(G)|, will be equated with the pair of
the unary term |id| and the 1-tuple of elements |(g)| where |g = g_1 g_2| in |G|. Similarly for
|H| and the other operations (and terms generally). Ultimately, the quotient identifies every
element with an element that consists of a pair of a term that is a fully right associated
set of multiplications ending in a unit where each variable is labelled with an element from
|U(G)| or |U(H)| in an alternating fashion. These are the <strong>reduced words</strong> in the Wikipedia
article.</p>
<p>This, perhaps combined with a more explicit spelling out of the equivalence relation, should make
it clear that this construction does actually correspond to the usual free product construction.
The name “free product” is also made a bit clearer, as we are essentially building the free
group on the disjoint union of the underlying sets of the inputs, and then quotienting that to
get the result. While there are some categorical treatments of normalization, the normalization
arguments used above were not guided by the category theory. The (underlying sets of the) models
produced by the above |F| and |R| functors big equivalence classes of “terms”. The above constructions
provide no guidance for finding “good” representatives of those equivalence classes.</p>
<h2 id="conclusions">Conclusions</h2>
<p>This was, of course, a very complex and round-about way of answering the title question.
Obviously the real goal was illustrating these ideas and illustrating how “abstract” categorical
reasoning can lead to relatively “concrete” results. Of course, these concrete constructions
are derived from other concrete constructions, usually concrete constructions of limits and
colimits in |\mathbf{Set}|. That said, category theory allows you to get a lot from a small
collection of relatively simple concrete constructions. Essentially, category theory is like
a programming language with a small set of primitives. You can write “abstract” programs in
terms of that language, but once you provide an “implementation” for those primitives, all
those “abstract” programs can be made concrete.</p>
<p>I picked (finite) coproducts, in particular, as they are where a bunch of complexity suddenly
arises when studying algebraic objects categorically, but (finite) coproducts are still
fairly simple.</p>
<p>For Lawvere theories, one thing to note is that the Lawvere theory is independent of the
presentation. Any presentation of the axioms of a group would give rise to the same Lawvere
theory. Of course, to explicitly describe the category would end up requiring a presentation
of the category anyway. Beyond Lawvere theories are algebraic theories and algebraic categories,
and further into essentially algebraic theories and categories. These extend to the multi-sorted
case and then into the finite limit preserving case. The theory of categories, for example,
cannot be presented as a Lawvere theory but is an essentially algebraic theory. There’s much
more that can be said even about specifically Lawvere theories, both from a theoretical
perspective, starting with monadicity, and from practical perspectives like algebraic effects.</p>
<p>Familiarity with the properties of functor categories, and especially categories of (co)presheaves
was behind many of these results, and many that I only mentioned in passing. It is always
useful to learn more about categories of presheaves. That said, most of the theory works
in an enriched context and often without too many assumptions. The fact that all we need
to talk about models is for the codomains of the functors to have finite products allows
quite broad application. We can talk about algebraic objects almost anywhere. For example,
sheaves of rings, groups, etc. can equivalently be described as models of the theories of
rings, groups, etc. in sheaves of sets.</p>
<p>Kan extensions unsurprisingly played a large role, as they almost always do when you’re
talking about (co)presheaves. One of the motivations for me to make this article was
a happy confluence of things I was reading leading to a nice, coend calculus way of
describing and proving finite-product-preservation for free models.</p>
<p>Thinking about what <em>exactly</em> was going on around finite-product-preservation was fairly interesting.
The incorrect definition of the free model functor could be corrected in a different
(though, of course, ultimately equivalent) way. The key is to remember that the coend
formula for the left Kan extension is generally a <em>copower</em> and not a cartesian product.
The copower for |\mathbf{Set}|-valued functors is different from the copower for
finite-product-preserving |\mathbf{Set}|-valued functors. For a category with (arbitrary)
coproducts, the copower corresponds to the coproduct of a constant family. We get,
|F(S) \cong \coprod_{S} \mathcal T(\mathsf{G}^1, {-})| as is immediately
evident from |F| being a left adjoint and a set |S| being the coproduct of |1| |S|-many times.
For the purposes of this article, this would have been less than satisfying as figuring
out what coproducts were was the nominal point.</p>
<p>That said, it isn’t completely unsatisfying
as this defines the free model in terms of a coproduct of, specifically, representables
and those are more tractable. In particular, an easy and neat exercise is to work
out what |\mathcal{T}(\mathsf{G}^n, {-}) + \mathcal{T}(\mathsf{G}^m, {-})| is.
Just use Yoneda and work out what must be true of the mapping out property, and remember
that the object you’re mapping into preserves finite products. Once you have finite
coproducts described, you can get all the rest via filtered colimits, and since those
commute with finite products that gives us arbitrary coproducts.</p>]]></summary>
</entry>
<entry>
    <title>Preserving, Reflecting, and Creating Limits</title>
    <link href="https://derekelkins.github.io/posts/preserves-reflects-creates.html" />
    <id>https://derekelkins.github.io/posts/preserves-reflects-creates.html</id>
    <published>2023-03-20 22:39:28-07:00</published>
    <updated>2023-03-21T05:39:28Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>This is a brief article about the notions of preserving, reflecting, and creating limits and,
by duality, colimits. Preservation is relatively intuitive, but the distinction between
reflection and creation is subtle.</p>
<h2 id="preservation-of-limits">Preservation of Limits</h2>
<p>A functor, |F|, <strong>preserves limits</strong> when it takes limiting cones to limiting cones. As
often happens in category theory texts, the notation focuses on the objects. You’ll often
see things like |F(X \times Y) \cong FX \times FY|, but implied is that one direction of
this isomorphism is the canonical morphism |\langle F\pi_1, F\pi_2\rangle|. To put it yet
another way, in this example we require |F(X \times Y)| to satisfy the universal property
of a product with the projections |F\pi_1| and |F\pi_2|.</p>
<p>Other than that subtlety, preservation is fairly intuitive.</p>
<h2 id="reflection-of-limits-versus-creation-of-limits">Reflection of Limits versus Creation of Limits</h2>
<p>A functor, |F|, <strong>reflects limits</strong> when whenever the image of a <em>cone</em> is a limiting cone,
then the original cone was a limiting cone. For products this would mean that if we
had a wedge |A \stackrel{p}{\leftarrow} Z \stackrel{q}{\to} B|, and |FZ| was the product
of |FA| and |FB| with projections |Fp| and |Fq|, then |Z| was the product of |A| and |B|
with projections |p| and |q|.</p>
<p>A functor, |F|, <strong>creates limits</strong> when whenever the image of a <em><strong>diagram</strong> has a limit</em>,
then the diagram itself has a limit and |F| preserves the limiting cones. For products
this would mean if |FX| and |FY| had a product, |FX \times FY|, then |X| and |Y| have
a product and |F(X \times Y) \cong FX \times FY| via the canonical morphism.</p>
<p>Creation of limits implies reflection of limits since we can just ignore the apex of the
cone. While creation is more powerful, often reflection is enough in practice as we usually
have a candidate limit, i.e. a cone. Again, this is often not made too explicit.</p>
<h3 id="example">Example</h3>
<p>Consider the posets:</p>
<p><span class="math display">$$\xymatrix{
     &amp;          &amp;                   &amp; c \\
X\ar@{}[r]|{\Large{=}} &amp; a \ar[r] &amp; b \ar[ur] \ar[dr] &amp;   \\
     &amp;          &amp;                   &amp; d 
\save "1,2"."3,4"*+[F]\frm{}
\restore
} \qquad \xymatrix{
     &amp;                   &amp; c \\
Y\ar@{}[r]|{\Large{=}} &amp; b \ar[ur] \ar[dr] &amp;   \\
     &amp;                   &amp; d 
\save "1,2"."3,3"*+[F]\frm{}
\restore
} \qquad \xymatrix{
     &amp; c \\ 
Z\ar@{}[r]|{\Large{=}} &amp;   \\ 
     &amp; d
\save "1,2"."3,2"*+[F]\frm{}
\restore
}$$</span></p>
<h4 id="failure-of-reflection">Failure of reflection</h4>
<p>Let |X=\{a, b, c, d\}| with |a \leq b \leq c| and |b \leq d| mapping to |Y=\{b, c, d\}|
where |a \mapsto b|. Reflection fails because |a| maps to a meet but is not itself a meet.</p>
<h4 id="failure-of-creation">Failure of creation</h4>
<p>If we change the source to just |Z=\{c, d\}|, then creation fails because |c| and |d| have a meet
in the image but not in the source. Reflection succeeds, though, because there are no
non-trivial cones in the source, so every cone (trivially) gets mapped to a limit cone.
It’s just that we don’t have any cones with both |c| and |d| in them.</p>
<p>In general, recasting reflection and creation of limits for posets gives us: Let |F: X \to Y| be
a monotonic function. |F| reflects limits if every lower bound that |F| maps to a meet is
already a meet. |F| creates limits if whenever |F[U]| has a meet for |U \subseteq X|, then |U|
already had a meet and |F| sends the meet of |U| to the meet of |F[U]|.</p>]]></summary>
</entry>
<entry>
    <title>Overlaps</title>
    <link href="https://derekelkins.github.io/posts/overlaps.html" />
    <id>https://derekelkins.github.io/posts/overlaps.html</id>
    <published>2021-01-05 19:46:59-08:00</published>
    <updated>2021-01-06T03:46:59Z</updated>
    <summary type="html"><![CDATA[<p><strong>tl;dr</strong> The notion of two sets overlapping is very common. Often it is expressed
via |A \cap B \neq \varnothing|. Constructively, this is not the best definition
as it does not imply |\exists x. x \in A \land x \in B|. Even classically, this
second-class treatment of overlapping obscures important and useful connections.
In particular, writing |U \between A| for “|U| overlaps |A|”, we have a De Morgan-like
duality situation with |\between| being dual to |\subseteq|. Recognizing and
exploiting this duality, in part by using more appropriate notation for “overlaps”,
can lead to new concepts and connections.</p>
<h3 id="introduction">Introduction</h3>
<p>The most common way I’ve seen the statement “|A| overlaps |B|” formalized
is |A \cap B \neq \varnothing|. To a constructivist, this definition isn’t
very satisfying. In particular, this definition of overlaps does not allow
us to constructively conclude that there exists an element contained in both
|A| and |B|. That is, |A \cap B \neq \varnothing| does not imply
|\exists x. x \in A \land x \in B| constructively.</p>
<p>As is usually the case, even if you are not philosophically a constructivist,
taking a constructivist perspective can often lead to better definitions and
easier to see connections. In this case, constructivism suggests the more positive
statement |\exists x. x \in A \land x \in B| be the definition of “overlaps”.
However, given that we now have two (constructively) non-equivalent definitions,
it is better to introduce notation to abstract from the particular definition. In
many cases, it makes sense to have a primitive notion of “overlaps”. Here I will
use the notation |A \between B| which is the most common option I’ve seen.</p>
<h3 id="properties">Properties</h3>
<p>We can more compactly write the quantifier-based definition as |\exists x \in A.x \in B|
using a common set-theoretic abbreviation. This presentation suggests a perhaps surprising
connection. If we swap the quantifier, we get |\forall x\in A.x \in B| which is commonly
abbreviated |A \subseteq B|. This leads to a duality between |\subseteq| and |\between|,
particularly in topological contexts. In particular, if we pick a containing set |X|,
then |\neg(U \between A) \iff U \subseteq A^c| where the complement is relative to |X|,
and |A| is assumed to be a subset of |X|. This is a De Morgan-like duality.</p>
<p>If we want to characterize these operations via an adjunction, or, more precisely, a Galois
connection, we have a slight awkwardness arising from |\subseteq| and |\between| being
binary predicates on sets. So, as a first step we’ll identify sets with predicates via, for
a set |A|, |\underline A(x) \equiv x \in A|. In terms of predicates, the adjunctions we
want are just a special case of the adjunctions characterizing the quantifiers.</p>
<p>\[\underline U(x) \land P \to \underline A(x) \iff P \to U \subseteq A\]</p>
<p>\[U \between B \to Q \iff \underline B(x) \to (\underline U(x) \to Q)\]</p>
<p>What we actually want is a formula of the form |U \between B \to Q \iff B \subseteq (\dots)|.
To do this, we need an operation that will allow us to produce a set from a predicate. This is
exactly what set comprehension does. For reasons that will become increasingly clear, we’ll
assume that |A| and |B| are subsets of a set |X|. We will then consider quantification relative
to |X|. The result we get is:</p>
<p>\[\{x \in U \mid P\} \subseteq A \iff \{x \in X \mid x \in U \land P\} \subseteq A \iff P \to U \subseteq A\]</p>
<p>\[U \between B \to Q \iff B \subseteq \{x \in X \mid x \in U \to Q\} \iff B \subseteq \{x \in U \mid \neg Q\}^c\]</p>
<p>The first and last equivalences require additionally assuming |U \subseteq X|.
The last equivalence requires classical reasoning. You can already see motivation to
limit to subsets of |X| here. First, set complementation, the |(-)^c|, only makes sense relative to
some containing set. Next, if we choose |Q \equiv \top|, then the latter formulas
state that <em>no matter what |B| is</em> it should be a subset of the expression that
follows it. Without constraining to subsets of |X|, this would require a universal
set which doesn’t exist in typical set theories.</p>
<p>Choosing |P| as |\top|, |Q| as |\bot|, and |B| as |A^c| leads to the
familiar |\neg (U \between A^c) \iff U \subseteq A|, i.e. |U| is a
subset of |A| if and only if it doesn’t overlap |A|’s complement.</p>
<p>Incidentally, characterizing |\subseteq| and |\between| in terms of Galois
connections, i.e. adjunctions, immediately gives us some properties for free via continuity.
We have |U \subseteq \bigcap_{i \in I}A_i \iff \forall i\in I.U \subseteq A_i|
and |U \between \bigcup_{i \in I}A_i \iff \exists i \in I.U \between A_i|. This
is relative to a containing set |X|, so |\bigcap_{i \in \varnothing}A_i = X|, and |U|
and each |A_i| are assumed to be subsets of |X|.</p>
<h3 id="categorical-perspective">Categorical Perspective</h3>
<p>Below I’ll perform a categorical analysis of the situation. I’ll mostly be using categorical
notation and perspectives to manipulate normal sets. That said, almost all of what I say will
be able to be generalized immediately just by reinterpreting the symbols.</p>
<p>To make things a bit cleaner in the future, and to make it easier to apply these ideas
beyond sets, I’ll introduce the concept of a <a href="https://en.wikipedia.org/wiki/Heyting_algebra">Heyting algebra</a>.
A Heyting algebra is a partially ordered set |H| satisfying the following:</p>
<ol type="1">
<li>|H| has two elements called |\top| and |\bot| satisfying for all |x| in |H|, |\bot \leq x \leq \top|.</li>
<li>We have operations |\land| and |\lor| satisfying for all |x|, |y|, |z| in |H|,
|x \leq y \land z| if and only |x \leq y| and |x \leq z|, and similarly for |\lor|,
|x \lor y \leq z| if and only |x \leq z| and |y \leq z|.</li>
<li>We have an operation |\to| satisfying for all |x|, |y|, and |z| in |H|,
|x \land y \leq z| if and only if |x \leq y \to z|.</li>
</ol>
<p>For those familiar with category theory, you might recognize this as simply the decategorification
of the notion of a <a href="https://ncatlab.org/nlab/show/bicartesian+closed+category">bicartesian closed category</a>.
We can define the <strong>pseudo-complement</strong>, |\neg x \equiv x \to \bot|.</p>
<p>Any <a href="https://en.wikipedia.org/wiki/Boolean_algebra_(structure)">Boolean algebra</a> is an
example of a Heyting algebra where we can define |x \to y| via |\neg x \lor y| where
here |\neg| is taken as primitive. In particular, subsets of a given set ordered by
inclusion form a Boolean algebra, and thus a Heyting algebra. The |\to| operation can also
be characterized by |x \leq y \iff (x \to y) = \top|. This lets us immediately see
that for subsets of |X|, |(A \to B) = \{x \in X \mid x \in A \to x \in B\}|. All
this can be generalized to the subobjects in any <a href="https://ncatlab.org/nlab/show/Heyting+category">Heyting category</a>.</p>
<p>As the notation suggests, intuitionistic logic (and thus classical logic) is another
example of a Heyting algebra.</p>
<p>We’ll write |\mathsf{Sub}(X)| for the partially ordered set of subsets of |X| ordered
by inclusion. As mentioned above, this is (classically) a Boolean algebra and thus a
Heyting algebra. Any function |f : X \to Y| gives a monotonic function
|f^* : \mathsf{Sub}(Y) \to \mathsf{Sub}(X)|. Note the swap. |f^*(U) \equiv f^{-1}(U)|.
(Alternatively, if we think of subsets in terms of characteristic functions, |f^*(U) \equiv U \circ f|.)
Earlier, we needed a way to turn predicates into sets. In this case, we’ll go the other way
and identify truth values with subsets of |1| where |1| stands for an arbitrary singleton set.
That is, |\mathsf{Sub}(1)| is the poset of truth values. |1| being the terminal object of |\mathbf{Set}|
induces the (unique) function |!_U : U \to 1| for any set |U|. This leads to the important
monotonic function |!_U^* : \mathsf{Sub}(1) \to \mathsf{Sub}(U)|. This can be described
as |!_U^*(P) = \{x \in U \mid P\}|. Note, |P| cannot contain |x| as a free variable.
In particular |!_U^*(\bot) = \varnothing| and |!_U^*(\top) = U|. This monotonic function
has left and right adjoints:</p>
<p>\[\exists_U \dashv {!_U^*} \dashv \forall_U : \mathsf{Sub}(U) \to \mathsf{Sub}(1)\]</p>
<p>|F \dashv G| for monotonic functions |F : X \to Y| and |G : Y \to X|
means |\forall x \in X. \forall y \in Y.F(x) \leq_Y y \iff x \leq_X G(y)|.</p>
<p>|\exists_U(A) \equiv \exists x \in U. x \in A| and |\forall_U(A) \equiv \forall x \in U. x \in A|.
It’s easily verified that each of these functions are monotonic.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>It seems like we should be done. These formulas are the formulas I originally gave for
|\between| and |\subseteq| in terms of quantifiers. The problem here is that these functions
are only defined for subsets of |U|. This is especially bad for interpreting |U \between A|
as |\exists_U(A)| as it excludes most of the interesting cases where |U| partially overlaps |A|.
What we need is a way to extend |\exists_U| / |\forall_U| beyond subsets of |U|. That is,
we need a suitable monotonic function |\mathsf{Sub}(X) \to \mathsf{Sub}(U)|.</p>
<p>Assume |U \subseteq X| and that we have an inclusion |\iota_U : U \hookrightarrow X|.
Then |\iota_U^* : \mathsf{Sub}(X) \to \mathsf{Sub}(U)| and |\iota_U^*(A) = U \cap A|.
This will indeed allow us to define |\subseteq| and |\between| as |U \subseteq A \equiv \forall_U(\iota_U^*(A))|
and |U \between A \equiv \exists_U(\iota_U^*(A))|. We have:</p>
<p>\[\iota_U[-] \dashv \iota_U^* \dashv U \to \iota_U[-] : \mathsf{Sub}(U) \to \mathsf{Sub}(X)\]</p>
<p>Here, |\iota_U[-]| is the direct image of |\iota_U|. This doesn’t really do anything in this case
except witness that if |A \subseteq U| then |A \subseteq X| because |U \subseteq X|.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>We can recover the earlier adjunctions by simply using these two pairs of adjunctions.
\[\begin{align}
U \between B \to Q
&amp; \iff \exists_U(\iota_U^*(B)) \to Q \\
&amp; \iff \iota_U^*(B) \subseteq {!}_U^*(Q) \\
&amp; \iff B \subseteq U \to \iota_U[{!}_U^*(Q)] \\
&amp; \iff B \subseteq \{x \in X \mid x \in U \to Q\}
\end{align}\]</p>
<p>Here the |\iota_U[-]| is crucial so that we use the |\to| of |\mathsf{Sub}(X)|
and not |\mathsf{Sub}(U)|.</p>
<p>\[\begin{align}
P \to U \subseteq A
&amp; \iff P \to \forall_U(\iota_U^*(A)) \\
&amp; \iff {!}_U^*(P) \subseteq \iota_U^*(A) \\
&amp; \iff \iota_U[{!}_U^*(P)] \subseteq A \\
&amp; \iff \{x \in X \mid x \in U \land P\} \subseteq A
\end{align}\]</p>
<p>In this case, the |\iota_U[-]| is truly doing nothing because |\{x \in X \mid x \in U \land P\}|
is the same as |\{x \in U \mid P\}|.</p>
<p>While we have |{!}_U^* \circ \exists_U \dashv {!}_U^* \circ \forall_U|, we
see that the inclusion of |\iota_U^*| is what breaks the direct connection between
|U \between A| and |U \subseteq A|.</p>
<h3 id="examples">Examples</h3>
<p>As a first example, write |\mathsf{Int}A| for the <strong>interior</strong> of |A| and |\bar A| for the <strong>closure</strong> of |A|
each with respect to some <a href="https://en.wikipedia.org/wiki/Topological_space#Definition_via_open_sets">topology</a>
on a containing set |X|.
One way to define |\mathsf{Int}A| is |x \in \mathsf{Int}A| if and only if there exists an open set
containing |x| that’s a subset of |A|. Writing |\mathcal O(X)| for the set of open sets, we
can express this definition in symbols:
\[x \in \mathsf{Int}A \iff \exists U \in \mathcal O(X). x \in U \land U \subseteq A\]
We have a “dual” notion:
\[x \in \bar A \iff \forall U \in \mathcal O(X). x \in U \to U \between A\]
That is, |x| is in the closure of |A| if and only if every open set containing |x| overlaps |A|.</p>
<p>As another example, here is a fairly unusual way of characterizing a compact subset |Q|.
|Q| is <strong>compact</strong> if and only if |\{U \in \mathcal O(X) \mid Q \subseteq U\}| is open
in |\mathcal O(X)| equipped with the <a href="https://ncatlab.org/nlab/show/Scott+topology">Scott topology</a><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.
As before, this suggests a “dual” notion characterized by |\{U \in \mathcal O(X) \mid O \between U\}|
being an open subset. A set |O| satisfying this is called <a href="https://ncatlab.org/nlab/show/overt+space"><strong>overt</strong></a>.
This concept is never mentioned in traditional presentations of point-set topology because <em>every</em>
subset is overt. However, if we don’t require that <em>arbitrary</em> unions of open sets are open (and only require
finite unions to be open) as happens in <a href="https://ncatlab.org/nlab/show/synthetic+topology">synthetic topology</a>
or if we aren’t working in a classical context then overtness becomes a meaningful concept.</p>
<p>One benefit of the intersection-based definition of overlaps is that it is
straightforward to generalize to many sets overlapping, namely |\bigcap_{i\in I} A_i \neq \varnothing|.
This is also readily expressible using quantifiers as: |\exists x.\forall i \in I. x \in A_i|.
As before, having an explicit “universe” set also clarifies this. So,
|\exists x \in X.\forall i \in I. x \in A_i| with |\forall i \in I. A_i \subseteq X| would
be better. The connection of |\between| to |\subseteq| suggests instead of this fully
symmetric presentation, it may still be worthwhile to single out a set producing
|\exists x \in U.\forall i \in I. x \in A_i| where |U \subseteq X|. This can be
read as “there is a point in |U| that touches/meets/overlaps every |A_i|”.
If desired we could notate this as |U \between \bigcap_{i \in I}A_i|. Negating and
complementing the |A_i| leads to the dual notion |\forall x \in U.\exists i \in I.x \in A_i|
which is equivalent to |U \subseteq \bigcup_{i \in I}A_i|. This dual notion could
be read as “the |A_i| (jointly) cover |U|” which is another common and important concept
in mathematics.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Ultimately, the concept of two (or more) sets overlapping comes up quite often. The usual
circumlocution, |A \cap B \neq \varnothing|, is both notationally and conceptually clumsy.
Treating overlapping as a first-class notion via notation and formulating definitions in terms
of it can reveal some common and important patterns.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>If one wanted to be super pedantic, I should technically write something like
|\{\star \mid \exists x \in U. x \in A\}| where |1 = \{\star\}|
because elements of |\mathsf{Sub}(1)| are subsets of |1|. Instead, we’ll conflate subsets
of |1| and truth values.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>If we think
of subobjects as (equivalence classes of) monomorphisms as is typical in category theory,
then because |\iota_U| is itself a monomorphism, the direct image, |\iota_U[-]|, is simply
post-composition by |\iota_U|, i.e. |\iota_U \circ {-}|.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The
Scott topology is the natural topology on the space of continuous functions |X \to \Sigma| where
|\Sigma| is the <a href="https://en.wikipedia.org/wiki/Sierpi%C5%84ski_space">Sierpinski space</a>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Complex-Step Differentiation</title>
    <link href="https://derekelkins.github.io/posts/complex-step-differentiation.html" />
    <id>https://derekelkins.github.io/posts/complex-step-differentiation.html</id>
    <published>2020-08-08 22:28:55-07:00</published>
    <updated>2020-08-09T05:28:55Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Complex-step differentiation is a simple and effective technique for numerically differentiating a(n analytic) function.
Discussing it is a neat combination of complex analysis, numerical analysis, and ring theory. We’ll see that it is very
closely connected to forward-mode automatic differentiation (FAD). For better or worse, while widely applicable, the scenarios
where complex-step differentiation is the <em>best</em> solution are a bit rare. To apply complex-step differentiation, you need
a version of your desired function that operates on complex numbers. If you have that, then you can apply complex-step
differentiation immediately. Otherwise, you need to adapt the function to complex arguments. This can be done essentially
automatically using the same techniques as automatic differentiation, but at that point you might as well use automatic
differentiation. Adapting the code to complex numbers or AD takes about the same amount of effort, however, the AD version
will be more efficient, more accurate, and easier to use.</p>
<p>Nevertheless, this serves as a simple example to illustrate several theoretical and practical ideas.</p>
<!--more-->
<h2 id="numerical-differentiation">Numerical Differentiation</h2>
<p>The problem we’re solving is given a function |f : \mathbb R \to \mathbb R| which is differentiable around a point |x_0|,
we’d like to compute its derivative |f’| at |x_0|. In many cases, |f| is <a href="https://en.wikipedia.org/wiki/Analytic_function">real analytic</a>
at the point |x_0| meaning |f| has a <a href="https://en.wikipedia.org/wiki/Taylor_series#Definition">Taylor series</a> which converges
to |f| in some open interval containing |x_0|.</p>
<p>The most obvious way of numerically differentiating |f| is to approximate the limit in the definition of
the derivative, \[f’(x) = \lim_{h\to 0} [f(x + h) - f(x)] / h\] by simply choosing a small value for |h| rather
than taking the limit. When |f| is real analytic at |x|, we can analyze the quality of this approximation by expanding |f(x + h)|
in a Taylor series at |x|. This produces \[[f(x + h) - f(x)]/h = f’(x) + O(h)\] A slight tweak produces a better result with the
same number of evaluations of |f|. Specifically, the Taylor series of |f(x + h) - f(x - h)| at |x| is equal to the
odd part the Taylor series of |f(x + h)| at |x|. This leads to the <strong>Central Differences</strong> formula:</p>
<p><span class="math display">$$f'(x) + O(h^2) = \frac{f(x + h) - f(x - h)}{2h}$$</span></p>
<p>The following interactively illustrates this using the function
<select style="width: 2em;" id="functionSelector">
<option value="0" selected>1</option>
<option value="1">2</option>
<option value="2">3</option>
<option value="3">4</option>
<option value="4">5</option>
</select> <span id="ex1">|f(x) = x^{9/2}|</span><span id="sin" style="display: none;">|f(x) = \sin(x)|</span><span id="exp" style="display: none;">|f(x) = e^x|</span><span id="ex2" style="display: none;">|f(x) = e^x/(\sin(x)^3 + \cos(x)^3)|</span> evaluated at |x_0 =| <span class="point"></span>.
The correct answer to |17| digits is |f’(|<span class="point"></span>|) {}={}|<span id="correct"></span>. The slider ranges from |h=10^{-2}| to |h=10^{-20}|.</p>
<p><input id="realInput" type="range" min="2" max="20"><br />
|h|: <span id="realH"></span><br />
|f’(|<span class="point"></span>|)|: <span id="real"></span><br />
error: <span id="realError"></span></p>
<p>If you play with the slider using the first example, you’ll see that the error decreases until around |10^{-5}| after which it starts
increasing until |10^{-15}| where it is off by more than |1|. At |10^{-16}| the estimated derivative is |0| which is, of course,
completely incorrect. Even at |10^{-5}| the error is on the order of |10^{-9}| which is much higher than the
double precision floating point machine epsilon of approximately |10^{-16}|.</p>
<p>There are two issues here. First, we have the issue that if |x_0 \neq 0|, then |x_0 + h = x_0| for sufficiently small |h|.
This happens when |x_0/h| has a magnitude of around |10^{16}| or more.</p>
<p>The second issue here is known as catastrophic cancellation. For simplicity, let’s say |f(x)=1|. (It’s actually about |6.2| for the
first example.) Let’s further say for some small value of |h|, |f(x+h) = 1.00000000000020404346|. The value we care about is the
|0.00000000000020404346|, but given limited precision, we might have |f(x + h) = 1.000000000000204|, meaning we only
have three digits of precision for the value we care about. Indeed, as |h| becomes smaller we’ll lose more and more
precision in our desired value until we lose all precision which happens when |f(x + h) = f(x)|. It is generally
a bad idea numerically to calculate a small value by subtracting two larger values for this reason.</p>
<p>We would not have the first issue if |x_0 = 0| as in the second and fourth examples (|f(x) = e^x|).
We would not have the second issue if |f(x) = 0| as in the second and third examples (|f(x) = \sin(x)| with |x_0 = \pi|).
We have neither issue in the second example of |f(x) = \sin(x)| with |x_0 = 0|.
This will become important later.</p>
<p>We have a dilemma. For the theory, we want as small a value of |h| as possible without being zero. In practice, we start
losing precision as |h| gets smaller, and generally larger values of |h| are going to be less impacted by this.</p>
<p>Let’s set this aside for now and look at other ways of numerically computing the derivative in the hopes that
we can avoid this problem.</p>
<h2 id="cauchys-residue-theorem">Cauchy’s Residue Theorem</h2>
<p>If we talk about functions |f : \mathbb C \to \mathbb C|, the analogue of real analyticity
is <a href="https://en.wikipedia.org/wiki/Holomorphic_function">holomorphicity</a> or <a href="https://en.wikipedia.org/wiki/Holomorphic_functions_are_analytic">complex analyticity</a>.
A complex function is <strong>holomorphic</strong> if it satisfies the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations">Cauchy-Riemann equations</a>.
(See the <a href="#appendix">Appendix</a> for more details about where the Cauchy-Riemann equations come from.)
A complex function is <strong>complex analytic</strong> if it has a Taylor series which converges to the function. It can be proven
that these two descriptions are equivalent, though this isn’t a trivial fact. We can also talk about functions that
are holomorphic or complex analytic on an open subset of |\mathbb C| and at a point by considering an open subset around
that point. The typical choice of open subset is some suitably small open disk in the complex plane about the point.
(Other common domains are ellipses, infinite strips, and areas bounded by <a href="https://en.wikipedia.org/wiki/Hankel_contour">Hankel contours</a>
and variations such as sideways opening parabolas.)</p>
<p>A major fact about holomorphic functions is the <a href="https://en.wikipedia.org/wiki/Cauchy%27s_integral_theorem">Cauchy integral theorem</a>.
If |f| is a holomorphic function inside a (suitably nice) closed curve |\Gamma| in the complex plane, then |\oint_\Gamma f(z)\mathrm dz = 0|.
Again, |\Gamma| will typically be chosen to be some circle. (Integrals like this in the complex plane are often called
<strong>contour integrals</strong> and the curves we’re integrating along are called <strong>contours</strong>.)</p>
<p>Things get really interesting when we generalize to <a href="https://en.wikipedia.org/wiki/Meromorphic_function">meromorphic functions</a>
which are complex functions that are holomorphic except at an isolated set of points. These take the form of <a href="https://en.wikipedia.org/wiki/Pole_(complex_analysis)"><strong>poles</strong></a>
which are points |z_0| such that |1/f(z_0) = 0|, i.e. poles are where a function goes to infinity as, e.g., |1/z| does at |0|.
The generalization of Cauchy’s integral theorem is <a href="https://en.wikipedia.org/wiki/Residue_theorem">Cauchy’s Residue Theorem</a>. <em>This theorem
is surprising and is one of the most useful theorems in all of mathematics both theoretically and practically</em>.</p>
<p>We’ll only need a common special case of it. Let |f| be a holomorphic function, then |f(z)/(z - z_0)^n| is a meromorphic function
with a single pole of order |n| at |z_0|. If |\Gamma| is a positively oriented, simple closed curve containing |z_0|,
then <span class="math display">$$f^{(n-1)}(z_0) = \frac{(n-1)!}{2 \pi i}\oint_{\Gamma} \frac{f(z)\mathrm dz}{(z - z_0)^n}$$</span> In this case, |f^{(n-1)}(z_0)/(n-1)!|
is the <strong>residue</strong> of |f(z)/(z - z_0)^n| at |z_0|. More generally, if there are multiple poles in the area bounded by |\Gamma|, then we will
sum up their residues.</p>
<p>This formula provides us a means of calculating the |(n-1)|-st Taylor coefficient of a complex analytic function at any point.
For our particular purposes, we’ll only need the |n=2| case, \[f’(z_0) = \frac{1}{2 \pi i}\oint_{\Gamma} \frac{f(z)\mathrm dz}{(z - z_0)^2}\]</p>
<p>For the remainder of this section I want to give some examples of how Cauchy’s Residue Theorem is used both theoretically
and practically. This whole article will itself be another practical example of Cauchy’s Residue Theorem. This is not exhaustive
by any means.</p>
<p>To start illustrating some of the surprising properties of this theorem, we can take the |n=1| case which states that we can evaluate
a holomorphic function at any point via |f(z_0) = \frac{1}{2 \pi i}\oint_{\Gamma} \frac{f(z)\mathrm dz}{z - z_0}| where |\Gamma|
is any contour which bounds an area containing |z_0|. This leads to an interesting discreteness. Not only can we evaluate
a (holomorphic) function (or any of its derivatives) at a point via the values of the function on a contour, the only significant
constraint on that contour is that it bound an area containing the desired point. In other words, no matter how we deform the contour the
integral is constant except when we deform the contour so as not to bound an area containing the point being evaluated, at which point
the integral’s value is |0|<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. It may seem odd to use an integral to evaluate
a function at a point, but it can be useful when there are numerical issues with evaluating the function near the desired point<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. In fact, these results show that if we know the values of a holomorphic function on the boundary of a given open subset of
the complex plane, then we know the value of the function <em>everywhere</em>. In this sense, holomorphic functions (and analytic
functions in general) are extremely rigid.</p>
<p>This leads to the notion of <a href="https://en.wikipedia.org/wiki/Analytic_continuation">analytic continuation</a>
where we try to compute an analytic function beyond its overt domain of definition. This is the basis of most “sums of divergent series”.
For example, there is the first-year calculus fact that the sum of the infinite series |\sum_{n=0}^\infty x^n| is |1/(1-x)| converging
on the interval |x \in (-1,1)|. In fact, the proof of convergence only needs |\|x\| &lt; 1| so we can readily generalize to
complex |z| with |\|z\| &lt; 1|, i.e. |z| contained in the open unit disk. However, |1/(1-z)| is a meromorphic function that is holomorphic
everywhere except for |z=1|, therefore there is a <em>unique</em> analytic function defined everywhere except |z=1| that agrees with
the infinite sum on the unit disk, namely |1/(1-z)| itself. Choosing |z=2| leads to the common example of “summing a divergent series”
with “|\sum_{n=0}^\infty 2^n = -1|” which really means “the value at |2| of the unique complex analytic function which agrees
with this infinite series when it converges”.</p>
<p>Sticking with just evaluation, applying the Cauchy Residue theorem to quadrature, i.e. numerical integration, leads to an interesting
connection to a rational approximation problem. Say we want to compute |\int_{-1}^1 f(x) \mathrm dx|, we can use the Cauchy
integral to evaluate |f(x)| leading to <span class="math display">$$\int_{-1}^1 f(x) \mathrm dx
= \int_{-1}^1 \frac{1}{2\pi i}\oint \frac{f(z)\mathrm dz}{z - x}\mathrm dx
= \frac{1}{2\pi i}\oint f(z)\int_{-1}^1 \frac{\mathrm dx}{z - x}\mathrm dz
= \frac{1}{2\pi i}\oint f(z)\log\left(\frac{z+1}{z-1}\right)\mathrm dz$$</span>
A quadrature formula looks like |\int_{-1}^1 f(x) \mathrm dx \approx \sum_{k=1}^N w_k f(x_k)|. The sum
can be written as a Cauchy integral of |\oint f(z)\sum_{k=1}^N \frac{1}{2\pi i}\frac{w_k\mathrm dz}{z - x_k}|. We thus have
<span class="math display">$$\left|\frac{1}{2\pi i}\oint f(z)\left[\log\left(\frac{z+1}{z-1}\right) - \sum_{k=1}^N \frac{w_k}{z - x_k}\right]\mathrm dz\right|$$</span> as the error of
the approximation. The sum is a rational function (in partial fraction form) and thus the error is minimized by points (|x_k|)
and weights (|w_k|) that lead to better rational approximations of |\log((z+1)/(z-1))|<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>The ability to calculate coefficients of the Taylor series of a holomorphic function is, by itself, already a valuable
tool in both theory and practice. In particular, the coefficients of a <a href="https://en.wikipedia.org/wiki/Generating_function">generating function</a>
or a <a href="https://en.wikipedia.org/wiki/Z-transform">Z-transform</a> can be computed with Cauchy integrals. This has applications
in probability theory, statistics, finance, combinatorics, recurrences, differential equations, and signal processing.
Indeed, when |z_0 = 0| and |\Gamma| is the unit circle, then the Cauchy integral is a component of the <a href="https://en.wikipedia.org/wiki/Fourier_series#Complex-valued_functions">Fourier series</a>
of |f|. Approximating these integrals with the Trapezoid Rule (which we’ll discuss in a bit) produces the <a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform">Discrete Fourier Transform</a>.</p>
<p>Let |p| be a polynomial and, for simplicity, assume all its zeroes are of multiplicity one. Then |1/p(z)| is a meromorphic function
that’s holomorphic everywhere except for the roots of |p|. The Cauchy integral |\frac{1}{2\pi i}\oint_{\Gamma} \frac{p’(z)\mathrm dz}{p(z)}|
counts the number of roots of |p| contained in the area bounded by |\Gamma|. If we know there is only one root of |p| within
the area bounded by |\Gamma|, then we can compute that root with |\frac{1}{2\pi i}\oint_{\Gamma} \frac{z p’(z)\mathrm dz}{p(z)}|.
A better approach is to use the formulas |\left(\oint \frac{z\mathrm dz}{p(z)}\right)/\left(\oint \frac{\mathrm dz}{p(z)}\right)|.
Similar ideas can be used to adapt this to counting and finding multiple roots. See
<a href="https://doi.org/10.1137/130931035">Numerical Algorithms based on Analytic Function Values at Roots of Unity</a> by Austin, Kravanja, and
Trefethen (2014) which is a good survey in general.</p>
<p>Another very common use of Cauchy’s Residue Theorem is to sum (convergent) infinite series. |\tan(\pi z)/\pi| has a zero at
|z = k| for each integer |k| and a non-zero derivative at those points. In fact, the derivative is |1|. Alternatively,
we could use |\sin(\pi z)/\pi| which has a zero at |z = k| for each integer |k| but has derivative |(-1)^k| at those points.
Therefore, |\pi\cot(\pi z) = \pi/\tan(\pi z)| has a (first-order) pole at |z = k| for each integer |k|
with residue |1|. In particular, if |f| is a holomorphic function (at least near the real axis), then the value of the
Cauchy integral of |f(z)\pi\cot(\pi z)| along a Hankel contour will be |2\pi i \sum_{k=0}^\infty f(k)|. Along an infinite strip
around the real axis we’d get |2 \pi i \sum_{k=-\infty}^\infty f(k)|. As an example, we can consider the
<a href="https://en.wikipedia.org/wiki/Basel_problem">famous sum</a>, |\sum_{k=1}^\infty 1/k^2|. It can be shown that if |f| is a meromorphic
function whose poles are not located at integers and |\vert zf(z)\vert| is bounded for sufficiently large |\vert z\vert|, then |\oint f(z)\pi \cot(\pi z)\mathrm dz = 0|. We thus have
that \[\sum_{k=-\infty}^{\infty} f(k) = -\sum_j \mathrm{Res}(f(z)\pi\cot(\pi z); z_j)\] where |z_j| are the poles of |f|.
In particular, |f(z) = \frac{1}{z^2 + a^2}| has (first-order) poles at |\pm ai|. This gives us simply
<span class="math display">$$\sum_{k=-\infty}^{\infty} \frac{1}{k^2 + a^2} = -\pi\frac{\cot(\pi a i)-\cot(-\pi a i)}{2ai} = \frac{\pi}{a}\coth(\pi a)$$</span>
where I’ve used |\coth(x) = i\cot(xi)| and the fact that |\coth| is an odd function. Exploiting the symmetry of the sum
gives us <span class="math display">$$\sum_{k=1}^{\infty} \frac{1}{k^2 + a^2} = \frac{\pi}{2a}\coth(\pi a) - \frac{1}{2a^2}$$</span> By expanding |\coth|
in a Laurent series, we see that the limit of the right-hand side as |a| approaches |0| is |\frac{\pi^2}{6}|. While contour
integration is quite effective for coming up with analytic solutions to infinite sums, numerically integrating the contour
integrals is also highly effective as illustrated in <a href="https://doi.org/10.1007/s10543-006-0077-9">Talbot quadratures and rational approximations</a>
by Trefethen, Weideman, and Schmelzer (2006), for example.</p>
<h2 id="computing-the-integrals">Computing the Integrals</h2>
<p>We’ve seen in the previous section that |f’(z_0) = \frac{1}{2\pi i}\oint_{\Gamma} \frac{f(z)\mathrm dz}{(z-z_0)^2}|.
This doesn’t much help us if we don’t have a way to compute the integrals. From this point forward, fix |\Gamma|
as a circle of radius |h| centered on |z_0|.</p>
<p>Before that, let’s consider numerical integration in general. Say we want to integrate the real function |f| from |0| to |b|,
i.e. we want to calculate |\int_0^b f(x)\mathrm dx|. The most obvious way to go about it is to approximate the Riemann
sums that define the (Riemann) integral. This would produce a formula like
|\int_0^b f(x)\mathrm dx \approx \frac{b}{N}\sum_{k=0}^{N-1} f(bk/N)| corresponding to summing the areas of rectangles
whose left points are the values of |f|. As before with central differences, relatively minor tweaks will give better approximations. In particular,
we get the two roughly equivalent approximations of the <a href="https://en.wikipedia.org/wiki/Midpoint_rule#Midpoint_rule"><strong>Midpoint Rule</strong></a>
\[\int_0^b f(x)\mathrm dx \approx \frac{b}{N}\sum_{k=0}^{N-1} f\left(\frac{b(k+(1/2))}{N}\right)\] where we take the midpoint rather than the
left or right point, and the <a href="https://en.wikipedia.org/wiki/Trapezoidal_rule"><strong>Trapezoid Rule</strong></a>
\[\int_0^b f(x)\mathrm dx \approx \frac{b}{2N}\sum_{k=0}^{N-1}[f(b(k+1)/N) + f(bk/N)]\] where we average the left and right Riemann
sums. While both of these perform substantially better than the left/right Riemann sums, they are still rather basic
quadrature rules; the error decreases as |O(1/N^2)|.</p>
<p>Something special happens when |f| is a periodic function. First, the Trapezoid rule reduces to
|\frac{b}{N}\sum_{k=0}^{N-1} f(bk/N)|. More importantly, the Midpoint rule and the Trapezoid rule both start converging
geometrically rather than quadratically. Furthermore, for the particular case we’re interested in, namely integrating analytic
functions along a circle in the complex plane, these quadrature rules are optimal. Let |\zeta| be the |2N|-th root of unity.
The Trapezoid rule corresponds to sum the values of |f| at the even powers of |\zeta| scaled by the radius |h| and translated
by |z_0|, and the Midpoint rule corresponds to the sum of the odd powers.</p>
<p>We now have two parameters for approximating a Cauchy integral via the Trapezoid or Midpoint rules: the radius |h| and the
number of points |N|.</p>
<p>Complex-Step Differentiation corresponds to approximating the Cauchy integral for the derivative using the extreme case of
the Midpoint rule with |N=2| and very small radii (i.e. values of |h|). Meanwhile, Central Differences corresponds to the extreme case
of using the Trapezoid rule with |N=2| and very small radii. To spell this out a bit more, we perform the substitution
|z - z_0 = he^{\theta i}| which leads to |\mathrm dz = hie^{\theta i}\mathrm d\theta| and
<span class="math display">$$\frac{1}{2\pi i}\oint_{|z - z_0| = h} \frac{f(z)\mathrm dz}{(z - z_0)^2} = \frac{1}{2 \pi h}\int_0^{2\pi} f(z_0 + he^{\theta i})e^{-\theta i}\mathrm d\theta$$</span></p>
<p>Applying the Trapezoid rule to the right hand side of this corresponds to picking |\theta = 0, \pi|, while applying the
Midpoint rule corresponds to picking |\theta = \pm \pi/2|. |e^{\theta i} = \pm 1| for |\theta = 0, \pi|, and |e^{\theta i} = \pm i|
for |\theta = \pm \pi/2|. For the Trapezoid rule, this leads to \[f’(z_0) \approx \frac{1}{2h}[f(z_0 + h) - f(z_0 - h)]\] which is
Central Differences. For the Midpoint rule, this leads to \[f’(z_0) \approx \frac{1}{2hi}[f(z_0 + hi) - f(z_0 - hi)]\] This
is Complex-Step Differentiation when |z_0| is real.</p>
<h2 id="complex-step-differentiation">Complex-Step Differentiation</h2>
<p>As just calculated, <strong>Complex-Step Differentiation</strong> computes the derivative at the <em>real</em> number |x_0| via the formula:
<span class="math display">$$f'(x_0) \approx \frac{1}{2hi} [f(x_0 + hi) - f(x_0 - hi)]$$</span> Another perspective on this formula is that it is just the
Central Differences formula along the imaginary axis instead of the real axis.</p>
<p>When |f| is complex analytic and real-valued on real arguments, then we have
|f(\overline z) = \overline{f(z)}| where |\overline z| is the complex conjugate of |z|, i.e. it maps |a + bi| to |a - bi|
or |re^{\theta i}| to |re^{-\theta i}|. This leads to
|f(x_0 + hi) - f(\overline{x_0 + hi}) = f(x_0 + hi) - \overline{f(x_0 + hi)} = 2i\operatorname{Im}(f(x_0 + hi))|. This lets us simplify
Complex-Step Differentiation to |f’(x_0) \approx \operatorname{Im}(f(x_0 + hi))/h|.</p>
<p>Here is the earlier interactive example but now using Complex-Step Differentiation. As |h| decreases in magnitude, the error
steadily decreases until there is no error at all.</p>
<p><input id="complexInput" type="range" min="2" max="20"><br />
|h|: <span id="complexH"></span><br />
|f’(|<span class="point"></span>|)|: <span id="complex"></span><br />
error: <span id="complexError"></span></p>
<p>This formula using |\operatorname{Im}| avoids catastrophic cancellation simply by not doing a subtraction. However, it turns out
for real |x_0| (which is necessary to derive the simplified formula), there isn’t a problem either way. Using the first form of
the Complex-Step Differentiation formula is also numerically stable. The key here is that the imaginary part of |x_0| and |f(x_0)| are
both |0| and so we don’t get catastrophic cancellation for the same reason we wouldn’t get it with Central Differences if |f(x_0) = 0|.
This suggests that if we wanted to evaluate |f’| at some non-zero point on the imaginary axis, Complex-Step Differentiation would
perform poorly while Central Differences would perform well. Further, if we wanted to evaluate |f’| at some point not on either
the real or imaginary axes, neither approach would perform well. In this case, choosing different values for |N| and the radius
would be necessary<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>A third perspective on Complex-Step Differentiation comes when we think about which value of |h| should we use. The smaller |f’(x_0)|
is, the smaller we’d want |h| to be. Unlike Central Differences, there is little stopping us from having |h| be <em>very</em> small and
values like |h=10^{-100}| are typical. In fact, around |h=10^{-155}| in double precision floating point arithmetic, |h| gets the
theoretically useful property that |h^2 = 0| due to underflow. In this case, |x_0 + hi| behaves like |x_0 + \varepsilon| where
|\varepsilon^2 = 0|. This is the defining property of the ring of <a href="https://en.wikipedia.org/wiki/Dual_number">dual numbers</a>.
Dual numbers are exactly what are used in forward-mode automatic differentiation.</p>
<h2 id="forward-mode-automatic-differentiation">Forward-Mode Automatic Differentiation</h2>
<p>The ring of dual numbers has numbers of the form |a + b\varepsilon| where |a, b \in \mathbb R|. This behaves just like
the complex numbers except that instead of |i^2 = -1| we have |\varepsilon^2 = 0|. The utility of dual numbers for
our purposes can be seen by expanding |f(x_0 + \varepsilon)| in a Taylor series about |x_0|. We get
|f(x_0 + \varepsilon) = f(x_0) + f’(x_0)\varepsilon|. All higher power terms of the Taylor series are zero because |\varepsilon^2 = 0|.
We can thus get the derivative of |f| simply by computing |f(x + \varepsilon)| and then looking at the coefficient of |\varepsilon|
in the result.</p>
<p>In this example there is no interactivity as we are not estimating the derivative in the AD case but instead calculating it in parallel.
There is no |h| parameter.</p>
<p>|f’(|<span class="point"></span>|)|: <span id="fad"></span><br />
error: <span id="fadError"></span></p>
<p>As the end of the previous section indicated, Complex-Step Differentiation approximates this (often exactly) by using |hi| as |\varepsilon|.
Nevertheless, this is not ideal. Often the complex versions of a function will be more costly than their dual number counterparts.
For example, |(a + bi)(c + di) = (ac - bd) + (ad + bc)i| involves four real multiplications and two additions.
|(a + b\varepsilon)(c + d\varepsilon) = ac + (ad + bc)\varepsilon| involves three real multiplications and one addition on
the other hand.</p>
<h2 id="references">References</h2>
<p><a href="https://doi.org/10.1137/S003614459631241X">Using Complex Variables to Estimate Derivatives of Real Functions</a> by Squire and Trapp (1998)
is the first(?) published paper <em>specifically</em> about the idea of complex-step differentiation. It’s a three page paper and the authors
are not claiming any originality but just demonstrating the effectiveness of ideas from the ’60s that the authors found to be underappreciated.</p>
<p><a href="https://doi.org/10.1145/838250.838251">The Complex-Step Derivative Approximation</a> by Martins, Sturdza, and Alonso (2003) does
a much deeper dive into the theory behind complex-step differentiation and its connections to automatic differentiation.</p>
<p>You may have noticed the name “Trefethen” in many of the papers cited. Nick Trefethen and his collaborators have been doing amazing
work for the past couple of decades, most notably in the <a href="https://www.chebfun.org/about/">Chebfun project</a>. Looking at
Trefethen’s book <a href="http://www.chebfun.org/ATAP/">Approximation Theory and Approximation Practice</a> (and <a href="https://people.maths.ox.ac.uk/trefethen/atapvideos.html">lectures</a>)
recently reintroduced me to <a href="https://people.maths.ox.ac.uk/trefethen/papers.html">Trefethen’s work</a>. This particular article was
prompted by a footnote in the paper <a href="https://doi.org/10.1137/130932132">The Exponentially Convergent Trapezoidal Rule</a> which I highly
recommend. In fact, I highly recommend Chebfun as well as nearly all of Trefethen’s work. It is routinely compelling, interesting, and
well presented.</p>
<h2 id="appendix">Appendix</h2>
<p>Using the language of <a href="http://geocalc.clas.asu.edu/">Geometric Calculus</a>, we can write a very general form of the Fundamental
Theorem of Calculus. Namely, \[\int_{\mathcal M} \mathrm d^m\mathbf x \cdot \nabla f(\mathbf x) = \oint_{\partial \mathcal M}\mathrm d^{m-1}\mathbf x f(\mathbf x)\] where
|\mathcal M| is an |m|-dimensional manifold. Here |f| is a multivector-valued vector function. If |m=2| and |\nabla f = 0|,
then this would produce a formula very similar to the Cauchy integral formula.</p>
<p>Writing |f(x + yi) = u(x, y) + v(x, y)i|, the Cauchy-Riemann equations are |\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}|
and |\frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x}|. However, |\nabla f = 0| leads to the slightly
different equations |\frac{\partial u}{\partial x} = -\frac{\partial v}{\partial y}| and |\frac{\partial u}{\partial y} = \frac{\partial v}{\partial x}|.</p>
<p>The resolution of this discrepancy is found by recognizing that we don’t want |f| to be a vector-valued vector function but rather
a spinor-valued spinor function. It is most natural to identify complex numbers with the even subalgebra of the 2D geometric
algebra. If |\mathbf e_1| and |\mathbf e_2| are the two orthonormal basis vectors of the 2D space, then the pseudoscalar
|I \equiv \mathbf e_1\wedge \mathbf e_2 = \mathbf e_1 \mathbf e_2| satisfies |I^2 = -1|. For the 2D case, a spinor
is a multivector of the form |a + bI|.</p>
<p>We can generalize the vector derivative, |\nabla|, to a multivector derivative |\nabla_X| where |X| is a multivector variable
by using the generic formula for the directional derivative in a linear space and then defining |\nabla_X| to be a linear
combination of directional derivatives. Given any |\mathbb R|-linear space |V| and an element |v \in V|, we can
define the directional derivative of |f : V \to V| in the direction |v| via
|\frac{\partial f}{\partial v}(x) \equiv \frac{\mathrm d f(x + \tau v)}{\mathrm d\tau}|. In our case,
we have the basis vectors |\{1, \mathbf e_1, \mathbf e_2, I\}| though we only care about the even subalgebra
corresponding to the basis vectors |\{1, I\}|. Define |\partial_1 f(x) \equiv \frac{\mathrm d f(x + \tau)}{\mathrm d \tau}|
and |\partial_I f(x) \equiv \frac{\mathrm d f(x + \tau I)}{\mathrm d \tau}| assuming |f| is a spinor-valued function<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.
We can then define |\nabla_{\mathbf z} \equiv \partial_1 + I\partial_I|. We now have |\nabla_{\mathbf z} f = 0| is
the equivalent to the Cauchy-Riemann equations where |f| is now a spinor-valued spinor function, i.e. a function of |\mathbf z|.</p>
See <a href="https://doi.org/10.1016/0022-247X(68)90002-4">Multivector Functions</a> by Hestenes for more about this.
<script>
var Num = (function () {
    function Num(real) {
        this.real = real;
    }
    Num.prototype.add = function (z) {
        return new Num(this.real + z.real);
    };
    Num.prototype.addReal = function (x) {
        return new Num(this.real + x);
    };
    Num.prototype.sub = function (z) {
        return new Num(this.real - z.real);
    };
    Num.prototype.scale = function (a) {
        return new Num(a * this.real);
    };
    Num.prototype.mul = function (z) {
        return new Num(this.real * z.real);
    };
    Num.prototype.div = function (z) {
        return new Num(this.real / z.real);
    };
    Num.prototype.exp = function () {
        return new Num(Math.exp(this.real));
    };
    Num.prototype.sin = function () {
        return new Num(Math.sin(this.real));
    };
    Num.prototype.cos = function () {
        return new Num(Math.cos(this.real));
    };
    Num.prototype.powN = function (n) {
        return new Num(Math.pow(this.real, n));
    };
    return Num;
}());

var Complex = (function () {
    function Complex(real, imag) {
        this.real = real;
        this.imag = imag;
    }
    Complex.cis = function (theta) {
        return new Complex(Math.cos(theta), Math.sin(theta));
    };
    Complex.prototype.normSquared = function () {
        var r = this.real;
        var i = this.imag;
        return r * r + i * i;
    };
    Complex.prototype.norm = function () {
        return Math.sqrt(this.normSquared());
    };
    Complex.prototype.arg = function () {
        return Math.atan2(this.imag, this.real);
    };
    Complex.prototype.add = function (z) {
        return new Complex(this.real + z.real, this.imag + z.imag);
    };
    Complex.prototype.addReal = function (x) {
        return new Complex(this.real + x, this.imag);
    };
    Complex.prototype.sub = function (z) {
        return new Complex(this.real - z.real, this.imag - z.imag);
    };
    Complex.prototype.scale = function (a) {
        return new Complex(a * this.real, a * this.imag);
    };
    Complex.prototype.mul = function (z) {
        var r = this.real;
        var i = this.imag;
        var zr = z.real;
        var zi = z.imag;
        return new Complex(r * zr - i * zi, r * zi + i * zr);
    };
    Complex.prototype.div = function (z) {
        return this.mul(z.powN(-1));
    };
    Complex.prototype.exp = function () {
        return Complex.cis(this.imag).scale(Math.exp(this.real));
    };
    Complex.prototype.cos = function () {
        var r = this.real;
        var i = this.imag;
        return new Complex(Math.cos(r)*Math.cosh(i), -Math.sin(r)*Math.sinh(i));
    };
    Complex.prototype.sin = function () {
        var r = this.real;
        var i = this.imag;
        return new Complex(Math.sin(r)*Math.cosh(i), Math.cos(r)*Math.sinh(i));
    };
    Complex.prototype.powN = function (n) {
        var r2 = this.normSquared();
        var theta = this.arg();
        return Complex.cis(n * theta).scale(Math.pow(r2, 0.5 * n));
    };
    return Complex;
}());

var Dual = (function () {
    function Dual(real, eps) {
        this.real = real;
        this.eps = eps;
    }
    Dual.prototype.add = function (z) {
        return new Dual(this.real + z.real, this.eps + z.eps);
    };
    Dual.prototype.addReal = function (x) {
        return new Dual(this.real + x, this.eps);
    };
    Dual.prototype.sub = function (z) {
        return new Dual(this.real - z.real, this.eps - z.eps);
    };
    Dual.prototype.scale = function (a) {
        return new Dual(a * this.real, a * this.eps);
    };
    Dual.prototype.mul = function (z) {
        var r = this.real;
        var e = this.eps;
        var zr = z.real;
        var ze = z.eps;
        return new Dual(r * zr, r * ze + e * zr);
    };
    Dual.prototype.div = function (z) {
        return this.mul(z.powN(-1));
    };
    Dual.prototype.exp = function () {
        return new Dual(Math.exp(this.real), this.eps * Math.exp(this.real));
    };
    Dual.prototype.sin = function () {
        return new Dual(Math.sin(this.real), this.eps * Math.cos(this.real));
    };
    Dual.prototype.cos = function () {
        return new Dual(Math.cos(this.real), this.eps * -Math.sin(this.real));
    };
    Dual.prototype.powN = function (n) {
        return new Dual(Math.pow(this.real, n), n * this.eps * Math.pow(this.real, n - 1));
    };
    return Dual;
}());

var functions = [
    function (x) { return x.powN(9 / 2); },
    function (x) { return x.sin(); },
    function (x) { return x.sin(); },
    function (x) { return x.exp(); },
    function (x) { return x.exp().div(x.sin().powN(3).add(x.cos().powN(3))); }];
var functionSpans = [
    document.getElementById('ex1'),
    document.getElementById('sin'),
    document.getElementById('sin'),
    document.getElementById('exp'),
    document.getElementById('ex2')];
var points = [1.5, 0, Math.PI, 0, 1.5];
var correct = ["18.600812734259759", "1.0000000000000000", "-1.0000000000000000", "1.0000000000000000", "3.6220337007163260"]

// Midpoint method
function midpoint(f, x, h) {
    return f(new Num(x + h)).sub(f(new Num(x - h))).scale(0.5 / h).real;
}

// Complex-step differentiation
function complexStep(f, x, h) {
    // return f(new Complex(x, h)).sub(f(new Complex(x, -h))).scale(0.5 / h).imag;
    return f(new Complex(x, h)).imag / h;
}

// Forward Automatic Differentiation
function FAD(f, x) {
    return f(new Dual(x, 1)).eps;
}

var correctText = document.getElementById('correct');
var functionSelector = document.getElementById('functionSelector');

var realH = document.getElementById('realH');
var real = document.getElementById('real');
var realError = document.getElementById('realError');
var realInput = document.getElementById('realInput');
realInput.addEventListener('input', function() {
    update(functionSelector.value);
});

var complexH = document.getElementById('complexH');
var complex = document.getElementById('complex');
var complexError = document.getElementById('complexError');
var complexInput = document.getElementById('complexInput');
complexInput.addEventListener('input', function() {
    update(functionSelector.value);
});

var fad = document.getElementById('fad');
var fadError = document.getElementById('fadError');

function update(i) {
    var f = functions[i];
    var x = points[i];
    document.querySelectorAll('span.point').forEach(function (pointText) {
        pointText.textContent = points[i];
    });
    correctText.textContent = correct[i];
    var correctValue = parseFloat(correct[i]);

    var rv = parseFloat(realInput.value);
    var rh = Math.pow(10, -rv);
    var ry = midpoint(f, x, rh);
    realH.textContent = rh;
    real.textContent = ry;
    realError.textContent = Math.abs(ry - correctValue);

    var cv = parseFloat(complexInput.value);
    var ch = Math.pow(10, -cv);
    var cy = complexStep(f, x, ch);
    complexH.textContent = ch;
    complex.textContent = cy;
    complexError.textContent = Math.abs(cy - correctValue);

    fad.textContent = FAD(f, x);
    fadError.textContent = Math.abs(FAD(f, x) - correctValue);

    functionSpans.forEach(function (span) {
        span.style = "display: none;";
    });
    functionSpans[i].style = "display: inline;";
}

update(functionSelector.value);

functionSelector.addEventListener('input', function() {
    update(functionSelector.value);
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>In terms of the general theory of partial differential equations, we are saying that |z^{-1}| is
a <a href="https://en.wikipedia.org/wiki/Green&#39;s_function">Green’s function</a> for |\nabla|. We can then understand everything that is happening
here in terms of general results. In particular, it is the two-dimensional case of the results described in
<a href="https://doi.org/10.1016/0022-247X(68)90002-4">Multivector Functions</a> by Hestenes.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>See
<a href="https://doi.org/10.1137/130931035">Numerical Algorithms based on Analytic Function Values at Roots of Unity</a> by Austin, Kravanja, and Trefethen (2014)
for an example. Also, with some minor tweaks, we can have that “point” be a matrix and these integrals can be used to calculate functions
of matrices, e.g. the square root, exponent, inverse, and log of a matrix. See
<a href="https://doi.org/10.1137/070700607">Computing |A^\alpha|, |\log(A)|, and Related Matrix Functions by Contour Integrals</a> by Hale, Higham, and Trefethen (2009)
for details.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>See
<a href="https://doi.org/10.1137/060659831">Is Gauss Quadrature Better than Clenshaw-Curtis?</a> by Trefethen (2008) for more details.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>While focused on issues that mostly come up with very high-order derivatives, e.g. |100|-th derivatives and higher,
<a href="https://arxiv.org/abs/0910.1841">Accuracy and Stability of Computing High-Order Derivatives of Analytic Functions by Cauchy Integrals</a>
by Bornemann (2009) nevertheless has a good discussions of the concerns here.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>If
we allowed arbitrary multivector-valued functions, then we’d need to add a projection producing the tangential derivative.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Enriched Indexed Categories, Syntactically</title>
    <link href="https://derekelkins.github.io/posts/enriched-indexed-categories-syntactically.html" />
    <id>https://derekelkins.github.io/posts/enriched-indexed-categories-syntactically.html</id>
    <published>2020-07-05 22:03:27-07:00</published>
    <updated>2020-07-06T05:03:27Z</updated>
    <summary type="html"><![CDATA[<script>
extraMacros = {
  proarrow: "{\\mathrel{-\\!\\!\\!\\mapsto}}",
  V: "{\\mathcal V}",
  O: "{\\mathsf O}",
  A: "{\\mathsf A}",
};
</script>
<h2 id="introduction">Introduction</h2>
<p>This is part 3 in a series. See <a href="/posts/internal-language-of-indexed-monoidal-categories.html">the previous part about internal languages for indexed monoidal categories</a>
upon which this part heavily depends.</p>
<p>In category theory, the hom-sets between two objects can often be equipped with some extra structure which is respected
by identities and composition. For example, the set of group homomorphisms between two abelian groups is itself an abelian group by defining
the operations pointwise. Similarly, the set of monotonic functions between two partially ordered sets (posets) is a poset
again by defining the ordering pointwise. Linear functions between vector spaces form a vector space. The set of functors
between small categories is a small category. Of course, the structure
on the hom-sets can be different than the objects. Trivially, with the earlier examples a vector space is an abelian group, so we could
say that linear functions form an abelian group instead of a vector space. Likewise groups are monoids. Less trivially, the set of relations
between two sets is a partially ordered set via inclusion. There are many cases where instead of hom-sets we have hom-objects that
aren’t naturally thought of as sets. For example, we can have hom-objects be non-negative (extended) real numbers from which
the category laws become the laws of a generalized metric space. We can identify posets with categories who hom-objects are elements
of a two element set or, even better, a two element poset with one element less than or equal to the other.</p>
<p>This general process is called <a href="https://ncatlab.org/nlab/show/enriched+category+theory">enriching</a> a category in some other
category which is almost always called |\V| in the generic
case. We then talk about having |\V|-categories and |\V|-functors, etc. In a specific case, it will be something
like |\mathbf{Ab}|-categories for an |\mathbf{Ab}|-enriched category, where |\mathbf{Ab}| is the category of abelian groups.
Unsurprisingly, not just <em>any</em> category will do for |\V|. However, it turns out very little structure is needed to define
a notion of |\V|-category, |\V|-functor, |\V|-natural transformation, and |\V|-profunctor. The
usual “baseline” is that |\V| is a <a href="https://ncatlab.org/nlab/show/monoidal+category">monoidal category</a>. As mentioned in the previous
post, paraphrasing Bénabou, notions of “families of objects/arrows” are ubiquitous and fundamental in category theory. It is useful for
our purposes to make this structure explicit. For very little cost, this will also provide a vastly more general notion that will readily
capture enriched categories, <a href="https://ncatlab.org/nlab/show/indexed+category">indexed categories</a>, and categories that are simultaneously
indexed and enriched, of which <a href="https://ncatlab.org/nlab/show/internal+category">internal categories</a> are an example. The tool for this is
a <a href="https://ncatlab.org/nlab/show/Grothendieck+fibration">(Grothendieck) fibration</a> aka a fibered category or the mostly equivalent concept
of an indexed category.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>To that end, instead of just a monoidal category, we’ll be using <a href="https://ncatlab.org/nlab/show/indexed+monoidal+category">indexed monoidal categories</a>.
Typically, to get an experience as much like ordinary category theory as possible, additional structure is assumed on |\V|. In
particular, it is assumed to be an <a href="https://ncatlab.org/nlab/show/cosmos#indexed_bnabou_cosmoi">(indexed) cosmos</a> which means
that it is an indexed symmetric monoidally closed category with indexed coproducts preserved by |\otimes| and indexed products
and fiberwise finite limits and colimits (preserved by the indexed structure). This is quite a lot more structure which I’ll introduce
in later parts. In this part, I’ll make no assumptions beyond having an indexed monoidal category.</p>
<!--more-->
<h2 id="basic-category-theory-in-indexed-monoidal-categories">Basic Category Theory in Indexed Monoidal Categories</h2>
<p>The purpose of the machinery of the previous posts is to make this section seem boring and pedestrian. Other than being
a little more explicit and formal, for most of the following concepts it will look like we’re restating the usual definitions
of categories, functors, and natural transformations. The main exception is profunctors which will be presented in a quite different
manner, though still in a manner that is easy to connect to the usual presentation. (We will see how to recover the usual presentation
in later parts.)</p>
<p>While I’ll start by being rather explicit about indexes and such, I will start to suppress that detail over time as most of it
is inferrable. One big exception is that right from the start I’ll omit the explicit dependence of primitive terms on indexes.
For example, while I’ll write |\mathsf F(\mathsf{id}) = \mathsf{id}| for the first functor law, what the syntax of the previous
posts says I should be writing is |\mathsf F(s, s; \mathsf{id}(s)) = \mathsf{id}(s)|.</p>
<p>To start, I want to introduce two different notions of |\V|-category, small |\V|-categories and large |\V|-categories,
and talk about what this distinction actually means. I will proceed afterwards with the “large” notions, e.g. |\V|-functors between
large |\V|-categories, as the small case will be an easy special case.</p>
<h3 id="small-v-categories">Small |\V|-categories</h3>
<p>The <strong>theory of a small |\V|-category</strong> consists of:</p>
<ul>
<li>an index type |\O|,</li>
<li>a linear type |s, t : \O \vdash \A(t, s)|,</li>
<li>a linear term |s : \O; \vdash \mathsf{id} : \A(s, s)|, and</li>
<li>a linear term |s, u, t : \O; g : \A(t, u), f : \A(u, s) \vdash g \circ f : \A(t, s)|</li>
</ul>
<p>satisfying <span class="math display">$$\begin{gather}
s, t : \O; f : \A(t, s) \vdash \mathsf{id} \circ f = f = f \circ \mathsf{id} : \A(t, s)\qquad \text{and} \\ \\
s, u, v, t : \O; h : \A(t, v), g : \A(v, u), f : \A(u, s) \vdash h \circ (g \circ f) = (h \circ g) \circ f : \A(t, s)
\end{gather}$$</span></p>
<p>In the notation of the previous posts, I’m saying |\O : \mathsf{IxType}|, |\A : (\O, \O) \to \mathsf{Type}|,
|\mathsf{id} : (s : \O;) \to \A(s, s)|, and |\circ : (s, u, t : \O; \A(t, u), \A(u, s)) \to \A(t, s)|
are primitives added to the signature of the theory. I’ll continue to use the earlier, more pointwise presentation above to
describe the signature.</p>
<p>A <strong>small |\V|-category</strong> for an |\mathbf S|-indexed monoidal category |\V| is then an interpretation of
this theory. That is, an object |O| of |\mathbf S| as the interpretation of |\O|, and an
object |A| of |\V^{O\times O}| as the interpretation of |\A|. The interpretation of |\mathsf{id}|
is an arrow |I_O \to \Delta_O^* A| of |\V^O|, where |\Delta_O : O \to O\times O| is the diagonal
arrow |\langle id, id\rangle| in |\mathbf S|. The interpretation of |\circ| is an
arrow |\pi_{23}^* A \otimes \pi_{12}^* A \to \pi_{13}^* A| of |\V^{O\times O \times O}|
where |\pi_{ij} : X_1 \times X_2 \times X_3 \to X_i \times X_j| are the appropriate projections.</p>
<p>Since we can prove in the internal language that the choice of |()| for |\O|, |s, t : () \vdash I|
for |\A|, |s, t : (); x : I \vdash x : I| for |\mathsf{id}|,
and |s, u, t : (); f : I, g: I \vdash \mathsf{match}\ f\ \mathsf{as}\ *\ \mathsf{in}\ g : I| for |\circ|
satisfies the laws of the theory of a small |\V|-category, we know we have a |\V|-category which I’ll call |\mathbb I|
for any |\V|<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>For |\V = \mathcal Fam(\mathbf V)|, |O| is a set of objects. |A| is an |(O\times O)|-indexed family of objects of |\mathbf V|
which we can write |\{A(t,s)\}_{s,t\in O}|. The interpretation of |\mathsf{id}| is an |O|-indexed family of arrows
of |\mathbf V|, |\{ id_s : I_s \to A(s, s) \}_{s\in O}|. Finally, the interpretation of |\circ| is a family of arrows
of |\mathbf V|, |\{ \circ_{s,u,t} : A(t, u)\otimes A(u, s) \to A(t, s) \}_{s,u,t \in O}|. This is exactly
the data of a (small) |\mathbf V|-enriched category. One example is when |\mathbf V = \mathbf{Cat}| which
produces (strict) |2|-categories.</p>
<p>For |\V = \mathcal Self(\mathbf S)|, |O| is an object of |\mathbf S|. |A| is an arrow of |\mathbf S| into |O\times O|, i.e.
an object of |\mathbf S/O\times O|. I’ll write the object part of this as |A| as well, i.e. |A : A \to O\times O|. The idea
is that the two projections are the target and source of the arrow. The interpretation of |\mathsf{id}| is an arrow |ids : O \to A|
such that |A \circ ids = \Delta_O|, i.e. the arrow produced by |ids| should have the same target and source. Finally, the interpretation
of |\circ| is an arrow |c| from the pullback of |\pi_2 \circ A| and |\pi_1 \circ A| to |A|. The source of this arrow is the
object of composable pairs of arrows. We further require |c| to produce an arrow with the appropriate target and source of the
composable pair. This is exactly the data for a category internal to |\mathbf S|. An interesting case for contrast with the
previous paragraph is that a category internal to |\mathbf{Cat}| is a double category.</p>
<p>For |\V = \mathcal Const(\mathbf V)|, the above data is exactly the data of a <a href="https://ncatlab.org/nlab/show/monoid+object">monoid object</a>
in |\mathbf V|. This is a formal illustration that a (|\mathbf V|-enriched) category is just an “indexed monoid”.
Indeed, |\mathcal Const(\mathbf V)|-functors will be monoid homomorphisms and |\mathcal Const(\mathbf V)|-profunctors
will be double-sided monoid actions. In particular, when |\mathbf V = \mathbf{Ab}|, we get rings, ring homomorphisms,
and bimodules of rings. The intuitions here are the guiding ones for the construction we’re realizing.</p>
<p>One aspect of working in a not-necessarily-symmetric (indexed) monoidal category is the choice of the standard
order of composition or diagrammatic order is not so trivial since it is not possible to even state what it
means for them to be the equivalent. To be clear, this definition isn’t really taking a stance on the issue.
We can interpret |\A(t, s)| as the type of arrows |s \to t| and then |\circ| will be the standard
order of composition, or as the type of arrows |t \to s| and then |\circ| will be the diagrammatic order.
In fact, there’s nothing in this definition that stops us from having |\A(t, s)| being the type
of arrows |t \to s| while still having |\circ| be standard order composition as usual. The issue comes up
only once we consider |\V|-profunctors as we will see.</p>
<h3 id="large-v-categories">Large |\V|-categories</h3>
<p>A <strong>large |\V|-category</strong> is a model of a theory of the following form. There is</p>
<ul>
<li>a collection of index types |\O_x|,</li>
<li>for each pair of index types |\O_x| and |\O_y|, a linear type |s : \O_x, t : \O_y \vdash \A_{yx}(t, s)|,</li>
<li>for each index type |\O_x|, a linear term |s : \O_x; \vdash \mathsf{id}_x : \A_{xx}(s, s)|, and</li>
<li>for each triple of index types, |\O_x|, |\O_y|, and |\O_z|, a linear term
|s: \O_x, u : \O_y, t : \O_z; g : \A_{zy}(t, u), f : \A_{yx}(u, s) \vdash g \circ_{xyz} f : \A_{zx}(t, s)|</li>
</ul>
<p>satisfying the same laws as small |\V|-categories, just with some extra subscripts. Clearly, a small |\V|-category
is just a large |\V|-category where the collection of index types consists of just a single index type.</p>
<h3 id="small-versus-large">Small versus Large</h3>
<p>The typical way of describing the difference between small and large (|\V|-)categories would be to say something like: “By
having a collection of index types in a large |\V|-category, we can have a proper class of them. In a
small |\V|-category, the index type of objects is interpreted as an object in a category, and a proper class can’t
be an object of a category<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.” However, for us, there’s
a more directly relevant distinction. Namely, while we had a single theory of small |\V|-categories, there is no
single theory of large |\V|-categories. Different large |\V|-categories correspond to models of (potentially) different theories.
In other words, the notion of a small |\V|-category is able to be captured by our notion of theory but not the concept
of a large |\V|-category. This extends to |\V|-functors, |\V|-natural transformations,
and |\V|-profunctors. In the small case, we can define a single theory which captures each of these concepts,
but that isn’t possible in the large case. In general, notions of “large” and “small” are about what we can internalize
within the relevant object language, usually a set theory. Arguably, the only reason we speak of “size” and of proper
classes being “large” is that the <a href="https://en.wikipedia.org/wiki/Axiom_schema_of_specification">Axiom of Specification</a>
outright states that any subclass of a set is a set, so proper classes in <strong>ZFC</strong> can’t be subsets of any set.
As I’ve mentioned <a href="/posts/finite.html">elsewhere</a>, you can definitely have set theories with proper classes that are
contained in even finite sets, so the issue isn’t one of “bigness”.</p>
<p>The above discussion also explains the hand-wavy word “collection”. The collection is a collection in the meta-language in
which we’re discussing/formalizing the notion of theory. When working <em>within</em> the theory of a particular
large |\V|-category, all the various types and terms are just available ab initio and are independent. There is
no notion of “collection of types” within the theory and nothing indicating that some types are part of a “collection” with others.</p>
<p>Another perspective on this distinction between large and small |\V|-categories is that small |\V|-categories have
a <em>family</em> of arrows, identities, and compositions with respect to the notion of “family” represented by our internal
language. If we hadn’t wanted to bother with formulating the internal language of an <em>indexed</em> monoidal category, we
could have still defined the notion of |\V|-category with respect to the internal language of a (non-indexed) monoidal
category. It’s just that all such |\V|-categories (except for monoid objects) would have to be large |\V|-categories. That is,
the indexing and notion of “family” would be at a meta-level. Since most of the |\V|-categories of interest will be
large (though, generally a special case called a |\V|-fibration which reins in the size a bit), it may seem that there
was no real benefit to the indexing stuff. Where it comes in, or rather where small |\V|-categories come in, is that
our notion of (co)complete means “has all (co)limits of <em>small</em> diagrams” and small diagrams are |\V|-functors from
small |\V|-categories.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> There are several other places, e.g. the notion
of presheaf, where we implicitly depend on what we mean by “small |\V|-category”. So while we won’t usually be
focused on small |\V|-categories, which |\V|-categories are small impacts the structure of the whole theory.</p>
<h3 id="v-functors">|\V|-functors</h3>
<p>The formulation of |\V|-functors is straightforward. As mentioned before, I’ll only present the “large” version.</p>
<p>Formally, we can’t formulate a theory of just a |\V|-functor, but rather we need to formulate a
theory of “a pair of |\V|-categories and a |\V|-functor between them”.</p>
<p>A <strong>|\V|-functor between (large) |\V|-categories |\mathcal C| and |\mathcal D|</strong> is a model of
a theory consisting of a theory of a large |\V|-category, of which |\mathcal C| is a model, and a theory
of a large |\V|-category which I’ll write with primes, of which |\mathcal D| is a model, and model of
the following additional data:</p>
<ul>
<li>for each index type |\O_x|, an index type |\O’_{F_x}| and
an index term, |s : \O_x \vdash \mathsf F_x(s) : \O’_{F_x}|, and</li>
<li>for each pair of index types, |\O_x| and |\O_y|, a linear term
|s : \O_x, t : \O_y; f : \A_{yx}(t, s) \vdash \mathsf F_{yx}(f) : \A’_{F_yF_x}(\mathsf F_y(t), \mathsf F_x(s))|</li>
</ul>
<p>satisfying <span class="math display">$$\begin{gather}
s : \O_x; \vdash \mathsf F_{xx}(\mathsf{id}_x) = \mathsf{id}'_{F_x}: \A'_{F_xF_x}(F_x(s), F_x(s))\qquad\text{and} \\
s : \O_x, u : \O_y, t : \O_z; g : \A_{zy}(t, u), f : \A_{yx}(u, s)
    \vdash \mathsf F_{zx}(g \circ_{xyz} f) = F_{zy}(g) \circ'_{F_xF_yF_z} F_{yx}(f) : \A'_{F_zF_x}(F_z(t), F_x(s))
\end{gather}$$</span></p>
<p>The assignment of |\O’_{F_x}| for |\O_x| is, again, purely metatheoretical. From within the theory, all we know
is that we happen to have some index types named |\O_x| and |\O’_{F_x}| and some data relating them. The fact
that there is some kind of mapping of one to the other is not part of the data.</p>
<p>Next, I’ll define <strong>|\V|-natural transformations</strong> . As before, what we’re really doing is defining |\V|-natural transformations
as a model of a theory of “a pair of (large) |\V|-categories with a pair of |\V|-functors between them and a |\V|-natural transformation
between those”. As before, I’ll use primes to indicate the types and terms of the second of each pair of subtheories. Unlike
before, I’ll only mention what is added which is:</p>
<ul>
<li>for each index type |\O_x|, a linear term |s : \O_x; \vdash \tau_x : \A’_{F’_xF_x}(\mathsf F’_x(s), \mathsf F_x(s))|</li>
</ul>
<p>satisfying <span class="math display">$$\begin{gather}
s : \O_x, t : \O_y; f : \A_{yx}(t, s)
    \vdash \mathsf F'_{yx}(f) \circ'_{F_xF'_xF'_y} \tau_x = \tau_y \circ'_{F_xF_yF'_y} \mathsf F_{yx}(f) : \A'_{F'_yF_x}(\mathsf F'_y(t), \mathsf F_x(s))
\end{gather}$$</span></p>
<p>In practice, I’ll suppress the subscripts on all but index types as the rest are inferrable. This makes the above equation the
much more readable
<span class="math display">$$\begin{gather}
s : \O_x, t : \O_y; f : \A(t, s)
    \vdash \mathsf F'(f) \circ' \tau = \tau \circ' \mathsf F(f) : \A'(\mathsf F'(t), \mathsf F(s))
\end{gather}$$</span></p>
<h3 id="v-profunctors">|\V|-profunctors</h3>
<p>Here’s where we need to depart from the usual story. In the usual story, a |\mathbf V|-enriched profunctor
|P : \mathcal C \proarrow \mathcal D| is
a |\mathbf V|-enriched functor |P : \mathcal C\otimes\mathcal D^{op}\to\mathbf V| (or, often, the opposite
convention is used |P : \mathcal C^{op}\otimes\mathcal D \to \mathbf V|). There are many problems with this
definition in our context.</p>
<ol type="1">
<li>Without symmetry, we have no definition of opposite category.</li>
<li>Without symmetry, the tensor product of |\mathbf V|-enriched categories doesn’t make sense.</li>
<li>|\mathbf V| is not itself a |\mathbf V|-enriched category, so it doesn’t make sense to talk
about |\mathbf V|-enriched functors into it.</li>
<li>Even if it was, we’d need some way of converting between arrows of |\mathbf V| as a category and
arrows of |\mathbf V| as a |\mathbf V|-enriched category.</li>
<li>The equation |P(g \circ f, h \circ k) = P(g, k) \circ P(f, h)| requires symmetry. (This is arguably
2 again.)</li>
</ol>
<p>All of these problems are solved when |\mathbf V| is a symmetric monoidally closed category.</p>
<p>Alternatively, we can reformulate the notion of a |\V|-profunctor so that it works in our context
and is equivalent to the usual one when it makes sense.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> To this end, at a low level a |\mathbf V|-enriched profunctor is a family of
arrows <span class="math display">$$\begin{gather}P : \mathcal C(t, s)\otimes\mathcal D(s', t') \to [P(s, s'), P(t, t')]\end{gather}$$</span> which
satisfies <span class="math display">$$\begin{gather}P(g \circ f, h \circ k)(p) = P(g, k)(P(f, h)(p))\end{gather}$$</span> in the internal language of a symmetric
monoidally closed category among other laws. We can uncurry |P| to eliminate the need for closure,
getting <span class="math display">$$\begin{gather}P : \mathcal C(t, s)\otimes \mathcal D(s', t')\otimes P(s, s') \to P(t, t')\end{gather}$$</span>
satisfying <span class="math display">$$\begin{gather}P(g \circ f, h \circ k, p) = P(g, k, P(f, h, p))\end{gather}$$</span> We see that we’re always going to need
to permute |f| and |h| past |k| unless we move the third argument to the second producing the
nice <span class="math display">$$\begin{gather}P : \mathcal C(t, s)\otimes P(s, s') \otimes \mathcal D(s', t') \to P(t, t')\end{gather}$$</span> and the
law <span class="math display">$$\begin{gather}P(g \circ f, p, h \circ k) = P(g, P(f, p, h), k)\end{gather}$$</span> which no longer requires symmetry. This is
also where the order of the arguments of |\circ| drives the order of the arguments of |\V|-profunctors.</p>
<p>A <strong>|\V|-profunctor</strong>, |P : \mathcal C \proarrow \mathcal D|, is a model of the theory (containing
subtheories for |\mathcal C| and |\mathcal D| etc. as in the |\V|-functor case) having:</p>
<ul>
<li>for each pair of index types |\O_x| and |\O’_{x’}|,
a linear type |s : \O_x, t : \O’_{x’} \vdash \mathsf P_{x’x}(t, s)|, and</li>
<li>for each quadruple of index types |\O_x|, |\O_y|, |\O’_{x’}|, and |\O’_{y’}|,
a linear term |s : \O_x, s’ : \O’_{x’}, t : \O_y, t’ : \O’_{y’};
f : \A_{yx}(t, s), p : \mathsf P_{xx’}(s, s’), h : \A’_{x’y’}(s’, t’) \vdash \mathsf P_{yxx’y’}(f, p, h) : \mathsf P_{yy’}(t, t’)|</li>
</ul>
<p>satisfying <span class="math display">$$\begin{align}
 &amp; s : \O_x, s' : \O'_{x'}; p : \mathsf P(s, s')
    \vdash \mathsf P(\mathsf{id}, p, \mathsf{id}') = p : \mathsf P(s, s') \\ \\
 &amp; s : \O_x, s' : \O'_{x'}, u : \O_y, u' : \O'_{y'}, t : \O_z, t' : \O'_{z'}; \\
 &amp;  g : \A(t, u), f : \A(u, s), p : \mathsf P(s, s'), h : \A'(s', u'), k : \A'(u', t') \\
    \vdash\  &amp; \mathsf P(g \circ f, p, h \circ' k) = \mathsf P(g, \mathsf P(f, p, h), k) : \mathsf P(t, t')
\end{align}$$</span></p>
<p>This can also be equivalently presented as a pair of a left and a right action satisfying <a href="https://en.wikipedia.org/wiki/Bimodule#Definition">bimodule laws</a>. We’ll make
the following definitions |\mathsf P_l(f, p) = \mathsf P(f, p, \mathsf {id})|
and |\mathsf P_r(p, h) = \mathsf P(\mathsf{id}, p ,h)|.</p>
<p>A <strong>|\V|-presheaf</strong> on |\mathcal C| is a |\V|-profunctor |P : \mathbb I \proarrow \mathcal C|. Similarly,
a <strong>|\V|-copresheaf</strong> on |\mathcal C| is a |\V|-profunctor |P : \mathcal C \proarrow \mathbb I|.</p>
<p>Of course, we have the fact that the term <span class="math display">$$\begin{gather}
s : \O_x, t : \O_y, s' : \O_z, t' : \O_w; h : \A(t, s), g : \A(s, s'), f : \A(s', t')
    \vdash h \circ g \circ f : \A(t, t')
\end{gather}$$</span> witnesses the interpretation of |\A| as a |\V|-profunctor |\mathcal C \proarrow \mathcal C|
for any |\V|-category, |\mathcal C|, which we’ll call the <strong>hom |\V|-profunctor</strong>. More generally, given
a |\V|-profunctor |P : \mathcal C \proarrow \mathcal D|, and |\V|-functors |F : \mathcal C’ \to \mathcal C|
and |F’ : \mathcal D’ \to \mathcal D|, we have the |\V|-profunctor |P(F, F’) : \mathcal C’ \proarrow \mathcal D’|
defined as <span class="math display">$$\begin{gather}
s : \O_x, s' : \O'_{x'}, t : \O_y, t' : \O'_{y'}; f : \A(t, s), p : \mathsf P(\mathsf F(s), \mathsf F'(s')), f' : \A'(s', t')
    \vdash \mathsf P(\mathsf F(f), p, \mathsf F'(f')) : \mathsf P(\mathsf F(t), \mathsf F'(t'))
\end{gather}$$</span> In particular, we have the <strong>representable |\V|-profunctors</strong> when |P| is the hom |\V|-profunctor and
either |F| or |F’| is the identity |\V|-functor, e.g. |\mathcal C(Id, F)| or |\mathcal C(F, Id)|.</p>
<h3 id="multimorphisms">Multimorphisms</h3>
<p>There’s a natural notion of morphism of |\V|-profunctors which we could derive either via passing the notion of
natural transformation of the bifunctorial view through the same reformulations as above, or by generalizing the notion of
a bimodule homomorphism. This would produce a notion like: a |\V|-natural transformation from |\alpha : P \to Q|
is a |\alpha : P(t, s) \to Q(t, s)| satisfying |\alpha(P(f, p, h)) = Q(f, \alpha(p), h)|. While there’s nothing wrong
with this definition, it doesn’t quite meet our needs. One way to see this is that it would be nice to have a bicategory
whose |0|-cells were |\V|-categories, |1|-cells |\V|-profunctors, and |2|-cells |\V|-natural
transformations as above. The problem there isn’t the |\V|-natural transformations but the |1|-cells. In particular,
we don’t have composition of |\V|-profunctors. In the analogy with bimodules, we don’t have tensor products so we
can’t reduce multilinear maps to linear maps; therefore, linear maps don’t suffice, and we really want a notion of multilinear maps.</p>
<p>So, instead of a bicategory what we’ll have is a virtual bicategory (or, more generally, a <a href="https://ncatlab.org/nlab/show/virtual+double+category">virtual double category</a>).
A virtual bicategory is to a bicategory what a multicategory is to a monoidal category, i.e. multicategories are “virtual monoidal
categories”. The only difference between a virtual bicategory and a multicategory is that instead of our multimorphisms having
arbitrary lists of objects as their sources, our “objects” (|1|-cells) themselves have sources and targets (|0|-cells) and our
multimorphisms (|2|-cells) have <em>composable sequences</em> of |1|-cells as their sources.</p>
<p>A <strong>|\V|-multimorphism</strong> from a composable sequence of |\V|-profunctors |P_1, \dots, P_n| to
the |\V|-profunctor |Q| is a model of the theory consisting of the various necessary subtheories and:</p>
<ul>
<li>a linear term, |s_0 : \O_{x_0}^0, \dots, s_n : \O_{x_n}^n;
p_1 : \mathsf P_{x_0x_1}^1(s_0, s_1), \dots, p_n : \mathsf P_{x_{n-1}x_n}^n(s_{n-1}, s_n)
\vdash \tau_{x_0\cdots x_n}(p_1, \dots, p_n) : \mathsf Q_{x_0x_n}(s_0, s_n)|</li>
</ul>
<p>satisfying <span class="math display">$$\begin{align}
&amp; t, s_0 : \O^0, \dots, s_n : \O^n;
    f : \A^0(t, s_0), p_1 : \mathsf P^1(s_0, s_1), \dots, p_n : \mathsf P^n(s_{n-1}, s_n) \\
    \vdash\ &amp; \tau(\mathsf P_l^0(f, p_1), \dots, p_n) = \mathsf Q_l(f, \tau(p_1, \dots, p_n)) : \mathsf Q(t, s_n) \\ \\
&amp; s_0 : \O^0, \dots, s_n, s : \O^n;
    p_1 : \mathsf P^1(s_0, s_1), \dots, p_n : \mathsf P^n(s_{n-1}, s_n), f : \A^n(s_n, s) \\
    \vdash\ &amp; \tau(p_1, \dots, \mathsf P_r^n(p_n, f)) = \mathsf Q_r(\tau(p_1, \dots, p_n), f) : \mathsf Q(s_0, s) \\ \\
&amp; s_0 : \O^0, \dots, s_n : \O^n; \\
    &amp; p_1 : \mathsf P^1(s_0, s_1), \dots, p_i : \mathsf P^i(s_{i-1}, s_i), f : \A^i(s_i, s_{i+1}),
      p_{i+1} : \mathsf P^{i+1}(s_i, s_{i+1}), \dots, p_n : \mathsf P^n(s_{n-1}, s_n) \\
    \vdash\ &amp; \tau(p_1, \dots, \mathsf P_r^i(p_i, f), p_{i+1}, \dots, p_n) = \tau(p_1, \dots, p_i, \mathsf P_l^{i+1}(f, p_{i+1}) \dots, p_n) : \mathsf Q(s_0, s_n)
\end{align}$$</span>
except for the |n=0| case in which case the only law is <span class="math display">$$\begin{gather}
t, s : \O^0; f : \A^0(t, s) \vdash \mathsf Q_l(f, \tau()) = \mathsf Q_r(\tau(), f) : \mathsf Q(t, s)
\end{gather}$$</span></p>
<p>The laws involving the action of |\mathsf Q| are called <strong>external equivariance</strong>, while the remaining law is called <strong>internal
equivariance</strong>. We’ll write |\V\mathbf{Prof}(P_1, \dots, P_n; Q)| for the set of |\V|-multimorphisms from the
composable sequence of |\V|-profunctors |P_1, \dots, P_n| to the |\V|-profunctor |Q|.</p>
<p>As with multilinear maps, we can characterize composition via a universal property. Write |Q_1\diamond\cdots\diamond Q_n|
for the <strong>composite |\V|-profunctor</strong> (when it exists) of the composable sequence |Q_1, \dots, Q_n|. We then have
for any pair of composable sequences |R_1, \dots, R_m| and |S_1, \dots, S_k| which compose with |Q_1, \dots, Q_n|,
<span class="math display">$$\begin{gather}
\V\mathbf{Prof}(R_1,\dots, R_m, Q_1 \diamond \cdots \diamond Q_n, S_1, \dots, S_k; -)
    \cong \V\mathbf{Prof}(R_1,\dots, R_m, Q_1, \dots, Q_n, S_1, \dots, S_k; -)
\end{gather}$$</span> where the forward direction is induced by precomposition with a |\V|-multimorphism
|Q_1, \dots, Q_n \to Q_1 \diamond \cdots \diamond Q_n|. A |\V|-multimorphism with this property
is called <strong>opcartesian</strong>. The |n=0| case is particularly important and, for a |\V|-category |\mathcal C|,
produces the <strong>unit |\V|-profunctor</strong>, |U_\mathcal C : \mathcal C \proarrow \mathcal C| as the composite
of the empty sequence. When we have all composites, |\V\mathbf{Prof}| becomes an actual bicategory rather
than a virtual bicategory. |\V\mathbf{Prof}| always has all units, namely the hom |\V|-profunctors. Much like we
can define the tensor product of modules by quotienting the tensor product of their underlying abelian groups by internal
equivariance, we will find that we can make composites when we have enough (well-behaved) colimits<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
<p>Related to composites, we can talk about left/right closure of |\V\mathbf{Prof}|. In this case we have the
natural isomorphisms: <span class="math display">$$\begin{gather}
\V\mathbf{Prof}(Q_1,\dots, Q_n, R; S) \cong \V\mathbf{Prof}(Q_1, \dots, Q_n; R \triangleright S) \\
\V\mathbf{Prof}(R, Q_1, \dots, Q_n; S) \cong \V\mathbf{Prof}(Q_1, \dots, Q_n;S \triangleleft R)
\end{gather}$$</span> Like composites, this merely characterizes these constructs; they need not exist in general.
These will be important when we talk about Yoneda and (co)limits in |\V|-categories.</p>
<p>A |\V|-natural transformation |\alpha : F \to G : \mathcal C \to \mathcal D| is the same as |\alpha\in\V\mathbf{Prof}(;\mathcal D(G, F))|.</p>
<h3 id="example-proof">Example Proof</h3>
<p>Just as an example, let’s prove a basic fact about categories for arbitrary |\V|-categories. This will use an informal
style.</p>
<p>The fact will be that full and faithful functors reflect isomorphisms. Let’s go through the typical proof for the ordinary
category case.</p>
<p>Suppose we have an natural transformation |\varphi : \mathcal D(FA, FB) \to \mathcal C(A, B)| natural in |A| and |B| such that |\varphi|
is an inverse to |F|, i.e. the action of the functor |F| on arrows. If |Ff \circ Fg = id| and |Fg \circ Ff = id|, then by the naturality
of |\varphi|, |\varphi(id) = \varphi(Ff \circ id \circ Fg) = f \circ \varphi(id) \circ g| and similarly with |f| and |g| switched.
We now just need to show that |\varphi(id) = id| but |id = F(id)|, so |\varphi(id) = \varphi(F(id)) = id|. |\square|</p>
<p>Now in the internal language. We’ll start with the theory of a |\V|-functor, so we have |\O|, |\O’|, |\A|, |\A’|, and |\mathsf F|.
While the previous paragraph talks about a natural transformation, we can readily see that it’s really a multimorphism. In our case, it
is a |\V|-multimorphism |\varphi| from |\A’(\mathsf F, \mathsf F)| to |\A|. Before we do that though, we need to show that |\mathsf F|
itself is a |\V|-multimorphism. This corresponds to the naturality of the action on arrows of |F| which we took for granted in the previous
paragraph. This is quickly verified: the external equivariance equations are just the functor law for composites. The additional data we have
is two linear terms |\mathsf f| and |\mathsf g| such that |\mathsf F(\mathsf f) \circ \mathsf F(\mathsf g) = \mathsf{id}|
and |\mathsf F(\mathsf g) \circ \mathsf F(\mathsf f) = \mathsf{id}|. Also, |\varphi(\mathsf F(h)) = h|.
The result follows through almost identically to the previous paragraph.
|\varphi(\mathsf{id}) = \varphi(\mathsf F(\mathsf f) \circ \mathsf F(\mathsf g)) = \varphi(\mathsf F(\mathsf f) \circ \mathsf{id} \circ \mathsf F(\mathsf g))|,
we apply external equivariance twice to get |\mathsf f \circ \varphi(\mathsf{id}) \circ \mathsf g|. The functor law for |\mathsf{id}|
gives |\varphi(\mathsf{id}) = \varphi(\mathsf F(\mathsf{id})) = \mathsf{id}|. A quick glance verifies that all these
equations use their free variables linearly as required. |\square|</p>
<p>As a warning, in the above |\mathsf f| and |\mathsf g| are not free variables but constants, i.e. primitive linear terms. Thus
there is no issue with an equation like |\mathsf F(\mathsf f) \circ \mathsf F(\mathsf g) = \mathsf{id}| as both sides have no free variables.</p>
<p>This is a very basic result but, again, the payoff here is how boring and similar to the usual case this is. For contrast, the definition
of an internal profunctor is given <a href="https://ncatlab.org/nlab/show/internal+profunctor">here</a>. This definition is easier to
connect to our notion of |\V|-presheaf, specifically a |\mathcal Self(\mathbf S)|-presheaf, than it is to the usual |\mathbf{Set}|-valued
functor definition. While not hard, it would take me a bit of time to even formulate the above proposition, and a proof in terms
of the explicit definitions would be hard to recognize as just the ordinary proof.</p>
<p>For fun, let’s figure out what the |\mathcal Const(\mathbf{Ab})| case of this result says explicitly.
A |\mathcal Const(\mathbf{Ab})|-category is a ring, a |\mathcal Const(\mathbf{Ab})|-functor is a ring homomorphism, and
a |\mathcal Const(\mathbf{Ab})|-profunctor is a bimodule. Let |R| and |S| be rings
and |f : R \to S| be a ring homomorphism. An isomorphism in |R| viewed as a |\mathcal Const(\mathbf{Ab})|-category is just
an invertible element. Every ring, |R|, is an |R|-|R|-bimodule. Given any |S|-|S|-bimodule |P|, we have
an |R|-|R|-bimodule |f^*(P)| via restriction of scalars, i.e. |f^*(P)| has the same elements as |P| and for |p \in f^*(P)|,
|rpr’ = f(r)pf(r’)|. In particular, |f| gives rise to a bimodule homomorphism, i.e. a linear function, |f : R \to f^*(S)|
which corresponds to its action on arrows from the perspective of |f| as a |\mathcal Const(\mathbf{Ab})|-functor. If this
linear transformation has an inverse, then the above result states that when |f(r)| is invertible so is |r|. So to restate this
all in purely ring theoretic terms, given a ring homomorphism |f : R \to S| and an abelian group homomorphism |\varphi : S \to R|
satisfying |\varphi(f(rst)) = r\varphi(f(s))t| and |\varphi(f(r)) = r|, then if |f(r)| is invertible so is |r|.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Indexed categories are equivalent to <em>cloven</em> fibrations and, if you have the Axiom of Choice, all fibrations can
be cloven. Indexed categories can be viewed as <em>presentations</em> of fibrations.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This
suggests that we could define a small |\V|-category |\mathcal C \otimes \mathcal D| where |\mathcal C| and |\mathcal D|
are small |\V|-categories. Start formulating a definition of such a |\V|-category. You will get stuck. Where? Why? This implies
that the (ordinary, or better, 2-)category of small |\V|-categories does not have a monoidal product with |\mathbb I| as
unit in general.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>With a good understanding of what a class is, it’s clear that it doesn’t even make
sense to have a proper class be an object. In frameworks with an explicit notion of ”class”, this is often manifested by
saying that a class that is an element of another class is a set (and thus not a proper class).<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>This suggests that it might be interesting to consider categories that are (co)complete with
respect to this monoid notion of “small”. I don’t think I’ve ever seen a study of such categories. (Co)limits of
monoids are <a href="https://doi.org/10.1007/BFb0084220">not trivial</a>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>This is one of the main things I like about working
in weak foundations. It forces you to come up with better definitions that make it clear what is and is not
important and eliminates coincidences. Of course, it also produces definitions and theorems that are inherently
more general too.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This connection isn’t
much of a surprise as the tensor product of modules is exactly the (small) |\mathcal Const(\mathbf{Ab})| case of this.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>

</feed>
