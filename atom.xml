<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Hedonistic Learning</title>
    <link href="https://derekelkins.github.io/atom.xml" rel="self" />
    <link href="https://derekelkins.github.io" />
    <id>https://derekelkins.github.io/atom.xml</id>
    <author>
        <name>Derek Elkins</name>
        <email>derek.a.elkins+blog@gmail.com</email>
    </author>
    <updated>2020-04-30T06:06:52Z</updated>
    <entry>
    <title>Recursive Helping</title>
    <link href="https://derekelkins.github.io/posts/recursive-helping.html" />
    <id>https://derekelkins.github.io/posts/recursive-helping.html</id>
    <published>2020-04-29 23:06:52-07:00</published>
    <updated>2020-04-30T06:06:52Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Recursive helping is a technique for implementing lock-free concurrent data structures and algorithms. I’m going to illustrate this in the case of implementing a multi-variable compare-and-swap (MCAS) in terms of a single variable compare-and-swap. Basically everything I’m going to talk about comes from Keir Fraser’s PhD Thesis, <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-579.html">Practical Lock-Freedom</a> (2004) which I <strong>strongly</strong> recommend. Fraser’s thesis goes much further than this, e.g. fine-grained lock-free implementations of software transactional memory (STM)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Fraser went on to contribute to the initial implementations of STM in Haskell, though his thesis uses C++.</p>
<!--more-->
<p>First, some prerequisites.</p>
<h2 id="terms">Terms</h2>
<p>I imagine most developers when they hear the term “lock-free” take it to mean a concurrent algorithm implemented without using locks. It, however, has a technical definition. Assuming a concurrent application is functionally correct, e.g. it does the right thing if it terminates no matter how things are scheduled, we still have three liveness problems in decreasing order of severity:</p>
<ul>
<li><strong>Deadlock</strong> - the application getting stuck in state where no subprocesses can be scheduled</li>
<li><strong>Livelock</strong> - the application fails to make progress despite subprocesses being scheduled, e.g. endlessly retrying</li>
<li><strong>Starvation</strong> - some subprocesses never make progress even though the application as a whole makes progress</li>
</ul>
<p>In parallel, we have three properties corresponding to programs that cannot exhibit the above behaviors:</p>
<ul>
<li><strong>Obstruction-free</strong> - no deadlock, you’ll get this if you don’t use locks</li>
<li><strong>Lock-free</strong> - obstruction-free and no livelock</li>
<li><strong>Wait-free</strong> - lock-free and no starvation</li>
</ul>
<p>Wait-freedom is the most desirable property but was difficult to achieve with reasonable efficiency. However, <a href="https://dl.acm.org/doi/10.1145/2692916.2555261">relatively recent techniques</a> (2014) may do for wait-free algorithms what Fraser’s thesis did for lock-free algorithms, namely reduce a research problem to an exercise. These techniques, however, still start from a lock-free algorithm. Obstruction-freedom is usually what you get from concurrency control mechanisms that abort and retry in the case of conflicts. To achieve lock-freedom, we need to avoid losing and redoing work, or at least doing so repeatedly indefinitely.</p>
<p>See Fraser’s thesis for more formal definitions.</p>
<p>I’ll use “<strong>lockless</strong>” to mean an algorithm implemented without using locks.</p>
<h2 id="compare-and-swap">Compare-and-Swap</h2>
<p>The usual primitive used to implement and describe lockless algorithms is <a href="https://en.wikipedia.org/wiki/Compare-and-swap">compare-and-swap</a>, often just called <code>cas</code>. There are other possibilities, but <code>cas</code> is <a href="https://dl.acm.org/doi/10.1145/114005.102808">universal</a>, relatively simple, and widely implemented, e.g. as the <code>cmpxchg</code> operation for the x86 architecture. The following is a specification of a <code>cas</code> operation in Haskell with the additional note that this intended to be performed atomically (which it would not be in Haskell).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1"></a><span class="ot">cas ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> <span class="dt">IORef</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> a</span>
<span id="cb1-2"><a href="#cb1-2"></a>cas ref old new <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>    curr <span class="ot">&lt;-</span> readIORef ref</span>
<span id="cb1-4"><a href="#cb1-4"></a>    <span class="kw">if</span> curr <span class="op">==</span> old <span class="kw">then</span> <span class="kw">do</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>        writeIORef ref new</span>
<span id="cb1-6"><a href="#cb1-6"></a>        <span class="fu">return</span> curr</span>
<span id="cb1-7"><a href="#cb1-7"></a>    <span class="kw">else</span> <span class="kw">do</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>        <span class="fu">return</span> curr</span></code></pre></div>
<h2 id="specification-of-multiple-compare-and-swap">Specification of Multiple Compare-and-Swap</h2>
<p>The specification of multiple compare-and-swap is the straightforward extension to the above to several variables.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1"></a><span class="ot">mcasSpec ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> [(<span class="dt">IORef</span> a, a, a)] <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Bool</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>mcasSpec entries <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>    eqs <span class="ot">&lt;-</span> forM entries <span class="op">$</span> \(ref, old, _) <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>        curr <span class="ot">&lt;-</span> readIORef ref</span>
<span id="cb2-5"><a href="#cb2-5"></a>        <span class="fu">return</span> (curr <span class="op">==</span> old)</span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="kw">if</span> <span class="fu">and</span> eqs <span class="kw">then</span> <span class="kw">do</span> <span class="co">-- if all equal</span></span>
<span id="cb2-7"><a href="#cb2-7"></a>        forM_ entries <span class="op">$</span> \(ref, _, new) <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb2-8"><a href="#cb2-8"></a>            writeIORef ref new</span>
<span id="cb2-9"><a href="#cb2-9"></a>        <span class="fu">return</span> <span class="dt">True</span></span>
<span id="cb2-10"><a href="#cb2-10"></a>    <span class="kw">else</span> <span class="kw">do</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>        <span class="fu">return</span> <span class="dt">False</span></span></code></pre></div>
<p>The above is, again, intended to be executed atomically. It will be convenient to allow a bit more flexibility in the type producing the type:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1"></a><span class="ot">mcas ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> [(<span class="dt">MCASRef</span> a, a, a)] <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Bool</span></span></code></pre></div>
<p>where we have</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1"></a><span class="co">-- Abstract.</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="kw">newtype</span> <span class="dt">MCASRef</span> a <span class="ot">=</span> <span class="dt">MCASRef</span> {<span class="ot"> unMCASRef ::</span> <span class="dt">IORef</span> (<span class="dt">Either</span> (<span class="dt">T</span> a) a) } <span class="kw">deriving</span> ( <span class="dt">Eq</span> )</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="ot">newMCASRef ::</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">MCASRef</span> a)</span>
<span id="cb4-5"><a href="#cb4-5"></a>newMCASRef v <span class="ot">=</span> <span class="dt">MCASRef</span> <span class="op">&lt;$&gt;</span> newIORef (<span class="dt">Right</span> v)</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="ot">readMCASRef ::</span> <span class="dt">MCASRef</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> a</span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="co">-- Will be implemented below.</span></span></code></pre></div>
<p>The idea here is that, in addition to values of type <code>a</code>, we can also store values of type <code>T a</code> into the pointers for internal use, and we can unambiguously distinguish them from values of type <code>a</code>. <code>T a</code> can be any type constructor you like.</p>
<p>In the code below, I will assume <code>IORef</code>s have an <code>Ord</code> instance, i.e. that they can be sorted. This is <em>not</em> true, but an approach as in <a href="https://hackage.haskell.org/package/ioref-stable-0.1.1.0">ioref-stable</a> could be used to accomplish this. Alternatively, <code>Ptr</code>s to <code>StablePtr</code>s could be used.</p>
<p>We won’t worry about memory consistency concerns here. That is, we’ll assume sequential consistency where all CPU cores see all updates immediately.</p>
<p>I recommend stopping here and thinking about how you would implement <code>mcas</code> in terms of <code>cas</code> while, of course, achieving the desired atomicity. The solution I’ll present – the one from Fraser’s thesis – is moderately involved, so if the approach you come up with is very simple, then you’ve probably made a mistake. Unsurprisingly, the solution I’ll present makes use of the additional flexibility in the type and the ability to sort <code>IORef</code>s. I’m not claiming it is impossible to accomplish this without these though. For example, you could apply a universal construction which witnesses the universality of <code>cas</code>.</p>
<h2 id="recursive-helping">Recursive Helping</h2>
<p>Lockless algorithms typically proceed by attempting an operation and detecting conflicts, e.g. as in multi-version concurrency control. This requires storing enough information to tell that a conflicting operation has occurred/is occurring. Once a conflict is detected, the simplest solution is typically to abort and retry hoping that there isn’t a conflict the next time. This clearly leads to the possibility of livelock.</p>
<p>Instead of aborting, the later invocation of the operation could instead help the earlier one to complete, thereby getting it out of its way. This ensures that the first invocation will always, eventually complete giving us lock-freedom. However, this doesn’t guarantee that once the second invocation finishes helping the first invocation that a third invocation won’t jump in before the second invocation gets a chance to start on its own work. In this case, the second invocation will help the third invocation to complete before attempting to start itself. A process can end up spending all its time helping other processes while never getting its own work done, leading to starvation.</p>
<p>To perform recursive helping, we need an invocation to store enough information so that subsequent, overlapping invocations are able to assist. To accomplish this, we’ll store a (pointer to a) “descriptor” containing the parameters of the invocation being helped and potentially additional information<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. This is what we’ll use for the <code>T</code> type constructor.</p>
<p>The general approach will be: at the beginning of the operation we will attempt to “install” a descriptor in the first field we touch utilizing <code>cas</code>. There are then three possible outcomes. If we fail and find a value, then the operation has failed. If we fail and find an existing descriptor, then we (potentially recursively) help that descriptor. If we succeed, then we have successfully “acquired” the field and we “help” ourselves. We can have many processes all trying to help the same invocation at the same time, so it is still important that multiple identical help calls don’t interfere with each other. Just because we’re helping an invocation of an operation doesn’t mean that that the original process isn’t still executing.</p>
<p>Since we’ll be replacing pointers to values with pointers to descriptors, reading the value becomes non-trivial. In particular, if when we read a pointer we get a descriptor, we’ll need to help the invocation described to completion. We will need to keep doing this until we successfully read a value.</p>
<h2 id="conditional-compare-and-swap">Conditional Compare-and-Swap</h2>
<p>An operation we’ll use in the implementations of <code>mcas</code> is a conditional compare-and-swap (CCAS) where we only perform the swap if, additionally, an additional variable is set to a given value. It has the following specification.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">-- Specification. Implementations should perform this atomically.</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="ot">ccasSpec ::</span> (<span class="dt">Eq</span> a, <span class="dt">Eq</span> c) <span class="ot">=&gt;</span> <span class="dt">IORef</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IORef</span> c <span class="ot">-&gt;</span> c <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb5-3"><a href="#cb5-3"></a>ccasSpec ref old new condRef check <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>    curr <span class="ot">&lt;-</span> readIORef ref</span>
<span id="cb5-5"><a href="#cb5-5"></a>    cond <span class="ot">&lt;-</span> readIORef condRef</span>
<span id="cb5-6"><a href="#cb5-6"></a>    <span class="kw">if</span> cond <span class="op">==</span> check <span class="op">&amp;&amp;</span> curr <span class="op">==</span> old <span class="kw">then</span> <span class="kw">do</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>        writeIORef ref new</span>
<span id="cb5-8"><a href="#cb5-8"></a>    <span class="kw">else</span> <span class="kw">do</span></span>
<span id="cb5-9"><a href="#cb5-9"></a>        <span class="fu">return</span> ()</span></code></pre></div>
<p>We’ll need to show that this can be implemented in terms of <code>cas</code>, or rather a version with modifications similar to those mentioned for <code>mcas</code>. This will be a simple instance of the recursive helping approach that will be applied in the implementation of <code>mcas</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">type</span> <span class="dt">CCASDescriptor</span> a c <span class="ot">=</span> <span class="dt">IORef</span> (<span class="dt">CCASRef</span> a c, a, a, <span class="dt">IORef</span> c, c)</span>
<span id="cb6-2"><a href="#cb6-2"></a></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="kw">newtype</span> <span class="dt">CCASRef</span> a c <span class="ot">=</span> <span class="dt">CCASRef</span> {<span class="ot"> unCCASRef ::</span> <span class="dt">IORef</span> (<span class="dt">Either</span> (<span class="dt">CCASDescriptor</span> a c) a) } <span class="kw">deriving</span> ( <span class="dt">Eq</span>, <span class="dt">Ord</span> )</span></code></pre></div>
<p>We begin with the types. As described above, a <code>CCASRef</code> is just an <code>IORef</code> that holds either a value or a descriptor, and the descriptor is just an <code>IORef</code> pointing at a tuple holding the arguments to <code>ccas</code>. We won’t actually modify this latter <code>IORef</code> and instead are just using it for its object identity. It could be replaced with a read-only <code>IVar</code> or a <code>Unique</code> could be allocated and used as an identifier instead. In a lower-level language, this <code>IORef</code> corresponds to having a pointer to the descriptor.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1"></a><span class="ot">newCCASRef ::</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">CCASRef</span> a c)</span>
<span id="cb7-2"><a href="#cb7-2"></a>newCCASRef v <span class="ot">=</span> <span class="dt">CCASRef</span> <span class="op">&lt;$&gt;</span> newIORef (<span class="dt">Right</span> v)</span>
<span id="cb7-3"><a href="#cb7-3"></a></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="ot">readCCASRef ::</span> (<span class="dt">Eq</span> a, <span class="dt">Eq</span> c) <span class="ot">=&gt;</span> <span class="dt">CCASRef</span> a c <span class="ot">-&gt;</span> <span class="dt">IO</span> a</span>
<span id="cb7-5"><a href="#cb7-5"></a>readCCASRef ref <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>    x <span class="ot">&lt;-</span> readIORef (unCCASRef ref)</span>
<span id="cb7-7"><a href="#cb7-7"></a>    <span class="kw">case</span> x <span class="kw">of</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>        <span class="dt">Right</span> v <span class="ot">-&gt;</span> <span class="fu">return</span> v</span>
<span id="cb7-9"><a href="#cb7-9"></a>        <span class="dt">Left</span> d <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb7-10"><a href="#cb7-10"></a>            ccasHelp d</span>
<span id="cb7-11"><a href="#cb7-11"></a>            readCCASRef ref</span>
<span id="cb7-12"><a href="#cb7-12"></a></span>
<span id="cb7-13"><a href="#cb7-13"></a><span class="co">-- Not atomic. This CAS can fail even when it would be impossible if `ccas` was truly atomic.</span></span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="co">-- Example: ccas a reference to the same value but where the condRef is False. The ccas fails</span></span>
<span id="cb7-15"><a href="#cb7-15"></a><span class="co">-- and thus should behave as a no-op, but if a `casCCASRef` occurs during the course of the ccas,</span></span>
<span id="cb7-16"><a href="#cb7-16"></a><span class="co">-- the `casCCASRef` can fail even though it should succeed in all cases.</span></span>
<span id="cb7-17"><a href="#cb7-17"></a><span class="ot">casCCASRef ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> <span class="dt">CCASRef</span> a c <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Bool</span></span>
<span id="cb7-18"><a href="#cb7-18"></a>casCCASRef (<span class="dt">CCASRef</span> ref) old new <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb7-19"><a href="#cb7-19"></a>    curr <span class="ot">&lt;-</span> cas ref (<span class="dt">Right</span> old) (<span class="dt">Right</span> new)</span>
<span id="cb7-20"><a href="#cb7-20"></a>    <span class="fu">return</span> (curr <span class="op">==</span> <span class="dt">Right</span> old)</span>
<span id="cb7-21"><a href="#cb7-21"></a></span>
<span id="cb7-22"><a href="#cb7-22"></a><span class="ot">tryReadCCASRef ::</span> <span class="dt">CCASRef</span> a c <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">Maybe</span> a)</span>
<span id="cb7-23"><a href="#cb7-23"></a>tryReadCCASRef ref <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb7-24"><a href="#cb7-24"></a>    x <span class="ot">&lt;-</span> readIORef (unCCASRef ref)</span>
<span id="cb7-25"><a href="#cb7-25"></a>    <span class="fu">return</span> (<span class="kw">case</span> x <span class="kw">of</span> <span class="dt">Left</span> _ <span class="ot">-&gt;</span> <span class="dt">Nothing</span>; <span class="dt">Right</span> v <span class="ot">-&gt;</span> <span class="dt">Just</span> v)</span></code></pre></div>
<p>To get them out of the way, the following functions implement the reference-like aspects of a <code>CCASRef</code>. The descriptor is an internal implementation detail. The interface is meant to look like a normal reference to a value of type <code>a</code>. The main notes are:</p>
<ul>
<li>since the <code>CCASRef</code> may not contain a value when we read, we loop helping to complete the <code>ccas</code> until it does,</li>
<li><code>casCCASRef</code> is a slightly simplified <code>cas</code> used in<code>mcas</code> but should be not be considered part of the interface, and</li>
<li><code>tryReadCCASRef</code> is used in the implementation of <code>mcas</code>, but you quite possibly wouldn’t provide it otherwise.</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb8-1"><a href="#cb8-1"></a><span class="ot">ccas ::</span> (<span class="dt">Eq</span> a, <span class="dt">Eq</span> c) <span class="ot">=&gt;</span> <span class="dt">CCASRef</span> a c <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IORef</span> c <span class="ot">-&gt;</span> c <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb8-2"><a href="#cb8-2"></a>ccas ref old new condRef check <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>    d <span class="ot">&lt;-</span> newIORef (ref, old, new, condRef, check)</span>
<span id="cb8-4"><a href="#cb8-4"></a>    v <span class="ot">&lt;-</span> cas (unCCASRef ref) (<span class="dt">Right</span> old) (<span class="dt">Left</span> d)</span>
<span id="cb8-5"><a href="#cb8-5"></a>    go d v</span>
<span id="cb8-6"><a href="#cb8-6"></a>  <span class="kw">where</span> go d (<span class="dt">Left</span> d&#39;) <span class="ot">=</span> <span class="kw">do</span> <span class="co">-- descriptor already there</span></span>
<span id="cb8-7"><a href="#cb8-7"></a>            ccasHelp d&#39;</span>
<span id="cb8-8"><a href="#cb8-8"></a>            v <span class="ot">&lt;-</span> cas (unCCASRef ref) (<span class="dt">Right</span> old) (<span class="dt">Left</span> d)</span>
<span id="cb8-9"><a href="#cb8-9"></a>            go d v</span>
<span id="cb8-10"><a href="#cb8-10"></a>        go d (<span class="dt">Right</span> curr) <span class="op">|</span> curr <span class="op">==</span> old <span class="ot">=</span> ccasHelp d <span class="co">-- we succeeded</span></span>
<span id="cb8-11"><a href="#cb8-11"></a>                          <span class="op">|</span> <span class="fu">otherwise</span> <span class="ot">=</span> <span class="fu">return</span> ()   <span class="co">-- we failed</span></span>
<span id="cb8-12"><a href="#cb8-12"></a>    </span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="ot">ccasHelp ::</span> (<span class="dt">Eq</span> a, <span class="dt">Eq</span> c) <span class="ot">=&gt;</span> <span class="dt">CCASDescriptor</span> a c <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb8-14"><a href="#cb8-14"></a>ccasHelp d <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb8-15"><a href="#cb8-15"></a>    (<span class="dt">CCASRef</span> ref, old, new, condRef, check) <span class="ot">&lt;-</span> readIORef d</span>
<span id="cb8-16"><a href="#cb8-16"></a>    cond <span class="ot">&lt;-</span> readIORef condRef</span>
<span id="cb8-17"><a href="#cb8-17"></a>    _ <span class="ot">&lt;-</span> cas ref (<span class="dt">Left</span> d) (<span class="dt">Right</span> <span class="op">$!</span> <span class="kw">if</span> cond <span class="op">==</span> check <span class="kw">then</span> new <span class="kw">else</span> old)</span>
<span id="cb8-18"><a href="#cb8-18"></a>    <span class="fu">return</span> ()</span></code></pre></div>
<p>Here we illustrate the (not so recursive) helping pattern. <code>ccas</code> allocates a descriptor and then attempts to “acquire” the reference. There are three possibilities.</p>
<ol type="1">
<li>We find a descriptor already there, in which case we help it and then try to acquire the reference again.</li>
<li>The CAS succeeds and thus we successfully “acquire” the reference. We then “help ourselves”.</li>
<li>The CAS fails with an unexpected (non-descriptor) value. Thus, the CCAS fails and we do nothing.</li>
</ol>
<p>Helping, implemented by <code>ccasHelp</code>, just performs the logic of CCAS. If we’ve gotten to <code>ccasHelp</code>, we know the invocation described by the descriptor did, in fact, find the expected value there. By installing our descriptor, we’ve effectively “locked out” any other calls to <code>ccas</code> until we complete. We can thus check the <code>condRef</code> at our leisure. As long as our descriptor is still in the <code>CCASRef</code>, which we check via a <code>cas</code>, we know that there have been no intervening operations, including other processes completing this <code>ccas</code>. <code>ccasHelp</code> is idempotent in the sense that running it multiple times, even in parallel, with the same descriptor is the same as running it once. This is due to the fact that we only (successfully) CAS in the descriptor once, so we can only CAS it out at most once.</p>
<h2 id="multiple-compare-and-swap">Multiple Compare-and-Swap</h2>
<p>The setup for MCAS is much the same as CCAS. The main additional complexity comes from the fact that we need to simultaneously “acquire” multiple references. This is handled by a two-phase approach. In the first phase, we attempt to “acquire” each reference. We proceed to the second phase once we’ve either seen that the MCAS is going to fail, or we have successfully “acquired” each reference. In the second phase, we either reset all the “acquired” references to their old values if the MCAS failed or to their new values if it succeeded. The MCAS will be considered to have occurred atomically at the point we record this decision, via a CAS, i.e. between the two phases.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">data</span> <span class="dt">MCASStatus</span> <span class="ot">=</span> <span class="dt">UNDECIDED</span> <span class="op">|</span> <span class="dt">FAILED</span> <span class="op">|</span> <span class="dt">SUCCESSFUL</span> <span class="kw">deriving</span> ( <span class="dt">Eq</span> )</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="kw">data</span> <span class="dt">MCASDescriptor&#39;</span> a <span class="ot">=</span> <span class="dt">MCASDescriptor</span> [(<span class="dt">MCASRef</span> a, a, a)] (<span class="dt">IORef</span> <span class="dt">MCASStatus</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="kw">type</span> <span class="dt">MCASDescriptor</span> a <span class="ot">=</span> <span class="dt">IORef</span> (<span class="dt">MCASDescriptor&#39;</span> a)</span>
<span id="cb9-5"><a href="#cb9-5"></a></span>
<span id="cb9-6"><a href="#cb9-6"></a><span class="kw">newtype</span> <span class="dt">MCASRef</span> a <span class="ot">=</span> <span class="dt">MCASRef</span> {<span class="ot"> unMCASRef ::</span> <span class="dt">CCASRef</span> (<span class="dt">Either</span> (<span class="dt">MCASDescriptor</span> a) a) <span class="dt">MCASStatus</span> }</span>
<span id="cb9-7"><a href="#cb9-7"></a>    <span class="kw">deriving</span> ( <span class="dt">Eq</span>, <span class="dt">Ord</span> )</span></code></pre></div>
<p>As with CCAS, an <code>MCASRef</code> is a reference, in this case a <code>CCASRef</code>, that either holds a value or a descriptor. The descriptor holds the arguments of <code>mcas</code>, as with <code>ccas</code>, but it additionally holds a status reference. This status reference will be used as the condition reference of the CCAS. In particular, as we’ll see, we will only perform <code>ccas</code>’s when the status is <code>UNDECIDED</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb10-1"><a href="#cb10-1"></a><span class="ot">newMCASRef ::</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">MCASRef</span> a)</span>
<span id="cb10-2"><a href="#cb10-2"></a>newMCASRef v <span class="ot">=</span> <span class="dt">MCASRef</span> <span class="op">&lt;$&gt;</span> newCCASRef (<span class="dt">Right</span> v)</span>
<span id="cb10-3"><a href="#cb10-3"></a></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="ot">readMCASRef ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> <span class="dt">MCASRef</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> a</span>
<span id="cb10-5"><a href="#cb10-5"></a>readMCASRef ref <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb10-6"><a href="#cb10-6"></a>    x <span class="ot">&lt;-</span> readCCASRef (unMCASRef ref)</span>
<span id="cb10-7"><a href="#cb10-7"></a>    <span class="kw">case</span> x <span class="kw">of</span></span>
<span id="cb10-8"><a href="#cb10-8"></a>        <span class="dt">Right</span> v <span class="ot">-&gt;</span> <span class="fu">return</span> v</span>
<span id="cb10-9"><a href="#cb10-9"></a>        <span class="dt">Left</span> d <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb10-10"><a href="#cb10-10"></a>            mcasHelp d</span>
<span id="cb10-11"><a href="#cb10-11"></a>            readMCASRef ref</span></code></pre></div>
<p>There’s nothing to say about the reference interface functions. They are essentially identical to the CCAS ones for the same reasons only with <code>CCASRef</code>s instead of <code>IORef</code>s.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb11-1"><a href="#cb11-1"></a><span class="ot">mcas ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> [(<span class="dt">MCASRef</span> a, a, a)] <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Bool</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>mcas entries <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>    status <span class="ot">&lt;-</span> newIORef <span class="dt">UNDECIDED</span></span>
<span id="cb11-4"><a href="#cb11-4"></a>    d <span class="ot">&lt;-</span> newIORef (<span class="dt">MCASDescriptor</span> (sortOn (\(ref, _, _) <span class="ot">-&gt;</span> ref) entries) status)</span>
<span id="cb11-5"><a href="#cb11-5"></a>    mcasHelp d</span></code></pre></div>
<p>The <code>mcas</code> function is fairly straightforward. It allocates a status reference and a descriptor and delegates most of the work to <code>mcasHelp</code>. The main but critical subtlety is the sort. This is critical to ensuring termination.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb12-1"><a href="#cb12-1"></a><span class="ot">mcasHelp ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> <span class="dt">MCASDescriptor</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Bool</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>mcasHelp d <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>    <span class="dt">MCASDescriptor</span> entries statusRef <span class="ot">&lt;-</span> readIORef d</span>
<span id="cb12-4"><a href="#cb12-4"></a>    <span class="kw">let</span> phase1 [] <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb12-5"><a href="#cb12-5"></a>            _ <span class="ot">&lt;-</span> cas statusRef <span class="dt">UNDECIDED</span> <span class="dt">SUCCESSFUL</span></span>
<span id="cb12-6"><a href="#cb12-6"></a>            phase2</span>
<span id="cb12-7"><a href="#cb12-7"></a>        phase1 ((<span class="dt">MCASRef</span> ref, old, new)<span class="op">:</span>es) <span class="ot">=</span> tryAcquire ref old new es</span>
<span id="cb12-8"><a href="#cb12-8"></a></span>
<span id="cb12-9"><a href="#cb12-9"></a>        tryAcquire ref old new es <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb12-10"><a href="#cb12-10"></a>            _ <span class="ot">&lt;-</span> ccas ref (<span class="dt">Right</span> old) (<span class="dt">Left</span> d) statusRef <span class="dt">UNDECIDED</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>            v <span class="ot">&lt;-</span> tryReadCCASRef ref</span>
<span id="cb12-12"><a href="#cb12-12"></a>            <span class="kw">case</span> v <span class="kw">of</span></span>
<span id="cb12-13"><a href="#cb12-13"></a>                <span class="dt">Just</span> (<span class="dt">Left</span> d&#39;) <span class="op">|</span> d <span class="op">==</span> d&#39; <span class="ot">-&gt;</span> phase1 es <span class="co">-- successful acquisition</span></span>
<span id="cb12-14"><a href="#cb12-14"></a>                               <span class="op">|</span> <span class="fu">otherwise</span> <span class="ot">-&gt;</span> <span class="kw">do</span> <span class="co">-- help someone else</span></span>
<span id="cb12-15"><a href="#cb12-15"></a>                                    mcasHelp d&#39;</span>
<span id="cb12-16"><a href="#cb12-16"></a>                                    tryAcquire ref old new es</span>
<span id="cb12-17"><a href="#cb12-17"></a>                <span class="dt">Just</span> (<span class="dt">Right</span> curr) <span class="op">|</span> curr <span class="op">==</span> old <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb12-18"><a href="#cb12-18"></a>                    status <span class="ot">&lt;-</span> readIORef statusRef</span>
<span id="cb12-19"><a href="#cb12-19"></a>                    <span class="kw">if</span> status <span class="op">==</span> <span class="dt">UNDECIDED</span> <span class="kw">then</span> <span class="kw">do</span></span>
<span id="cb12-20"><a href="#cb12-20"></a>                        tryAcquire ref old new es <span class="co">-- failed to acquire but could still succeed</span></span>
<span id="cb12-21"><a href="#cb12-21"></a>                    <span class="kw">else</span> <span class="kw">do</span></span>
<span id="cb12-22"><a href="#cb12-22"></a>                        phase2</span>
<span id="cb12-23"><a href="#cb12-23"></a>                _ <span class="ot">-&gt;</span> <span class="kw">do</span> <span class="co">-- failed MCAS</span></span>
<span id="cb12-24"><a href="#cb12-24"></a>                    _ <span class="ot">&lt;-</span> cas statusRef <span class="dt">UNDECIDED</span> <span class="dt">FAILED</span></span>
<span id="cb12-25"><a href="#cb12-25"></a>                    phase2</span>
<span id="cb12-26"><a href="#cb12-26"></a></span>
<span id="cb12-27"><a href="#cb12-27"></a>        phase2 <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb12-28"><a href="#cb12-28"></a>            status <span class="ot">&lt;-</span> readIORef statusRef</span>
<span id="cb12-29"><a href="#cb12-29"></a>            <span class="kw">let</span> succeeded <span class="ot">=</span> status <span class="op">==</span> <span class="dt">SUCCESSFUL</span></span>
<span id="cb12-30"><a href="#cb12-30"></a>            forM_ entries <span class="op">$</span> \(<span class="dt">MCASRef</span> ref, old, new) <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb12-31"><a href="#cb12-31"></a>                casCCASRef ref (<span class="dt">Left</span> d) (<span class="dt">Right</span> (<span class="kw">if</span> succeeded <span class="kw">then</span> new <span class="kw">else</span> old))</span>
<span id="cb12-32"><a href="#cb12-32"></a>            <span class="fu">return</span> succeeded</span>
<span id="cb12-33"><a href="#cb12-33"></a></span>
<span id="cb12-34"><a href="#cb12-34"></a>    phase1 entries </span></code></pre></div>
<p><code>phase1</code> attempts to “acquire” each <code>MCASRef</code> by using <code>tryAcquire</code> which will move <code>phase1</code> to the next entry each time it succeeds. Therefore, if <code>phase1</code> reaches the end of the list and there was no interference, we will have successfully “acquired” all references. We record this with a CAS against <code>statusRef</code>. If this CAS succeeds, then the MCAS will be considered successful and conceptually to have occurred at this point. If the CAS fails, then some other process has already completed this MCAS, possibly in success or failure. We then move to <code>phase2</code>.</p>
<p><code>tryAcquire</code> can also detect that the MCAS should fail. In this case, we immediately attempt to record this fact via a CAS into <code>statusRef</code>. As with the successful case, this CAS succeeding marks the conceptual instant that the MCAS completes. As before, we then move on to <code>phase2</code>.</p>
<p>We never enter <code>phase2</code> without <code>statusRef</code> being set to either <code>SUCCESSFUL</code> or <code>FAILED</code>. <code>phase2</code> is completely straightforward. We simply set each “acquired” reference to either the new or old value depending on whether the MCAS succeeded or not. The <code>casCCASRef</code> will fail if either we never got around to “acquiring” a particular reference (in the case of MCAS failure), or if a reference was written to since it was “acquired”. Since such writes conceptually occurred after the MCAS completed, we do not want to overwrite them.</p>
<p>During <code>tryAcquire</code>, there are a few cases that lead to retrying. First, if we find that a reference has already been “acquired” by some other MCAS operation, we recursively help it. Here, the sorting of the references is important to ensure that any MCAS operation we help will never try to help us back. It’s easy to see that without the sorting, if two MCAS operations on the same two references each acquired one of the references, the (concurrent) recursive calls to <code>mcasHelp</code> would become infinite loops. With a total ordering on references, each recursive call to <code>mcasHelp</code> will be at a greater reference and thus must eventually terminate. The other case for <code>tryAcquire</code>, is that the expected value is written after the <code>ccas</code> but before the <code>tryReadCCASRef</code>. In this case, we try again unless the status has already been decided. It might seem like this is just an optimization, and that we could instead treat this as the MCAS failing. However, the intervening write may have written the value that was there before the <code>ccas</code>, meaning that there was never a point at which the MCAS could have failed.</p>
<p>References are only “acquired” at the <code>ccas</code> in <code>phase1</code>. Once the status has been decided, no references may be “acquired” any longer. Since it’s impossible to enter <code>phase2</code> without deciding the status, once one process enters <code>phase2</code>, no processes are capable of “acquiring” references. This makes <code>phase2</code> idempotent and, indeed, each CAS in <code>phase2</code> is independently idempotent. Overlapping executions of <code>phase1</code> are fine essentially because each <code>ccas</code> is idempotent and the <code>statusRef</code> can change at most once.</p>
<p>Let’s see how an <code>mcas</code> interacts with other operations from the perspective of atomicity. If we attempt to read a reference via <code>readMCASRef</code> which is included in the list of references of an ongoing <code>mcas</code>, there are two possibilities. Either that reference has not yet been “acquired” by the <code>mcas</code>, in which case the read will occur conceptually before the MCAS, or it has been “acquired” in which case the read will help the MCAS to completion and then try again after the MCAS. The story is similar for overlapping <code>mcas</code>’s. The <code>mcas</code> which “acquires” the least reference in their intersection will conceptually complete first, either because it literally finishes before the second <code>mcas</code> notices or because the second <code>mcas</code> will help it to completion. Writes are only slightly different.</p>
<p>It is important to note that these operations are NOT atomic with respect to some other reasonable operations. Most notably, they are not atomic with respect to blind writes. It is easy to construct a scenario where two blind writes happen in sequence but the first appears to happen after the <code>mcas</code> and the second before. Except for initialization, I don’t believe there are any blind writes to references involved in a <code>mcas</code> in Fraser’s thesis. Fraser’s thesis does, however, contain <code>cas</code> operations directly against these references. These are also not atomic for exactly the same reason <code>casCCASRef</code> isn’t. That said, Fraser’s uses of <code>cas</code> against <code>MCASRef</code>s are safe, because in each case they just retry until success.</p>
<h2 id="conclusion">Conclusion</h2>
<p>While I’ve gone into a good amount of detail here, I’ve mainly wanted to illustrate the concept of recursive helping. It’s a key concept for lock-free and wait-free algorithm designs, but it also may be a useful idea to have in mind when designing concurrent code even if you aren’t explicitly trying to achieve a lock-free guarantee.</p>
<!--

Scenario:

```               
               |--mcas((a, 1, 11), (b, 2, 22))---|
w(a,1)--w(b,1)----acq(a)------------------acq(b)--
--------------------------w(a,2)--w(b,2)----------
```

`w(a,2)` seems to happen after the `mcas` but `w(b,2)` seems to occur before.
-->
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>MCAS allows you to perform transactions involving multiple updates atomically. STM additionally allows you to perform transactions involving multiple reads and updates atomically.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>While I haven’t attempted it to see if it works out, it seems like you could make a generic “recursive helping” framework by storing an <code>IO</code> action instead. The “descriptors” have the flavor of defunctionalized continuations.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Beck-Chevalley</title>
    <link href="https://derekelkins.github.io/posts/beck-chevalley.html" />
    <id>https://derekelkins.github.io/posts/beck-chevalley.html</id>
    <published>2020-02-22 23:59:28-08:00</published>
    <updated>2020-02-23T07:59:28Z</updated>
    <summary type="html"><![CDATA[<p>This is a fairly technical article. This article will most likely not have any significance for you if you haven’t heard of the Beck-Chevalley condition before.</p>
<h2 id="introduction">Introduction</h2>
<p>When one talks about “indexed (co)products” in an indexed category, it is often described as follows:</p>
<p>Let |\mathcal C| be an <a href="https://ncatlab.org/nlab/show/indexed+category"><strong>|\mathbf S|-indexed category</strong></a>, i.e. a <a href="https://ncatlab.org/nlab/show/pseudofunctor">pseudofunctor</a> |\mathbf S^{op} \to \mathbf{Cat}| where |\mathbf S| is an ordinary category. Write |\mathcal C^I| for |\mathcal C(I)| and |f^* : \mathcal C^J \to \mathcal C^I| for |\mathcal C(f)| where |f : I \to J|. The functors |f^*| will be called <strong>reindexing functors</strong>. |\mathcal C| has <strong>|\mathbf S|-indexed coproducts</strong> whenever</p>
<ol type="1">
<li>each reindexing functor |f^*| has a left adjoint |\Sigma_f|, and</li>
<li>the Beck-Chevalley condition holds, i.e. whenever <br /><span class="math display">$$\require{AMScd}\begin{CD}
   I @&gt;h&gt;&gt; J \\
   @VkVV @VVfV \\
   K @&gt;&gt;g&gt; L
\end{CD}$$</span><br /> is a pullback square in |\mathbf S|, then the canonical morphism |\Sigma_k \circ h^* \to g^* \circ \Sigma_f| is an isomorphism.</li>
</ol>
<p>The first condition is reasonable, especially motivated with some examples, but the second condition is more mysterious. It’s clear that you’d need <em>something</em> more than simply a family of adjunctions, but it’s not clear how you could calculate the particular condition quoted. That’s the goal of this article. I will not cover what the Beck-Chevalley condition is intuitively saying. I cover that in <a href="https://math.stackexchange.com/a/2203383">this Stack Exchange answer</a> from a logical perspective, though there are definitely other possible perspectives as well.</p>
<p>Some questions are:</p>
<ol type="1">
<li>Where does the Beck-Chevalley condition come from?</li>
<li>What is this “canonical morphism”?</li>
<li>Why do we care about pullback squares in particular?</li>
</ol>
<!--more-->
<h2 id="indexed-functors-and-indexed-natural-transformations">Indexed Functors and Indexed Natural Transformations</h2>
<p>The concepts we’re interested in will typically be characterized by universal properties, so we’ll want an indexed notion of adjunction. We can get that by instantiating the general definition of an adjunction in any bicategory if we can make a bicategory of indexed categories. This is pretty easy since indexed categories are already described as pseudofunctors which immediately suggests a natural notion of indexed functor would be a pseudonatural transformation.</p>
<p>Explicitly, given indexed categories |\mathcal C, \mathcal D : \mathbf S^{op} \to \mathbf{Cat}|, an <strong>indexed functor</strong> |F : \mathcal C \to \mathcal D| consists of a functor |F^I : \mathcal C^I \to \mathcal D^I| for each object |I| of |\mathbf S| and a natural isomorphism |F^f : \mathcal D(f) \circ F^J \cong F^I \circ \mathcal C(f)| for each |f : I \to J| in |\mathbf S|.</p>
<p>An indexed natural transformation corresponds to a <a href="https://ncatlab.org/nlab/show/modification">modification</a> which is the name for the 3-cells between the 2-cells in the 3-category of 2-categories. For us, this works out to be the following: for each object |I| of |\mathbf S|, we have a natural transformation |\alpha^I : F^I \to G^I| such that for each |f : I \to J| the following diagram commutes <br /><span class="math display">$$\begin{CD}
\mathcal D(f) \circ F^J @&gt;id_{\mathcal D(f)}*\alpha^J&gt;&gt; \mathcal D(f) \circ G^J \\
@V\cong VV @VV\cong V \\
F^I \circ \mathcal C(f) @&gt;&gt;\alpha^I*id_{\mathcal C(f)}&gt; G^I \circ \mathcal C(f)
\end{CD}$$</span><br /> where the isomorphisms are the isomorphisms from the pseudonaturality of |F| and |G|.</p>
<h2 id="indexed-adjunctions-and-beck-chevalley">Indexed Adjunctions and Beck-Chevalley</h2>
<p>Indexed adjunctions can now be defined via the <a href="https://ncatlab.org/nlab/show/adjunction#direct_definition">unit and counit definition</a> which works in any bicategory. In particular, since indexed functors consist of families of functors and indexed natural transformations consist of families of natural transformations, both indexed by the objects of |\mathbf S|, part of the data of an indexed adjunction is a family of adjunctions.</p>
<p>Let’s work out what the additional data is. First, to establish notation, we have indexed functor |F : \mathcal D\to \mathcal C| and |U : \mathcal C \to \mathcal D| such that |F \dashv U| in an indexed sense. That means we have |\eta : Id \to U \circ F| and |\varepsilon : F \circ U \to Id| as indexed natural transformations. The first pieces of additional data, then, are the fact that |F| and |U| are indexed functors, so we have natural isomorphisms |F^f : \mathcal C(f)\circ F^J \to F^I\circ \mathcal D(f)| and |U^f : \mathcal C(f) \circ U^J \to U^I \circ \mathcal D(f)| for each |f : I \to J| in |\mathbf S|. The next pieces of additional data, or rather constraints, are the coherence conditions on |\eta| and |\varepsilon|. These work out to <br /><span class="math display">$$\begin{gather}
U^I(F^f)^{-1} \circ \eta_{\mathcal D(f)}^I = U_{F^J}^f \circ \mathcal D(f)\eta^J \qquad\text{and}\qquad
\varepsilon_{\mathcal C(f)}^I \circ F^I U^f = \mathcal C(f)\varepsilon^J \circ (F_{U^J}^f)^{-1}
\end{gather}$$</span><br /></p>
<p>This doesn’t look too much like the example in the introduction, but maybe some of this additional data is redundant. If we didn’t already know where we end up, one hint would be that |(F^f)^{-1} : F^I \circ \mathcal C(f) \to \mathcal D(f) \circ F^J| and |U^f : \mathcal D(f) \circ U^J \to U^I \circ \mathcal C(f)| look like <a href="https://ncatlab.org/nlab/show/mate">mates</a>. Indeed, it would be quite nice if they were as mates uniquely determine each other and this would make the reindexing give rise to a morphism of adjunctions. Unsurprisingly, this is the case.</p>
<p>To recall, generally, given adjunctions |F \dashv U : \mathcal C \to \mathcal D| and |F’ \dashv U’ : \mathcal C’ \to \mathcal D’|, a <strong>morphism of adjunctions</strong> from the former to the latter is a pair of functors |K : \mathcal C \to \mathcal C’| and |L : \mathcal D \to \mathcal D’|, and a natural transformation |\lambda : F’ \circ L \to K \circ F| or, equivalently, a natural transformation |\mu : L \circ U \to U’ \circ K|. You can show that there is a bijection |[\mathcal D,\mathcal C’](F’\circ L, K \circ F) \cong [\mathcal C, \mathcal D’](L \circ U, U’ \circ K)|. Concretely, |\mu = U’K\varepsilon \circ U’\lambda_U \circ \eta’_{LU}| provides the mapping in one direction. The mapping in the other direction is similar, and we can prove it is a bijection using the triangle equalities. |\lambda| and |\mu| are referred to as <strong>mates</strong> of each other.</p>
<p>In our case, |K| and |L| will be reindexing functors |\mathcal C(f)| and |\mathcal D(f)| respectively for some |f : I \to J|. We need to show that the family of adjunctions and the coherence conditions on |\eta| and |\varepsilon| force |(F^f)^{-1}| and |U^f| to be mates. The proof is as follows: <br /><span class="math display">$$\begin{align}
&amp; U^I \mathcal C(f) \varepsilon^J \circ U^I(F_{U^J}^f)^{-1} \circ \eta_{\mathcal D(f)U^J}^I &amp; \qquad \{\text{coherence of }\eta \} \\
= \quad &amp; U^I \mathcal C(f) \varepsilon^J \circ U_{F^JU^J}^f \circ \mathcal D(f)\eta_{U^J}^J &amp; \qquad \{\text{naturality of }U^f \} \\
= \quad &amp; U^f \circ \mathcal D(f)U^J\varepsilon^J \circ \mathcal D(f)\eta_{U^J}^J &amp; \qquad \{\text{functoriality of }\mathcal D(f) \} \\
= \quad &amp; U^f \circ \mathcal D(f)(U^J\varepsilon^J \circ \eta_{U^J}^J) &amp; \qquad \{\text{triangle equality} \} \\
= \quad &amp; U^f &amp;
\end{align}$$</span><br /></p>
<p>The next natural question is: if we know |(F^f)^{-1}| and |U^f| are mates, do we still need the coherence conditions on |\eta| and |\varepsilon|? The answer is “no”. <br /><span class="math display">$$\begin{align}
&amp; U_{F^J}^f \circ \mathcal D(f)\eta^J &amp; \qquad \{\text{mate of }U^f \} \\
= \quad &amp; U^I \mathcal C(f) \varepsilon_{F^J}^J \circ U^I(F_{F^J}^f)^{-1} \circ \eta^I_{\mathcal D(f)U^I} \circ \mathcal D(f)\eta^J &amp; \{\text{naturality of }\eta^I \} \\
= \quad &amp; U^I \mathcal C(f) \varepsilon_{F^J}^J \circ U^I(F_{F^J}^f)^{-1} \circ U^I F^I D(f)\eta^J \circ \eta_{\mathcal D(f)}^I &amp; \{\text{naturality of }U^I(F^f)^{-1} \} \\
= \quad &amp; U^I \mathcal C(f) \varepsilon_{F^J}^J \circ U^I\mathcal C(f)F^J\eta^J \circ U^I (F^f)^{-1} \circ \eta_{\mathcal D(f)}^I &amp; \{\text{functoriality of }U^I\mathcal C(f) \} \\
= \quad &amp; U^I \mathcal C(f)(\varepsilon_{F^J}^J \circ F^J\eta^J) \circ U^I(F^f)^{-1} \circ \eta_{\mathcal D(f)}^I &amp; \{\text{triangle equality} \} \\
= \quad &amp; U^I (F^f)^{-1} \circ \eta_{\mathcal D(f)}^I &amp;
\end{align}$$</span><br /> Similarly for the other coherence condition.</p>
<p>We’ve shown that if |U| is an indexed functor it has a left adjoint exactly when each |U^I| has a left adjoint, |F^I|, <strong><em>and</em></strong> for each |f : I \to J|, the mate of |U^f| with respect to those adjoints, which will be |(F^f)^{-1}|, is invertible. This latter condition is the Beck-Chevalley condition. As you can quickly verify, an invertible natural transformation doesn’t imply that it’s mate is invertible. Indeed, if |F| and |F’| are left adjoints and |\lambda : F’\circ L \to K \circ F| is invertible, then |\lambda^{-1} : K \circ F \to F’ \circ L| is not of the right form to have a mate (unless |F| and |F’| are also right adjoints and, particularly, an adjoint equivalence if we want to get an inverse to the mate of |\lambda|).</p>
<h2 id="comprehension-categories">Comprehension Categories</h2>
<p>We’ve answered questions 1 and 2 from above, but 3 is still open, and we’ve generated a new question: what is the <em>indexed</em> functor whose left adjoint we’re finding? The family of reindexing functors isn’t indexed by objects of |\mathbf S| but, most obviously, by <em>arrows</em> of |\mathbf S|. To answer these questions, we’ll consider a more general notion of indexed (co)products.</p>
<p>A <a href="https://ncatlab.org/nlab/show/categorical+model+of+dependent+types#comprehension_categories"><strong>comprehension category</strong></a> is a functor |\mathcal P : \mathcal E \to \mathbf S^{\to}| (where |\mathbf S^{\to}| is the <a href="https://ncatlab.org/nlab/show/arrow+category">arrow category</a>) such that |p = \mathsf{cod} \circ \mathcal P| is a <a href="https://ncatlab.org/nlab/show/Grothendieck+fibration">(Grothendieck) fibration</a> and |\mathcal P| takes (|p|-)cartesian arrows of |\mathcal E| to pullback squares in |\mathbf S^{\to}|. It won’t be necessary to know what a fibration is, as we’ll need only a few simple examples, but fibrations provide a different, and in many ways better, perspective<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> on indexed categories and being able to move between the perspectives is valuable.</p>
<p>A comprehension category can also be presented as a natural transformation |\mathcal P : \{{-}\} \to p| where |\{{-}\}| is just another name for |\mathsf{dom} \circ \mathcal P|. This natural transformation induces an indexed functor |\langle\mathcal P\rangle : \mathcal C \circ p \to \mathcal C \circ \{{-}\}| where |\mathcal C| is an |\mathbf S|-indexed category. We have <strong>|\mathcal P|-(co)products</strong> when there is an indexed (left) right adjoint to this indexed functor.</p>
<p>One of the most important fibrations is the codomain fibration |\mathsf{cod} : \mathbf S^{\to} \to \mathbf S| which corresponds to |Id| as a comprehension category. However, |\mathsf{cod}| is only a fibration when |\mathbf S| has all pullbacks. In particular, the cartesian morphisms of |\mathbf S^{\to}| are the pullback squares. However, we can define the notion of <a href="https://ncatlab.org/nlab/show/Cartesian+morphism">cartesian morphism</a> with respect to any functor; we only need |\mathbf S| to have pullbacks for |\mathsf{cod}| to be a fibration because a fibration requires you to have <em>enough</em> cartesian morphisms. However, given <em>any</em> functor |p : \mathcal E \to \mathbf S|, we have a subcategory |\mathsf{Cart}(p) \hookrightarrow \mathcal E| which consists of just the cartesian morphisms of |\mathcal E|. The composition |\mathsf{Cart}(p)\hookrightarrow \mathcal E \to \mathbf S| is always a fibration.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Thus, if we consider the category |\mathsf{Cart}(\mathsf{cod})|, this will consist of whatever pullback squares exist in |\mathbf S|. The inclusion |\mathsf{Cart}(\mathsf{cod}) \hookrightarrow \mathbf S^{\to}| gives us a comprehension category. Write |\vert\mathsf{cod}\vert| for that comprehension category. The definition in the introduction is now seen to be equivalent to having |\vert\mathsf{cod}\vert|-coproducts. That is, the indexed functor |\langle\vert\mathsf{cod}\vert\rangle| having an indexed left adjoint. The Beck-Chevalley condition is what is necessary to show that a family of left (or right) adjoints to (the components of) an indexed functor combine together into an <em>indexed</em> functor.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Indexed categories are, in some sense, a <em>presentation</em> of fibrations which are the more intrinsic notion. This means it is better to work out concepts with respect to fibrations and then see what this means for indexed categories rather than the other way around or even using the “natural” suggestions. This is why indexed categories are pseudofunctors rather than either strict or lax functors. For our purposes, we have an equivalence of 2-categories between the 2-category of |\mathbf S|-indexed categories and the 2-category of fibrations over |\mathbf S|.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Absolute Colimits</title>
    <link href="https://derekelkins.github.io/posts/absolute-colimits.html" />
    <id>https://derekelkins.github.io/posts/absolute-colimits.html</id>
    <published>2019-05-18 16:59:33-07:00</published>
    <updated>2019-05-18T23:59:33Z</updated>
    <summary type="html"><![CDATA[<p>In category theory a concept is called <strong>absolute</strong> if it is preserved by <em>all</em> functors. Identity arrows and composition are absolute by the definition of functor. Less trivially, isomorphisms are absolute. In general, anything that is described by a diagram commuting is absolute as that diagram will be preserved by any functor. This is generally the case, but if I tell you something is an absolute epimorphism, it’s not clear what diagram is causing that; the notion of epimorphism itself doesn’t reduce to the commutativity of a particular diagram.</p>
<p>Below I’ll be focused primarily on <a href="https://ncatlab.org/nlab/show/absolute+colimit">absolute colimits</a> as those are the most commonly used examples. They are an important part of the theory of <a href="https://ncatlab.org/nlab/show/monadicity+theorem">monadicity</a>. The trick to many questions about absolute colimits and related concepts is to see what it means for |\newcommand{\Set}{\mathbf{Set}}\newcommand{\Hom}{\mathsf{Hom}} \newcommand{onto}{\twoheadrightarrow}\Hom| functors to preserve them.</p>
<!--more-->
<h2 id="non-examples">Non-Examples</h2>
<p>To start, we can show that certain colimits <em>cannot</em> be absolute, at least for |\Set|-enriched category theory. In particular, initial objects and coproducts are never absolute. Using the trick above, this is easily proven.</p>
<p>\[\Hom(0,0)\cong 1 \not\cong 0\]</p>
<p>\[\Set(\Hom(X+Y,Z),1)\cong 1 \not\cong 2\cong\Set(\Hom(X,Z),1)+\Set(\Hom(Y,Z),1)\]</p>
<h2 id="absolute-epimorphisms">Absolute Epimorphisms</h2>
<p>What do absolute epimorphisms look like? We know that there <em>are</em> absolute epimorphisms because a split epimorphism is defined by a certain diagram commuting. Are there other absolute epimorphisms? To find out, we apply our trick.</p>
<p>Let |r:X\onto Y| be our epimorphism. Then we have the surjection \[\Hom(Y,r):\Hom(Y,X)\onto\Hom(Y,Y)\] but this means that for every arrow |f:Y\to Y|, there’s an arrow |s:Y\to X| such that |f = r \circ s|. As you can no doubt guess, we want to choose |f=id_Y|, and we then have that |r| is a split epimorphism. Therefore split epimorphisms are the only examples of absolute epimorphisms.</p>
<h2 id="split-coequalizers">Split Coequalizers</h2>
<p>Now let’s consider the coequalizer case. Let |f,g:X\to Y| and |e:Y\onto C| be their coequalizer which we’ll assume is absolute. Before we pull out our trick, we can immediately use the previous result to show that |e| has a section, i.e. an arrow |s : C\rightarrowtail Y| such that |id_C=e\circ s|. Moving on, we use the trick to get the diagram: \[\Hom(Y,X)\rightrightarrows\Hom(Y,Y)\onto\Hom(Y,C)\]</p>
<p>Next, we use the explicit construction of the coequalizer in |\Set| which |\Hom(Y,C)| is supposed to be canonically isomorphic to. That is, the coequalizer of |\Hom(Y,f)| and |\Hom(Y,g)| is |\Hom(Y,Y)| quotiented by the equivalence relation generated by the relation which identifies |h,k:Y\to Y| when |\exists j:Y\to X.h = f\circ j \land k = g\circ j|. Let |[h]| represent the equivalence class of |h|. The claim that |\Hom(Y,C)| is (with |\Hom(Y,e)|) a coequalizer of the above arrows implies that |e\circ h = \bar e([h])| and |[h]=\bar e^{-1}(e\circ h)| with |\bar e| and |\bar e^{-1}| forming an isomorphism. Of course, our next move is to choose |h=id_Y| giving |e=\bar e([id_Y])|. However, |e=e\circ s\circ e = \bar e([s\circ e])| so we get |[id_Y]=[s\circ e]| because |\bar e| is invertible.</p>
<p>If we call the earlier relation |\sim| and write |\sim^*| for its reflexive, symmetric, transitive closure, then |[id_Y] = \{h:Y\to Y\mid id_Y\sim^* h\}|. Therefore |id_Y \sim^* s\circ e|. Now let’s make a simplifying assumption and assume further that |id_Y \sim s\circ e|, i.e. that |id_Y| is <em>directly</em> related to |s\circ e| by |\sim|. By definition of |\sim| this means there is a |t : Y\to X| such that |id_Y = f\circ t| and |s\circ e = g\circ t|. A given triple of |f|, |g|, and |e| such that |e\circ f = e\circ g| and equipped with a |s : C\to Y| and |t : Y\to X| satisfying the previous two equations along with |q\circ s = id_C| is called a <a href="https://ncatlab.org/nlab/show/split+coequalizer"><strong>split coequalizer</strong></a>. This data is specified diagrammatically and so is preserved by all functors, thus split coequalizers are absolute. All that we need to show is that this data is enough, on its own, to show that |e| is a coequalizer.</p>
<p>Given any |q : Y\to Z| such that |q\circ f = q\circ g|, we need to show that there exists a unique arrow |C\to Z| which |q| factors through. The obvious candidate is |q\circ s| leading to us needing to verify that |q=q\circ s\circ e|. We calculate as follows: \[ \begin{align} q \circ s \circ e &amp; = q \circ g \circ t \\ &amp; = q \circ f \circ t \\ &amp; = q \end{align}\] Uniqueness then quickly follows since if |q = k\circ e| then |q\circ s = k\circ e \circ s = k|. |\square|</p>
<p>There’s actually an even simpler example where |s\circ e = id_Y| which corresponds to the trivial case where |f=g|.</p>
<h2 id="absolute-coequalizers">Absolute Coequalizers</h2>
<p>Split coequalizers show that (non-trivial) absolute coequalizers can exist, but they don’t exhaust all the possibilities. The obvious cause of this is the simplifying assumption we made where we said |id_Y\sim s\circ e| rather than |id_Y\sim^* s\circ e|. In the general case, the equivalence will be witnessed by a sequence of arrows |t_i : Y\to X| such that we have either |s\circ e = g\circ t_0| or |s \circ e = f\circ t_0|, then |f\circ t_0 = g\circ t_1| or |g\circ t_0 = f\circ t_1| respectively, and so on until we get to |f\circ t_n = id_Y| or |g\circ t_n = id_Y|. As a diagram, this is a fan of diamonds of the form |f\circ t_i = g\circ t_{i+1}| or |g\circ t_i = f\circ t_{i+1}| with a diamond with side |s\circ e| on one end of the fan and a triangle with |id_Y| on the other. All this data is diagrammatic so it is preserved by all functors making the coequalizer absolute. That it <em>is</em> a coequalizer uses the same proof as for split coequalizers except that we have a series of equalities to show that |q\circ s \circ e = q|, namely all those pasted together diamonds. There is no conceptual difficulty here; it’s just awkward to notate.</p>
<h2 id="absolute-colimits">Absolute Colimits</h2>
<p>The absolute coequalizer case captures the spirit of the general case, but you can see a description <a href="https://ncatlab.org/nlab/show/absolute+colimit#particular_absolute_colimits_2">here</a>. I’m not going to work through it, but you could as an exercise. Less tediously, you could work through absolute pushouts. If |P| is the pushout of |Y \leftarrow X \to Z|, then the functors to consider are |\Hom(P,-)| and |\Hom(Y,-)+\Hom(Z,-)|. For each, the pushout in |\Set| can be turned into a coequalizer of a coproduct. For the first functor, as before, this gives us an inverse image of |id_P| which will either be an arrow |P\to Y| or an arrow |P\to Z| which will play the role of |s|. The other functor produces a coequalizer of |\Hom(Y,Y)+\Hom(Z,Y)+\Hom(Y,Z)+\Hom(Z,Z)|. The generating relation of the equivalence relation states that there exists either an arrow |Y\to X| or an arrow |Z\to X| such that the appropriate equation holds. The story plays out much as before except that we have a sequence of arrows from two different hom-sets.</p>]]></summary>
</entry>
<entry>
    <title>Finite</title>
    <link href="https://derekelkins.github.io/posts/finite.html" />
    <id>https://derekelkins.github.io/posts/finite.html</id>
    <published>2018-03-26 00:57:41-07:00</published>
    <updated>2018-03-26T07:57:41Z</updated>
    <summary type="html"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>When sets are first introduced to students, usually examples are used with finite, explicitly presented sets. For example, #{1,2,3} uu {2,3,4} = {1,2,3,4}#. This is the beginning of the idea that a set is a “collection” of things. Later, when infinite sets are introduced, the idea that sets are “collections” of things is still commonly used as the intuitive basis for the definitions. While I personally find this a bit of a philosophical bait-and-switch, my main issue with it is that I don’t think it does a good job reflecting how we work with sets day-to-day nor for more in-depth set-theoretic investigations. Instead, I recommend thinking about infinite sets as defined by properties and #x in X# for some infinite set #X# means checking whether it satisfies the property defining #X#, <em>not</em> rummaging through the infinite elements that make up #X# and seeing if #x# is amongst them. This perspective closely reflects how we prove things about infinite sets. It makes it much clearer that the job is to find logical relationships between the properties that define sets.</p>
<p>Of course, this view can also be applied to finite sets and <em>should</em> be applied to them. For a constructivist, the notion of “finite set” splits into multiple inequivalent notions, and it is quite easy to show that there is a subset of #{0,1}# which is not “finite” with respect to strong notions of “finite” that are commonly used by constructivists. Today, though, I’ll stick with classical logic and classical set theory. In particular, I’m going to talk about the <a href="https://en.wikipedia.org/wiki/Internal_set_theory">Internal Set Theory</a> of Edward Nelson, or mostly the small fragment he used in <a href="https://web.math.princeton.edu/~nelson/books/rept.pdf">Radically Elementary Probability Theory</a>. In the <a href="https://web.math.princeton.edu/~nelson/books/1.pdf">first chapter of an unfinished book on Internal Set Theory</a>, he states the following:</p>
<blockquote>
<p>Perhaps it is fair to say that “finite” does not mean what we have always thought it to mean. What have we always thought it to mean? I used to think that I knew what I had always thought it to mean, but I no longer think so.</p>
</blockquote>
<p>While it may be a bit strong to say that Internal Set Theory leads to some question about what “finite” means, I think it makes a good case for questioning what “finite set” means. These concerns are similar to the relativity of the notion of “(un)countable set”.</p>
<!--more-->
<h3 id="minimal-internal-set-theory">Minimal Internal Set Theory</h3>
<p>Minimal Internal Set Theory (<strong>minIST</strong>) starts by taking all the axiom( schema)s of <a href="https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory"><strong>ZFC</strong></a> as axioms. So right from the get-go we can do “all of math” in exactly the same way we used to. <strong>minIST</strong> is a conservative extension of <strong>ZFC</strong>, so a formula of <strong>minIST</strong> can be translated to a formula of <strong>ZFC</strong> that is provable if and only if the original formula was provable in <strong>minIST</strong>.</p>
<p>Before going into what makes <strong>minIST</strong> different from <strong>ZFC</strong>, let’s talk about what “finite set” means in <strong>ZFC</strong> (and thus <strong>minIST</strong>). There are <a href="https://en.wikipedia.org/wiki/Finite_set">multiple equivalent characterizations of a set being “finite”</a>. One common definition is a set that has no bijection with a proper subset of itself. A more positive and more useful equivalent definition is a set is <strong>finite</strong> if and only if it has a bijection with #{k in bbbN | k &lt; n}# for some #n in bbbN#. This latter definition will be handier for our purposes here.</p>
<p>The only new primitive concept <strong>minIST</strong> adds is the concept of a “standard” natural number. That is, a new predicate symbol is added to the language of <strong>ZFC</strong>, <code class="asciimath">#sf "St"#</code>, and #n in bbbN# is <strong>standard</strong> if and only if <code class="asciimath">#sf "St"(n)#</code> holds. For convenience, we’ll define the following shorthand: <code class="asciimath">#forall^{:sf "St":}n.P(n)#</code> for <code class="asciimath">#forall n.sf "St"(n) =&gt; P(n)#</code>. This is very similar to the shorthand: #forall x in X.P(x)# which stands for #forall x.x in X =&gt; P(x)#. In fact, if we had a set of standard natural numbers then we could use this more common shorthand, but it will turn out that the existence of a set of standard natural numbers is contradictory.</p>
<p>The four additional axiom( schema)s of <strong>minIST</strong> are the following:</p>
<ol type="1">
<li><code class="asciimath">#sf "St"(0)#</code>, i.e. #0# is standard.</li>
<li><code class="asciimath">#forall n.sf "St"(n) =&gt; sf "St"(n+1)#</code>, i.e. if #n# is standard, then #n+1# is standard.</li>
<li><code class="asciimath">#P(0) ^^ (forall^{:sf "St":}n.P(n) =&gt; P(n+1)) =&gt; forall^{:sf "St":} n.P(n)#</code> for every formula #P#, i.e. we can do induction over standard natural numbers to prove things about all standard natural numbers.</li>
<li><code class="asciimath">#exists n in bbbN.not sf "St"(n)#</code>, i.e. there exists a natural number that is not standard.</li>
</ol>
<p>That’s it. There is a bit of a subtlety though. The <a href="https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory#3._Axiom_schema_of_specification_(also_called_the_axiom_schema_of_separation_or_of_restricted_comprehension)">Axiom Schema of Separation (aka the Axiom Schema of Specification or Comprehension)</a> is an axiom schema of <strong>ZFC</strong>. It is the axiom schema that justifies the notion of set comprehension, i.e. defining a subset of #X# via #{x in X | P(x)}#. This axiom schema is indexed by the formulas <em>of <strong>ZFC</strong></em>. When we incorporated <strong>ZFC</strong> into <strong>minIST</strong>, we only got the instances of this axiom schema for <strong>ZFC</strong> formulas, <em>not</em> for <strong>minIST</strong> formulas. That is, we are only allowed to form #{x in X | P(x)}# in <strong>minIST</strong> when #P# is a formula of <strong>ZFC</strong>. Formulas of <strong>minIST</strong> that are also formulas of <strong>ZFC</strong>, i.e. that don’t involve (directly or indirectly) <code class="asciimath">#sf "St"#</code> are called <strong>internal</strong> formulas. Other formulas are called <strong>external</strong>. So we can’t use the Axiom Schema of Separation to form the set of standard natural numbers via <code class="asciimath">#{n in bbbN | sf "St"(n)}#</code>. This doesn’t mean there isn’t some other way of making the set of standard natural numbers. That possibility is excluded by the following proof.</p>
<p>Suppose there is some set #S# such that <code class="asciimath">#n in S &lt;=&gt; sf "St"(n)#</code>. Using axioms 1 and 2 we can prove that #0 in S# and if #n in S# then #n+1 in S#. The <em>internal</em> form of induction, like all internal theorems, still holds. Internal induction is just axiom 3 but with #forall# instead of <code class="asciimath">#forall^{:sf "St":}#</code>. Using internal induction, we can immediately prove that #forall n in bbbN.n in S#, but this means <code class="asciimath">#forall n in bbbN.sf "St"(n)#</code> which directly contradicts axiom 4, thus there is no such #S#. #square#</p>
<p>This negative result is actually quite productive. For example, we can’t have a set #N# of nonstandard natural numbers otherwise we could define #S# as #bbbN \\ N#. This means that if we can prove some <em>internal</em> property holds for all nonstandard naturals, then there must exist a standard natural number for which it holds as well. Properties like this are called <strong>overspill</strong> because an internal property that holds for the standard natural numbers spills over into the nonstandard natural numbers. Here’s a small example. First, a real number #x# is <strong>infinitesimal</strong> if and only if there exists a nonstandard natural #nu# such that <code class="asciimath">#|x| &lt;= 1/nu#</code>. (Note, we can prove that every nonstandard natural is larger than every standard natural.) Two real numbers #x# and #y# are <strong>nearly equal</strong>, written |x \simeq y|, if and only if #x - y# is infinitesimal. A function #f : bbbR -&gt; bbbR# is <strong>(nearly) continuous at #x#</strong> if and only if for all #y in bbbR#, if |x \simeq y| then |f(x) \simeq f(y)|. Now if #f# is continuous at #x#, then for every standard natural #n# and #y in bbbR#, <code class="asciimath">#|x - y| &lt;= delta =&gt; |f(x) - f(y)| &lt;= 1/n#</code> holds for all infinitesimal #delta# by continuity. Thus by overspill it holds for some non-infinitesimal #delta#, i.e. for some #delta &gt; 1/m# for some standard #m#. #square#</p>
<p>In Radically Elementary Probability Theory, Nelson reformulates much of probability theory and stochastic process theory by restricting to the finite case. Traditionally, probability theory for finite sample spaces is (relatively) trivial. But remember, “finite” means bijective with #{k in bbbN | k &lt; n}# for some #n in bbbN# and that #n# can be nonstandard in <strong>minIST</strong>. The notion of “continuous” defined before translates to the usual notion when the <strong>minIST</strong> formula is translated to <strong>ZFC</strong>. Similarly, subsets of infinitesimal probability become subsets of measure zero. Sums of infinitesimals over finite sets of nonstandard cardinality become integrals. Little to nothing is lost, instead it is just presented in a different language, a language that often lets you say complex things simply.</p>
<p>Again, I want to reiterate that <em>everything true in</em> <strong>ZFC</strong> <em>is true in</em> <strong>minIST</strong>. One of the issues with Robinson-style nonstandard analysis is that the hyperreals are not an Archimedean field. An ordered field #F# is Archimedean if for all #x in F# such that #x &gt; 0# there is an #n in bbbN# such that #nx &gt; 1#. The hyperreals serve as a model of the (<strong>min</strong>)<strong>IST</strong> reals. <em>Within</em> <strong>minIST</strong>, the Archimedean property holds. Even if #x in bbbR# is infinitesimal, there is still a nonstandard #n in bbbN# such that #nx &gt; 1#. But this statement translates to the statement in the meta-language that there is a hypernatural #n# such that for every positive hyperreal #x#, #nx &gt; 1# which does hold. However, if we use the <em>meta-language’s</em> notion of “natural number”, this statement is false.</p>
<h3 id="internal-set-theory">Internal Set Theory</h3>
<p>Full Internal Set Theory (<strong>IST</strong>) only has three axiom schemas in addition to the axioms of <strong>ZFC</strong>. Like <strong>minIST</strong>, there is <code class="asciimath">#sf "St"#</code> but now we talk about standard sets in general, not just standard natural numbers. The three axiom schemas are as follows:</p>
<ol type="1">
<li><code class="asciimath">#forall^{:sf "St":}t_1 cdots forall^{:sf "St":}t_n.(forall^{:sf "St":} x.P(x, t_1, ..., t_n)) &lt;=&gt; forall x.P(x, t_1, ..., t_n)#</code> for internal #P# with no (other) free variables.</li>
<li><code class="asciimath">#(forall^{:sf "StFin":}X.exists y.forall x in X.P(x, y)) &lt;=&gt; exists y.forall^{:sf "St"} x.P(x, y)#</code> for internal #P# which may have other free variables.</li>
<li><code class="asciimath">#forall^{:sf "St":}X.exists^{:sf "St"} Y.forall^{:sf "St":}z.(z in Y &lt;=&gt; z in X ^^ P(z))#</code> for <em>any</em> formula #P# which may have other free variables.</li>
</ol>
<p>These are called the Transfer Principle (T), the Idealization Principle (I), and the Standardization Principle (S) respectively. <code class="asciimath">#forall^{sf "StFin"}X.P(X)#</code> means for all #X# which are both standard and finite. The Transfer Principle essentially states that an internal property is true for all sets if and only if it is true for all standard sets. The Transfer Principle allows us to prove that any internal property that is satisfied uniquely is satisfied by a standard set. This means everything we can define in ZFC is standard, e.g. the set of reals, the set of naturals, the set of functions from reals to naturals, any particular natural, #pi#, the Riemann sphere.</p>
<p>The Idealization Principle is what allows nonstandard sets to exist. For example, let #P(x, y) -= y in bbbN ^^ (x in bbbN =&gt; x &lt; y)#. The Idealization Principle then states that if for any finite set of natural numbers, we can find a natural number greater than all of them, then there is a natural number greater than any standard natural number. Since we clearly can find a natural number greater than any natural number in some given finite set of natural numbers, the premise holds and thus we have a natural number greater than any standard one and thus necessarily nonstandard. In fact, a similar argument can be used to show that <em>all</em> infinite sets have nonstandard elements. A related argument shows that a standard, finite set is exactly a set all of whose elements are standard.</p>
<p>The Standardization Principle isn’t needed to derive <strong>minIST</strong> nor is it needed for the following result so I won’t discuss it further. Another result derivable from Idealization is: there is a finite set that contains every standard set. To prove it, simply instantiate the Idealization Principle with <code class="asciimath">#P(x,y) -= x in y ^^ y\ "is finite"#</code> where “is finite” stands for the formalization of any of the definitions of “finite” given before. It is in the discussion after proving this that Nelson makes the statement quoted in the introduction. We can prove some results about this set. First, it can’t be standard itself. Here are three different reasons why: 1) if it were standard, then it would include itself violating the Axiom of Foundation/Regularity; 2) if it were standard, then by Transfer we could prove that it contains <em>all</em> sets again violating the Axiom of Foundation; 3) if it were standard, then by the result mentioned in the previous paragraph, it would contain <em>only</em> standard elements, but then we could intersect it with #bbbN# to get the set of standard naturals which does not exist. Like the fact that there is no smallest nonstandard natural, there is no smallest finite set containing all standard sets.</p>
<p><strong>IST</strong> also illustrates another concept. In set theory we often talk about classes. My experience with this concept was a series of poor explanations: a class was “the extent of a predicate”, “the ‘collection’ of entities that satisfy a predicate” and a proper class was a “collection” that was “too big” to be a set like the class of all sets. I muddled on with vague descriptions like this for quite some time before I finally figured out what they meant. Harking back to my first paragraph, a <strong>class</strong> (in <strong>ZFC</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>) is just a (unary) predicate, i.e. a formula (of <strong>ZFC</strong>) with one free variable, or at least it can be represented by such<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. A formula #P(x)# is a <strong>proper class</strong> if there is no set #X# such that #forall x.P(x) &lt;=&gt; x in X#. The “class of all sets” is simply the constantly true predicate. This is a proper class because a set #U# such that #forall x.x in U# leads to a contradiction, e.g. Russell’s paradox by warranting #{x in U | x notin x}# via the Axiom of Separation. It may have already dawned on you that <code class="asciimath">#sf "St"(x)#</code> is a class in (<strong>min</strong>)<strong>IST</strong> and, in fact, a <em>proper</em> class. All of those times when I said “there is no set of standard naturals/nonstandard naturals/infinitesimals/standard sets”, I could equally well have said “there is a proper class of standard naturals/nonstandard naturals/infinitesimals/standard sets”. The result of the previous paragraph means the (proper) class of standard sets – far from being “too big” to be a set – is a subclass of (the class induced by) a finite set! Classes were never about “size”<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>To reiterate, <strong>IST</strong> is a <em>conservative extension</em> of <strong>ZFC</strong>. Focusing first on the “conservative” part, when we translate the theorem that there is a finite set containing all standard sets to <strong>ZFC</strong>, it becomes the tautologous statement that every finite set is a subset of some finite set. This starts to reveal what is happening in <strong>IST</strong>. However, focusing on the “extension” part, <em>nothing in</em> <strong>ZFC</strong> <em>refutes any of</em> <strong>IST</strong>. To take a Platonist perspective, these nonstandard sets could “always have been there” and <strong>IST</strong> just finally lets us talk about them. If you were a Platonist but didn’t want to accept these nonstandard sets, the conservativity result would still allow you to use <strong>IST</strong> as “just a shorthand” for formulas in <strong>ZFC</strong>. Either way, it is clear that the notion of “finite set” in <strong>ZFC</strong> has a lot of wiggle room.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Some set theories have an explicit notion of “class”.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>The point is that any logically equivalent formula would do, i.e. classes are equivalence classes of formulas under logical equivalence. One could arguably go further and say that the two formulas represent the same class even if we can’t <em>prove</em> that they are logically equivalent as long as we (somehow) know they are “true” for exactly the same things. This extensional view of predicates is what the “extent” stuff is about.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>On the other hand, the Axiom Schema of Specification states that any subclass of a set is a set in ZFC, so for ZFC no proper class can be contained in a set. In fact, many non-traditional systems can be understood as only allowing certain subclasses of “sets” to be “sets”. For example, in constructive recursive mathematics, we may require a “set” to be recursively enumerable, but we can certainly have subclasses of recursively enumerable sets that are not recursively enumerable. See also <a href="https://mathoverflow.net/questions/299120/are-classes-still-larger-than-sets-without-the-axiom-of-choice">this</a> discussion on the Axiom of Choice and “size”.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>The Pedagogy of Logic: A Rant</title>
    <link href="https://derekelkins.github.io/posts/the-pedagogy-of-logic-a-rant.html" />
    <id>https://derekelkins.github.io/posts/the-pedagogy-of-logic-a-rant.html</id>
    <published>2018-02-28 20:35:14-08:00</published>
    <updated>2018-03-01T04:35:14Z</updated>
    <summary type="html"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>Over the years I’ve seen a lot of confusion about formal logic online. Usually this is from students (but I’ve also seen it with experienced mathematicians) on platforms like the Math or CS StackExchange. Now there is clearly some selection bias where the people asking the questions are the people who are confused, but while the questions <em>about</em> the confusions are common, the confusions are often evident even in questions about other aspects, the confusions are <em>in the (upvoted!) answers</em>, and when you look at the study material they are using it is often completely unsurprising that they are confused. Again, these confusions are also not limited to just “students”, though I’ll use that word in a broad sense below. Finally, on at least one of the points below, the confusion seems to be entirely on the instructors’ side.</p>
<p>An indication that this is true more broadly is this quote from the <a href="http://ncatlab.org/nlab/revision/constructive%20mathematics/64">nLab</a>:</p>
<blockquote>
<p>The distinction between object language and metalanguage exists even in classical mathematics, but it seems that most classical mathematicians are not used to remembering it, although it is not entirely clear why this should be so. One possibly relevant observation is that even if <span class="math inline"><em>P</em></span> is a statement which is neither provable nor disprovable (like the continuum hypothesis), in classical mathematics it is still provable that “<span class="math inline"><em>P</em></span> is either true or false.” Moreover, classical model theory often restricts itself to two-valued models in which the only truth values are “true” and “false,” although classical logic still holds in Boolean-valued models, and in such a case the truth value of <span class="math inline"><em>P</em></span> may be neither “true” nor “false,” although the truth value of “<span class="math inline"><em>P</em></span> or not <span class="math inline"><em>P</em></span>” is still “true.” Certainly when talking about classical truths which fail constructively, such as excluded middle, it is important to remember that “fails to be provable” is different from “is provably false.”</p>
</blockquote>
<p>To give an indication of the problem, here is my strong impression of what would happen if I gave a student who had just passed a full year introduction to formal logic the following exercise: “Give me a formal proof of #(neg P =&gt; neg Q) =&gt; (Q =&gt; P)#”. I suspect most would draw up a truth table. When I responded, “that’s not a formal proof,” they would be confused. If you are confused by that response, then this is definitely for you. I’m sure they would completely agree with me that if I asked for the inverse matrix of some matrix #M#, showing that the determinant of #M# is non-zero does not constitute an answer to that question. The parallelism of these scenarios would likely be lost on them though. While I think courses that focus on a syntactic approach to logic will produce students that are much more likely to give an appropriate answer to my exercise, that doesn’t mean they lack the confusions, just that this exercise isn’t a good discriminator for them. For example, if they don’t see the parallelism of the two scenarios I described, or worse, have no idea what truth tables have to do with anything, then they have some gap.</p>
<p>As a disclaimer, I am not an educator nor even an academic in any professional capacity.</p>
<p>As a summary, the points I will touch on are:</p>
<ul>
<li><a href="#syntax-versus-semantics">Not <em>continually</em> emphasizing the distinction between syntax and semantics</a>.</li>
<li><a href="#unnecessary-philosophizing">Unnecessary philosophizing</a>.</li>
<li><a href="#the-complete-absence-of-non-classical-logics">The complete absence of non-classical logics</a>.</li>
<li><a href="#silly-linguistics">Silly linguistics exercises</a>.</li>
<li><a href="#the-complete-conflation-of-negation-introduction-with-double-negation-elimination">The complete conflation of negation introduction with double negation elimination</a>.</li>
<li><a href="#overuse-of-indirect-proof">Overuse of indirect proof</a>.</li>
<li><a href="#smaller-issues">Some smaller issues that are not so bad and some missed opportunities</a>.</li>
</ul>
<p>If anyone reading this is aware of <em>introductory</em> textbooks or other resources (ideally freely available, but I’ll take a reasonably priced textbook too) that avoid most or all of the major issues I list and otherwise do a good job, I would be very interested. My own education on these topics has been piecemeal and eclectic and also strongly dependent on my background in programming. This leaves me with no reasonable recommendations. Please leave a comment or email me, if you have such a recommendation.</p>
<!--more-->
<h3 id="syntax-versus-semantics">Syntax versus Semantics</h3>
<p>The distinction between syntax and semantics is fundamental in logic. Many, many students, on the other hand, seem completely unaware of the distinction. The most blatant form of this is that for many students it seems quite clear that “proving a formula true” <em>means</em> plugging in truth values and seeing that the result is always “true”. Often they (seemingly) think a truth table <em>is</em> a formal proof. They find it almost impossible to state what the difference between provable and “true” is. The impression I get from some is that they think plugging truth values into formulas <em>is</em> what logic is about.</p>
<p>If the students get to classical predicate logic, most of these misunderstandings will be challenged, but students with this misunderstanding will struggle unnecessarily, and you can see this on sites like <a href="https://math.stackexchange.com">math.stackexchange.com</a> where questioners <a href="https://math.stackexchange.com/q/2646421/305738">present attempts to prove statements of predicate logic by reducing them to propositional statements</a>. It’s not clear to me what typically happens to students with such misunderstandings as they gain proficiency with classical predicate logic. Certainly some clear up their misunderstanding, but since it’s clear that some still have a slippery grasp of the distinction between syntax and semantics, some misunderstanding remains for them. My guess would be that many just view classical propositional logic and classical predicate logic as more different than they are with unrelated approaches to proof. Any instructors reading this can probably describe much stranger rationalizations that students have come up with, and perhaps give some indications of what faulty rationalizations are common.</p>
<p>Speaking of instructors, the pedagogical problem I see here is not that the textbook authors and instructors don’t understand these distinctions, or even that they don’t <em>present</em> these distinctions, but that they don’t continually <em>emphasize</em> these distinctions. Maybe they do in the classroom, but going by the lecture notes or books, the notation and terminology that many use make it <em>very</em> easy to conflate syntax and semantics. For understandable reasons, proofs of the soundness and completeness theorems that are necessary to justify this conflation are pushed off until much later.</p>
<p>I googled <code>lecture notes "classical propositional logic"</code> and looked at the <a href="http://www.philosophy.ed.ac.uk/undergraduate/documents/Propositional_Logic_2008_09.pdf">first hit I got</a>. It clearly has a section on semantics and multiple sections on a (syntactic) deductive system “helpfully” named “semantic tableaux”. It bases all concepts on truth tables introduced in section 4. Later concepts are justified by corresponding to the truth table semantics. Here is how it describes the semantics for negation:</p>
<blockquote>
<p>A negation #neg A# is <strong>true</strong> (in any situation) if and only if #A# is <strong>not true</strong> (in that situation).</p>
</blockquote>
<p>This is followed by the sentence: “For the moment, we shall suppress the qualifier ‘in any situation’[…]”, and we get the rules for the other connectives starting with the one for conjunction:</p>
<blockquote>
<p>A conjunction #A ^^ B# is <strong>true</strong> if and only if #A# is true and #B# is true.</p>
</blockquote>
<p>It later explicates the “in any situation” by talking about truth-value assignments and giving “rules” like:</p>
<blockquote>
<p>#A ^^ B# is true in an assignment iff #A# and #B# are true in that assignment.</p>
</blockquote>
<p>Between these two renditions of the rule for conjunction, the notes tell us what a “truth-value” is and seemingly what “is true” means with the following text:</p>
<blockquote>
<p>In classical propositional logic, formulas may have only <strong>one</strong> of two possible “<strong>truth values</strong>”. Each formula must be <strong>either true</strong> or <strong>false</strong>; and no formula may be both true and false. These values are written <strong>T</strong> and <strong>F</strong>. (Some authors use #1# and #0# instead. Nothing hinges on this.)</p>
</blockquote>
<p>The description so far makes it sound like logical formula are “expressions” which “compute” to <strong>true</strong> or <strong>false</strong>, which will also be written <strong>T</strong> or <strong>F</strong>, like arithmetical expressions. This is made worse in section 11 where the connectives are identified with truth-functions (which is only mentioned in this early section). It would be better to have a notation to distinguish a formula from its truth value e.g. #v(A)# or, the notation I like, |[\! [A]\!]|. (As I’ll discuss in the next section, it would also be better to avoid using philosophically loaded terms like “true” and “false”. #1# and #0# are better choices, but if you <em>really</em> wanted to drive the point home, you could use arbitrary things like “dog” and “cat”. As the authors state, “nothing hinges on this”.) Rewording the above produces a result that I believe is much harder to misunderstand.</p>
<blockquote>
<p>#v(A ^^ B)=1# if and only if #v(A)=1# and #v(B)=1#.</p>
</blockquote>
<p>This is likely to be perceived as less “intuitive” to students, but I strongly suspect the seeming cognitive ease was masking actually grappling with what is being said in the earlier versions of the statement. If we actually want to introduce truth functions, I would highly recommend using different notation for them, for example:</p>
<blockquote>
<p><code class="asciimath">#v(A ^^ B)=sf "and"(v(A),v(B))#.</code></p>
</blockquote>
<p>Of course, we can interpret “#A# is true” as defining a unary predicate on formulas, “_ is true”, but I seriously doubt this is where most students’ minds jump to when being introduced to formal logic, and this view is undermined by the notion of “truth values”. Again, I’m not saying the authors are saying things that are <em>wrong</em>, I’m saying that they make it very easy to misunderstand what is going on.</p>
<p>Following this, most of the remainder of the notes discusses the method of semantic tableaux. It mentions soundness and completeness but also states: “In this introductory course, we do not expect you to study the proof of these two results[.]”) At times it suggests that the tableaux method can stand on its own: “However, we can show that it is inconsistent <strong>without using a truth table</strong>, by a form of <strong>deductive reasoning</strong>. That is, by following computer-programmable <strong>inference rules</strong>.” But the method is largely presented as summarizing semantic arguments: “This rule just summarizes the information in the truth table that…” In my ideal world, this more deductive approach would be clearly presented as an <em>alternative</em> approach to thinking about logic with the inference rules not explained in terms of semantic notions but related to them after the fact.</p>
<p>My point isn’t to pick on these notes which seem reasonable enough overall. They have some of the other issues I’ll cover but also include some of the things I’d like to see in such texts.</p>
<h3 id="unnecessary-philosophizing">Unnecessary Philosophizing</h3>
<p>As mentioned in the previous section, words like “true” and “false” have a huge philosophical weight attached to them. It is quite easy to define any particular logic either syntactically or semantically without even suggesting the identification of anything with the philosophical notion of “truth”. My point isn’t that philosophy should be banished from a mathematical logic course, though that <em>is</em> an option, but that we can <em>separate</em> the definition of a logical system with how it relates to the intuitive or philosophical notion of “truth”. This allows us to present logics as the mathematical constructions they are and not feel the need to justify why any given rules/semantics really are the “right” ones. Another way of saying it is that your philosophical understanding/stance doesn’t change the definition of classical propositional logic. It just changes whether that logic is a good reflection of your philosophical views.</p>
<p>(As an aside, while I did read introductions to logic early on, the beginnings of my in-depth understanding of logic arose via programming language theory and type systems [and then category theory]. These areas often cover the same concepts and systems but with little to no philosophizing. Papers discuss classical and constructive logics with no philosophical commitment for or against the law of excluded middle being remotely implied.)</p>
<p>Of course, we do want to leverage our intuitive understanding of logic to motivate either the inference rules or the chosen semantics, but we can phrase the definitions as being <em>inspired by</em> the intuition rather than codifying it. Things like the <a href="https://math.stackexchange.com/q/232309/305738">common misgivings</a> students (and <a href="https://en.wikipedia.org/wiki/Material_conditional#Philosophical_problems_with_material_conditional">others</a>) have about material implication are a lot easier to deal with when logic isn’t presented as “The One True Formulation of Correct Reasoning”. Of course another way to undermine this impression would be to present a variety of logics including ones where material implication doesn’t hold. This is also confounded by unnecessary philosphizing. It makes it seem like non-classical logics are only of interest to crazy contrarians who reject “self-evident” “truths” like every statement is either “true” or “false”. While there definitely is a philosophical discussion that could be had here, there are plenty of <em>mathematical</em> reasons to be interested in non-classical logics regardless of your philosophical stance on truth. Non-classical logics can also shed a lot of light on what’s going on in classical logics.</p>
<p>This assumes non-classical logics are even mentioned at all.</p>
<h3 id="the-complete-absence-of-non-classical-logics">The Complete Absence of Non-Classical Logics</h3>
<p>I’m highly confident you can go through many introductory courses on logic and never have any inkling that there are logics other than classical logics. I suspect you can easily go through an entire math major and also be completely unaware of non-classical logics. For an introductory course on logic, I completely understand that time constraints can make it difficult to include any substantial discussion of non-classical logics. I somewhat less forgiving when it comes to textbooks.</p>
<p>What bothers me isn’t that non-classical logics aren’t given equal time - I wouldn’t expect them to be - it’s that classical logic is presented as a comprehensive picture of logic. This is likely, to some degree, unintentional, but it is easily fixable. First, at the beginning of the course, simply mention that the course will cover classical logics and there are other logics like constructive or paraconsistent logics that have different rules and different notions of semantics. Use less encompassing language. Instead of saying “logic”, say “a logic” or “the logic we’re studying”. Similarly, talk about “<em>a</em> semantics” rather than “<em>the</em> semantics”, and “<em>a</em> set of rules” rather than “<em>the</em> rules”. Consistently say “<em>classical</em> propositional/first-order/predicate logic” rather than just “propositional/first-order/predicate logic” or worse just “logic”. Note points where non-classical logics make different choices when the topics come up. For example, that truth tables are a semantics for classical propositional logic while non-classical logics will use a different notion of semantics. The law of excluded middle, indirect proof, and material implication are other areas where a passing mention of how different logics make different decisions about these topics could be made. The principle of explosion, related to the issues with material implication, is an interesting point where classical and constructive logics typically agree while paraconsistent logics differ. Heck, I suspect some people find my use of the plural, “logics”, jarring, as if there could be more than one logic. Fancy that!</p>
<p>The goal with this advice is just to indicate to students that there is a wider world of logic out there. I don’t expect students to be quizzed on this. Some of this advice is relevant even purely for classical logics. It’s not uncommon for people to ask questions on the Math StackExchange asking for a proof of some formula. When asked what rules they are using, they seem confused: “The rules of predicate logic(?!)” The idea that the presentation of (classical) predicate logic that they were given is only one of many possibilities seems never to have occurred to them. Similarly for semantics. So generally when a choice is made, it would seem of long-term pedagogical value to at least mention that a choice has been made and that other choices are possible.</p>
<h3 id="silly-linguistics">Silly Linguistics</h3>
<p>Nearly any week on the Math StackExchange you can <a href="https://math.stackexchange.com/q/2270547/305738">find</a> <a href="https://math.stackexchange.com/q/2536711/305738">someone</a> <a href="https://math.stackexchange.com/q/2589373/305738">asking</a> <a href="https://math.stackexchange.com/q/2549016/305738">about</a> an <a href="https://math.stackexchange.com/q/2658713/305738">exercise like the following</a>:</p>
<blockquote>
<p>Translate “Jan likes Sally but not Alice” to a logical formula.</p>
</blockquote>
<p>Exercises like these are just a complete and utter waste of time. The only people who <em>might</em> do something like this are linguists. Even then the “linguistics” presented in introductions to formal logic is (understandably) ridiculously naive, and an introductory course on formal logic is simply not the place for it anyway. What the vast majority of consumers of formal logic <em>do</em> need is to 1) be able to understand what <em>formal</em> expressions mean, and 2) be able to present <em>their own thoughts</em> formally. Even when formalizing another person’s informal proof, the process is one of understanding what the person was saying and then writing down your understanding in formal language. Instead what’s presented is rules-of-thumb like “‘but’ means ‘and’”.</p>
<p>I could understand having an in-class discussion where the instructor presents natural language statements and asks students interactively to provide various interpretations of the statements. The ambiguity in natural language should quickly become apparent and this motivates the introduction of unambiguous formal notation. What makes no sense to me is giving students sheets of natural language statements about everyday situations that they need to “formalize” and grading them on this. Even if this was valuable, which I don’t think it is, the opportunity cost of spending time on this rather than some other topic would make it negatively valued to me.</p>
<p>What is fine is telling students how to <em>read</em> formal notation in natural language as long as it’s clear that these readings are not definitions. I have <a href="https://math.stackexchange.com/q/2596766/305738">seen people</a> claim that e.g. #A =&gt; B# is <em>defined</em> as “if #A#, then #B#”, but this is likely a misunderstanding on their part and not what their instructors or books said. On the other hand, it doesn’t seem too rare for a textbook to present a logical connective and then explicate it in terms of natural language examples shortly before or after providing a more formal definition (via rules or semantics). I don’t find it completely surprising that some students confuse these examples as definitions especially when the actual definitions, at times, don’t look terribly different from schematic phrases of natural languages.</p>
<p>I think natural language examples, and especially everyday as opposed to mathematical examples, should largely be avoided, or at the very least should be used with care.</p>
<h3 id="the-complete-conflation-of-negation-introduction-with-double-negation-elimination">The Complete Conflation of Negation Introduction with Double Negation Elimination</h3>
<p>Negation introduction is the following rule: if by assuming #P# we can produce a contradiction, then we can conclude #neg P#. You could present this as an axiom: <code class="asciimath">#(P =&gt; _|_) =&gt; neg P#</code>.</p>
<p>Double negation elimination is the following rule: if by assuming #neg P# we can produce a contradiction, then we can conclude #P#. You could present this as an axiom: <code class="asciimath">#(neg P =&gt; _|_) =&gt; P#</code> or #neg neg P =&gt; P#.</p>
<p>It seems pretty evident that many experienced mathematicians, let alone instructors and students, don’t see any difference between these. “Proof by contradiction” or “reductio ad absurdum” are often used to ambiguously refer to either of the above rules. This has <a href="http://math.andrej.com/2010/03/29/proof-of-negation-and-proof-by-contradiction/">been discussed</a> a <a href="https://existentialtype.wordpress.com/2017/03/04/a-proof-by-contradiction-is-not-a-proof-that-derives-a-contradiction/">few</a> <a href="https://blog.plover.com/math/IL-contradiction.html">times</a>. When even the likes of Timothy Gowers <a href="https://gowers.wordpress.com/2010/03/28/when-is-proof-by-contradiction-necessary/#comment-6984">does this</a>, I think it is safe to say many others do as well. Or just look at the comments to <a href="http://math.andrej.com/2010/03/29/proof-of-negation-and-proof-by-contradiction/">Andrej’s article</a> which I <em>highly</em> recommend.</p>
<p>As discussed in the referenced articles, and evidenced in the comments, there seems to be two things going on here. First, it seems that many people believe that dropping double negations, i.e. treating #neg neg P# and #P# is just a thing that you can do. They incorrectly reason as follows: substitute #neg P# into <code class="asciimath">#(P =&gt; _|_) =&gt; neg P#</code> to derive <code class="asciimath">#(neg P =&gt; _|_) =&gt; neg neg P#</code> which is the same as <code class="asciimath">#(neg P =&gt; _|_) =&gt; P#</code>. Of course, double negation elimination, the thing they are trying to show follows from negation introduction, is what lets them do that last step in the first place! It’s clear that negation introduction only adds (introduces, if you will) negations and can’t eliminate them. The second is that any informal proof of the form “Suppose #P# … hence a contradiction” is called a “proof by contradiction”.</p>
<p>As alluded to in the articles, people develop and state false beliefs about constructive logics because they are unaware of this distinction. This is an excellent area where exploring non-classical logics can be hugely clarifying. The fact that negation introduction is not only derivable but often <em>axiomatic</em> in constructive logics, while adding double negation elimination to a constructive logic makes it classical, very clearly illustrates that there is a <em>huge</em> difference between these rules. But forget about constructive logics, what’s happening here is many mathematicians are <em>deeply confused</em> about the key thing that makes classical logics classical. Further, students are being <em>taught</em> this confusion. Clearly there is <em>some</em> pedagogical failure occurring here.</p>
<h3 id="overuse-of-indirect-proof">Overuse of Indirect Proof</h3>
<p>A major cause of the above conflation of negation introduction and double negation elimination is the overuse and overemphasis of indirect proof. This causes more problems than just the aforementioned conflation. For example, it’s <a href="https://math.stackexchange.com/q/2665890/305738">relatively</a> <a href="https://math.stackexchange.com/q/1747050/305738">common</a> to see proofs by students that are of the form: “Assume #not P#. We can show that #P# holds via [proof of #P# without using the assumption #not P#]. Hence we have #P# and #not P# and thus a contradiction. Therefore #P#.” This is, of course, silly. The indirect proof by contradiction is just a wrapper around a <em>direct</em> proof of #P#. To make this completely clear, the logical structure mirrors the logical structure of the following argument: “Assume for contradiction #1+1 != 2#. Since by calculation #1+1=2# we have a contradiction. Thus #1+1=2#.”</p>
<p>It’s not always this egregious. Often the use indirect proof isn’t <em>completely</em> superfluous like the previous examples, but there is nevertheless a direct proof which is clearer, more informative, more general, and shorter. If these proofs are clearer and shorter, why don’t students find them? I see a variety of reasons for this. First, indirect proof is conceptually more complicated so more effort is spent explaining it, more examples using it are given, and its use is encouraged for practice. Second, the proof system provided is often intimately based on classical propositional logic (e.g. the “semantic tableaux” above or resolution) and often essentially <em>requires</em> the use of indirect proof to prove anything. For example, if we take <a href="https://en.wikipedia.org/wiki/Resolution_(logic)">resolution</a> as the primitive rule, i.e. if #P vv Q# and #not Q# then #P#, then doing a case analysis requires an indirect proof in general since clearly both #P# and #Q# could hold so we can only hypothetically assume #not Q#. (If you want a more formal argument, see <a href="https://math.stackexchange.com/q/2436136/305738">this question</a> and its answers.) Relying on double negation elimination to get negation introduction is another example. It’s common in less formal approaches to effectively take all classical propositional tautologies as axioms (which blurs the distinction between syntax and semantics). Many equivalences or definitions also hide uses of indirect proof (if we were to demonstrate them with respect to a constructive logic). For example, #P =&gt; Q -= not P vv Q#, #not not P -= P#, #exists x. P(x) -= not forall x.not P(x)#, and #P vv Q -= not (not P ^^ not Q)#. Third, often definitions and theorems are given in a “negative” way so that their negations or contrapositives are actually more useful. For example, defining “injective” as #f(x)=f(y) =&gt; x = y# is a little more convenient than #x != y =&gt; f(x) != f(y)# e.g. when proving an injective function has a post-inverse. Last but not least, it certainly can be harder, up to and including impossible, to prove a statement directly (i.e. constructively). That extra information doesn’t come from nowhere. As an example, there is a simple non-constructive proof that either #sqrt(2)^sqrt(2)# or <code class="asciimath">#(sqrt(2)^sqrt(2))^sqrt(2)#</code> is a rational number that’s of the form of an irrational number raised to an irrational power, but which it is is <a href="https://en.wikipedia.org/wiki/Gelfond%E2%80%93Schneider_theorem">much</a> harder to show.</p>
<p>What I’d recommend is for instructors and authors to strongly encourage direct/constructive proofs suggesting that a direct proof should be attempted before reaching for indirect proof. Perhaps even <em>require</em> direct/constructive proofs unless the exercise specifies otherwise. This, of course, requires a proof system that doesn’t force the use of indirect proofs. I’d recommend a natural deduction or sequent style presentation where each logical connective is specified in terms of rules that are independent of the other connectives. (I’m not saying that the formal sequent calculus notation needs to be used, just that there should be a rule, formal or informal, for each rule in the usual presentation of natural deduction, say.) As in the earlier section, I strongly recommend keeping syntax and semantics separate: the proof system should be able to stand on its own. Definitions and theorems should be given in forms that are constructively-friendly, which, admittedly, can take a good amount of care. To avoid students spending a lot of time looking for simple direct proofs that don’t exist, give hints where indirect proof is or is not required. While it can be helpful to motivate the significance of direct proofs and why certain choices were made, there’s no need to explicitly reference constructive logics for any of this.</p>
<h3 id="smaller-issues">Smaller Issues</h3>
<p>Finally, here are some minor issues that either seem less common or less problematic as well as some potential missed opportunities.</p>
<p>The first is a failure of abstraction I’ve occasionally seen, and I think certain books encourage. I’ve seen students who talk about the “symbols” of their logic as “including …, <code>'('</code>, and <code>')'</code>”. While certainly how to parse strings into syntax trees is a core topic of formal <em>languages</em>, it’s not that important for formal logic. The issues with parsing formulas I see have to do with conventions on omitting parentheses, not an inability to match them. There can be some benefit to driving home that syntax is just inert symbols, but the concept of a syntax tree seems more important and the process of producing a syntax tree from a string of symbols rather secondary. Working at the level of strings adds a lot of complexity and offers little insight into the <em>logical</em> concepts.</p>
<p>As a missed opportunity, there’s a <em>lot</em> of algorithmic content in formal logic. This should come as no surprise as the mechanization of reasoning has been a long time dream of mathematics and was an explicit goal during the birth of formal logic. Unfortunately, if the instructors and/or students don’t have a programming background it can be hard to really leverage this. Still, my impression is that the only indication of the mechanizability of much of logic that students get is in the mechanical drudgery of doing certain exercises. Even when it’s not possible to actually implement some of the relevant ideas, they can still be mentioned. For example, it is often taken as a requirement of a proof system that the proofs can be mechanically verified. That is, we can write a program that takes a formal proof and tells you whether it is a valid proof or not. This should be at least mentioned. Even better, for more than just this reason, it can be experienced by using proof assistants like Coq or MetaMath or simpler things like <a href="http://logitext.mit.edu/main">Logitext</a>. There are other areas of mechanization. Clearly, we can mechanize the process of checking truth tables for classical propositional logic, though there are interesting efficiency aspects to mention here. The completeness theorem of classical propositional logic is also constructively provable producing an algorithm that will take a formula with a truth table showing its validity and will produce a formal proof of that formula. As a little more open-ended topic, much of proof <em>search</em> is mechanical. I’ve often seen students stumped by problems whose proofs can be found in a completely goal-directed manner without any guessing at all.</p>
<p>Another issue is the use of Hilbert-style presentations of logics. It does seem that many logic texts thankfully use natural deduction or some other humane proof system, but Hilbert-style presentations aren’t uncommon. The <em>benefit</em> of Hilbert-style presentations of logics is that they can simplify meta-logical reasoning. But many introductions to logic do little to no meta-logical reasoning. The downsides of Hilbert-style presentations is that they don’t match informal reasoning well, they don’t provide much insight into the logical connectives, and proofs in Hilbert-style proof systems are just painful where even simple results are puzzles. Via the lens of Curry-Howard, Hilbert-style proof systems correspond to combinatory algebras, and writing programs in combinator systems (such as <a href="http://www.madore.org/~david/programs/unlambda/">unlambda</a>) is similarly unpleasant. The fact that introducing logic using a Hilbert-style proof system is formally analogous to introducing programming using an esoteric programming language designed to be obfuscatory says something…</p>]]></summary>
</entry>
<entry>
    <title>Djinn in your browser</title>
    <link href="https://derekelkins.github.io/posts/djinn.html" />
    <id>https://derekelkins.github.io/posts/djinn.html</id>
    <published>2017-04-09 05:35:18-07:00</published>
    <updated>2017-04-09T12:35:18Z</updated>
    <summary type="html"><![CDATA[<p>I couldn’t find an online version of Djinn, so I ran it through GHCJS with some tweaks, added a basic interface and hosted it <a href="/djinn/">here</a>. It runs in your browser, so go crazy. If you want changes to the default settings or environment, feel free to suggest them. Right now everything is the default settings of the Djinn tool except that multiple results is enabled.</p>]]></summary>
</entry>
<entry>
    <title>Category Theory, Syntactically</title>
    <link href="https://derekelkins.github.io/posts/category-theory-syntactically.html" />
    <id>https://derekelkins.github.io/posts/category-theory-syntactically.html</id>
    <published>2016-11-24 22:23:17-08:00</published>
    <updated>2016-11-25T06:23:17Z</updated>
    <summary type="html"><![CDATA[<h4 id="or-how-model-theory-got-scooped-by-category-theory">(or: How Model Theory Got Scooped by Category Theory)</h4>
<h2 id="overview">Overview</h2>
<p>This will be a <em>very</em> non-traditional introduction to the ideas behind category theory. It will essentially be a slice through model theory (presented in a more programmer-friendly manner) with an unusual organization. Of course the end result will be <strong>***SPOILER ALERT***</strong> it was category theory all along. A secret decoder ring will be provided at the end. This approach is inspired by the notion of an internal logic/language and by Vaughn Pratt’s paper <a href="http://boole.stanford.edu/pub/yon.pdf">The Yoneda Lemma Without Category Theory</a>. <!-- \* --></p>
<p>I want to be very clear, though. This is not meant to be an analogy or an example or a guide for intuition. This <em>is</em> category theory. It is simply presented in a different manner.</p>
<p>Dan Doel pointed me at some draft lecture notes by Mike Shulman that seem very much in the spirit of this blog post (albeit aimed at an audience familiar with category theory): <a href="https://mikeshulman.github.io/catlog/catlog.pdf">Categorical Logic from a Categorical Point of View</a>. A theory in my sense corresponds to a category presentation (Definition 1.7.1) as defined by these lecture notes. Its oft-mentioned Appendix A will also look very familiar.</p>
<!--more-->
<h2 id="theories">Theories</h2>
<p>The first concept we’ll need is that of a theory. If you’ve ever implemented an interpreter for even the simplest language, then most of what follows modulo some terminological differences should be both familiar and very basic. If you are familiar with algebraic semantics, then that is exactly what is happening here only restricting to unary (but multi-sorted) algebraic theories.</p>
<p>For us, a <strong>theory</strong>, #ccT#, is a collection of <strong>sorts</strong>, a collection of (unary) <strong>function symbols</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, and a collection of <strong>equations</strong>. Each function symbol has an input sort and an output sort which we’ll call the <strong>source</strong> and <strong>target</strong> of the function symbol. We’ll write #ttf : A -&gt; B# to say that #ttf# is a function symbol with source #A# and target #B#. We define <code class="asciimath">#"src"(ttf) -= A#</code> and <code class="asciimath">#"tgt"(ttf) -= B#</code>. Sorts and function symbols are just symbols. Something is a sort if it is in the collection of sorts. Nothing else is required. A function symbol is not a function, it’s just a, possibly structured, name. Later, we’ll map those names to functions, but the same name may be mapped to different functions. In programming terms, a theory defines an interface or signature. We’ll write <code class="asciimath">#bb "sort"(ccT)#</code> for the collection of sorts of #ccT# and <code class="asciimath">#bb "fun"(ccT)#</code> for the collection of function symbols.</p>
<p>A (raw) <strong>term</strong> in a theory is either a <strong>variable</strong> labelled by a sort, #bbx_A#, or it’s a function symbol applied to a term, <code class="asciimath">#tt "f"(t)#</code>, such that the sort of the term #t# matches the source of #ttf#. The sort or target of a term is the sort of the variable if it’s a variable or the target of the outermost function symbol. The source of a term is the sort of the innermost variable. In fact, all terms are just sequences of function symbol applications to a variable, so there will always be exactly one variable. All this is to say the expressions need to be “well-typed” in the obvious way. Given a theory with two function symbols #ttf : A -&gt; B# and #ttg : B -&gt; A#, <code class="asciimath">#bbx_A#</code>, <code class="asciimath">#bbx_B#</code> , <code class="asciimath">#tt "f"(bbx_A)#</code>, and <code class="asciimath">#tt "f"(tt "g"(tt "f"(bbx_A)))#</code> are all examples of terms. <code class="asciimath">#tt "f"(bbx_B)#</code> and <code class="asciimath">#tt "f"(tt "f"(bbx_A))#</code> are not terms because they are not “well-typed”, and #ttf# by itself is not a term simply because it doesn’t match the syntax. Using Haskell syntax, we can define a data type representing this syntax if we ignore the sorting:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">data</span> <span class="dt">Term</span> <span class="ot">=</span> <span class="dt">Var</span> <span class="dt">Sort</span> <span class="op">|</span> <span class="dt">Apply</span> <span class="dt">FunctionSymbol</span> <span class="dt">Term</span></span></code></pre></div>
<p>Using GADTs, we could capture the sorting constraints as well:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">data</span> <span class="dt">Term</span> (<span class="ot">s ::</span> <span class="dt">Sort</span>) (<span class="ot">t ::</span> <span class="dt">Sort</span>) <span class="kw">where</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="dt">Var</span><span class="ot"> ::</span> <span class="dt">Term</span> t t</span>
<span id="cb2-3"><a href="#cb2-3"></a>    <span class="dt">Apply</span><span class="ot"> ::</span> <span class="dt">FunctionSymbol</span> x t <span class="ot">-&gt;</span> <span class="dt">Term</span> s x <span class="ot">-&gt;</span> <span class="dt">Term</span> s t</span></code></pre></div>
<p>An important operation on terms is <strong>substitution</strong>. Given a term #t_1# with source #A# and a term #t_2# with target #A# we define the substitution of #t_2# into #t_1#, written #t_1[bbx_A |-&gt; t_2]#, as:</p>
<blockquote>
<p>If #t_1 = bbx_A# then #bbx_A[bbx_A |-&gt; t_2] -= t_2#.</p>
<p>If <code class="asciimath">#t_1 = tt "f"(t)#</code> then <code class="asciimath">#tt "f"(t)[bbx_A |-&gt; t_2] -= tt "f"(t[bbx_A |-&gt; t_2])#</code>.</p>
</blockquote>
<p>Using the theory from before, we have:</p>
<blockquote>
<p><code class="asciimath">#tt "f"(bbx_A)[bbx_A |-&gt; tt "g"(bbx_B)] = tt "f"(tt "g"(bbx_B))#</code></p>
</blockquote>
<p>As a shorthand, for arbitrary terms #t_1# and #t_2#, #t_1(t_2)# will mean <code class="asciimath">#t_1[bbx_("src"(t_1)) |-&gt; t_2]#</code>.</p>
<p>Finally, equations<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. An <strong>equation</strong> is a pair of terms with equal source and target, for example, <code class="asciimath">#(: tt "f"(tt "g"(bbx_B)), bbx_B :)#</code>. The idea is that we want to identify these two terms. To do this we <a href="../posts/quotient-types-for-programmers.html">quotient</a> the set of terms by the congruence generated by these pairs, i.e. by the reflexive-, symmetric-, transitive-closure of the relation generated by the equations which further satisfies “if #s_1 ~~ t_1# and #s_2 ~~ t_2# then <code class="asciimath">#s_1(s_2) ~~ t_1(t_2)#</code>”. From now on, by “terms” I’ll mean this quotient with “raw terms” referring to the unquotiented version. This means that when we say “<code class="asciimath">#tt "f"(tt "g"(bbx_B)) = bbx_B#</code>”, we really mean the two terms are congruent with respect to the congruence generated by the equations. We’ll write #ccT(A, B)# for the collection of terms, in this sense, with source #A# and target #B#. To make things look a little bit more normal, I’ll write #s ~~ t# as a synonym for #(: s, t :)# when the intent is that the pair represents a given equation.</p>
<p>Expanding the theory from before, we get the <strong>theory of isomorphisms</strong>, <code class="asciimath">#ccT_{:~=:}#</code>, consisting of two sorts, #A# and #B#, two function symbols, #ttf# and #ttg#, and two equations <code class="asciimath">#tt "f"(tt "g"(bbx_B)) ~~ bbx_B#</code> and <code class="asciimath">#tt "g"(tt "f"(bbx_A)) ~~ bbx_A#</code>. The equations lead to equalities like <code class="asciimath">#tt "f"(tt "g"(tt "f"(bbx_A))) = tt "f"(bbx_A)#</code>. In fact, it doesn’t take much work to show that this theory only has four distinct terms: #bbx_A#, #bbx_B#, <code class="asciimath">#tt "f"(bbx_A)#</code>, and <code class="asciimath">#tt "g"(bbx_B)#</code>.</p>
<p>In traditional model theory or universal algebra we tend to focus on multi-ary operations, i.e. function symbols that can take multiple inputs. By restricting ourselves to only unary function symbols, we expose a duality. For every theory #ccT#, we have the <strong>opposite theory</strong>, #ccT^(op)# defined by using the same sorts and function symbols but swapping the source and target of the function symbols which also requires rewriting the terms in the equations. The rewriting on terms is the obvious thing, e.g. if #ttf : A -&gt; B#, #ttg : B -&gt; C#, and #tth : C -&gt; D#, then the term in #ccT#, <code class="asciimath">#tt "h"(tt "g"(tt "f"(bbx_A)))#</code> would become the term <code class="asciimath">#tt "f"(tt "g"(tt "h"(bbx_D)))#</code> in #ccT^(op)#. From this it should be clear that <code class="asciimath">#(ccT^(op))^(op) = ccT#</code>.</p>
<h2 id="product-theories">Product Theories</h2>
<p>Given two theories #ccT_1# and #ccT_2# we can form a new theory #ccT_1 xx ccT_2# called the <strong>product theory</strong> of #ccT_1# and #ccT_2#. The sorts of this theory are pairs of sorts from #ccT_1# and #ccT_2#. The collection of function symbols is the <em>disjoint union</em> <code class="asciimath">#bb "fun"(ccT_1) xx bb "sort"(ccT_2) + bb "sort"(ccT_1) xx bb "fun"(ccT_2)#</code>. A disjoint union is like Haskell’s <code>Either</code> type. Here we’ll write <code class="asciimath">#tt "inl"#</code> and <code class="asciimath">#tt "inr"#</code> for the left and right injections respectively. <code class="asciimath">#tt "inl"#</code> takes a function symbol from #ccT_1# and a sort from #ccT_2# and produces a function symbol of #ccT_1 xx ccT_2# and similarly for <code class="asciimath">#tt "inr"#</code>. If <code class="asciimath">#tt "f" : A -&gt; B#</code> in #ccT_1# and #C# is a sort of #ccT_2#, then <code class="asciimath">#tt "inl"(f, C) : (A, C) -&gt; (B, C)#</code> and similarly for <code class="asciimath">#tt "inr"#</code>.</p>
<p>The collection of equations for #ccT_1 xx ccT_2# consists of the following:</p>
<ul>
<li>for every equation, #l ~~ r# of #ccT_1# and every sort, #C#, of #ccT_2# we produce an equation #l’ ~~ r’# by replacing each function symbol #ttf# in #l# and #r# with <code class="asciimath">#tt "inl"(tt "f", C)#</code></li>
<li>similarly for equations of #ccT_2#</li>
<li>for every pair of function symbols #ttf : A -&gt; B# from #ccT_1# and #ttg : C -&gt; D# from #ccT_2#, we produce the equation <code class="asciimath">#tt "inl"(tt "f", D)(tt "inr"(A, tt "g")(bbx_{:(A, C")":})) ~~ tt "inr"(B, tt "g")("inl"(tt "f", C)(bbx_{:(A, C")":}))#</code></li>
</ul>
<p>The above is probably unreadable. If you work through it, you can show that every term of #ccT_1 xx ccT_2# is equivalent to a pair of terms #(t_1, t_2)# where #t_1# is a term in #ccT_1# and #t_2# is a term in #ccT_2#. Using this equivalence, the first bullet is seen to be saying that if #l = r# in #ccT_1# and #C# is a sort in #ccT_2# then #(l, bbx_C) = (r, bbx_C)# in #ccT_1 xx ccT_2#. The second is similar. The third then states</p>
<blockquote>
<p><code class="asciimath">#(t_1, bbx_C)((bbx_A, t_2)(bbx_{:"(A, C)":})) = (t_1, t_2)(bbx_{:"(A, C)":}) = (bbx_A, t_2)((t_1, bbx_C)(bbx_{:"(A, C)":}))#</code>.</p>
</blockquote>
<p>To establish the equivalence between terms of #ccT_1 xx ccT_2# and pairs of terms from #ccT_1# and #ccT_2#, we use the third bullet to move all the <code class="asciimath">#tt "inl"#</code>s outward at which point we’ll have a sequence of #ccT_1# function symbols followed by a sequence of #ccT_2# function symbols each corresponding to term.</p>
<p>The above might seem a bit round about. An alternative approach would be to define the function symbols of #ccT_1 xx ccT_2# to be all pairs of all the <em>terms</em> from #ccT_1# and #ccT_2#. The problem with this approach is that it leads to an <em>explosion</em> in the number of function symbols and equations required. In particular, it easily produces an infinitude of function symbols and equations even when provided with theories that only have a finite number of sorts, function symbols, and equations.</p>
<p>As a concrete and useful example, consider the theory #ccT_bbbN# consisting of a single sort, #0#, a single function symbol, #tts#, and no equations. This theory has a term for each natural number, #n#, corresponding to #n# applications of #tts#. Now let’s articulate #ccT_bbbN xx ccT_bbbN#. It has one sort, #(0, 0)#, two function symbols, <code class="asciimath">#tt "inl"(tt "s", 0)#</code> and <code class="asciimath">#tt "inr"(0, tt "s")#</code>, and it has one equation, <code class="asciimath">#tt "inl"(tt "s", 0)(tt "inr"(0, tt "s")(bbx_{:(0, 0")":})) ~~ tt "inr"(0, tt "s")(tt "inl"(tt "s", 0)(bbx_{:(0, 0")":}))#</code>. Unsurprisingly, the terms of this theory correspond to pairs of natural numbers. If we had used the alternative definition, we’d have had an infinite number of function symbols and an infinite number of equations.</p>
<p>Nevertheless, for clarity I will typically write a term of a product theory as a pair of terms.</p>
<p>As a relatively easy exercise — easier than the above — you can formulate and define the disjoint sum of two theories #ccT_1 + ccT_2#. The idea is that every term of #ccT_1 + ccT_2# corresponds to either a term of #ccT_1# or a term of #ccT_2#. Don’t forget to define what happens to the equations.</p>
<p>Related to these, we have the theory #ccT_{:bb1:}#, which consists of one sort and no function symbols or equations, and #ccT_{:bb0:}# which consists of no sorts and thus no possibility for function symbols or equations. #ccT_{:bb1:}# has exactly one term while #ccT_{:bb0:}# has no terms.</p>
<h2 id="collages">Collages</h2>
<p>Sometimes we’d like to talk about function symbols whose source is in one theory and target is in another. As a simple example, that we’ll explore in more depth later, we may want function symbols whose sources are in a product theory. This would let us consider terms with multiple inputs.</p>
<p>The natural way to achieve this is to simply make a new theory that contains sorts from both theories plus the new function symbols. A <strong>collage</strong>, #ccK#, from a theory #ccT_1# to #ccT_2#, written #ccK : ccT_1 ↛ ccT_2#, is a theory whose collection of sorts is the disjoint union of the sorts of #ccT_1# and #ccT_2#. The function symbols of #ccK# consist for each function symbol #ttf : A -&gt; B# in #ccT_1#, a function symbol <code class="asciimath">#tt "inl"(ttf) : tt "inl"(A) -&gt; tt "inl"(B)#</code>, and similarly for function symbols from #ccT_2#. Equations from #ccT_1# and #ccT_2# are likewise taken and lifted appropriately, i.e. #ttf# is replaced with <code class="asciimath">#tt "inl"(ttf)#</code> or <code class="asciimath">#tt "inr"(ttf)#</code> as appropriate. Additional function symbols of the form <code class="asciimath">#k : tt "inl"(A) -&gt; tt "inr"(Z)#</code> where #A# is a sort of #ccT_1# and #Z# is a sort of #ccT_2#, and potentially additional equations involving these function symbols, may be given. (If no additional function symobls are given, then this is exactly the disjoint sum of #ccT_1# and #ccT_2#.) These additional function symbols and equations are what differentiate two collages that have the same source and target theories. Note, there are <em>no</em> function symbols <code class="asciimath">#tt "inr"(Z) -&gt; tt "inl"(A)#</code>, i.e. where #Z# is in #ccT_2# and #A# is in #ccT_1#. That is, there are no function symbols going the “other way”. To avoid clutter, I’ll typically assume that the sorts and function symbols of #ccT_1# and #ccT_2# are disjoint already, and dispense with the <code class="asciimath">#tt "inl"#</code>s and <code class="asciimath">#tt "inr"#</code>s.</p>
<p>Summarizing, we have <code class="asciimath">#ccK(tt "inl"(A), "inl"(B)) ~= ccT_1(A, B)#</code>, <code class="asciimath">#ccK(tt "inr"(Y), tt "inr"(Z)) ~= ccT_2(Y, Z)#</code>, and <code class="asciimath">#ccK(tt "inr"(Z), tt "inl"(A)) = O/#</code> for all #A#, #B#, #Y#, and #Z#. <code class="asciimath">#ccK(tt "inl"(A), tt "inr"(Z))#</code> for any #A# and #Z# is arbitrary generated. To distinguish them, I’ll call the function symbols that go from one theory to another <strong>bridges</strong>. More generally, an arbitrary term that has it’s source in one theory and target in another will be described as a <strong>bridging term</strong>.</p>
<p>Here’s a somewhat silly example. Consider #ccK_+ : ccT_bbbN xx ccT_bbbN ↛ ccT_bbbN# that has one bridge <code class="asciimath">#tt "add" : (0, 0) -&gt; 0#</code> with the equations <code class="asciimath">#tt "add"(tt "inl"(tts, 0)(bbx_("("0, 0")"))) ~~ tts(tt "add"(bbx_("("0, 0")")))#</code> and <code class="asciimath">#tt "add"(tt "inr"(0, tts)(bbx_("("0, 0")"))) ~~ tts(tt "add"(bbx_("("0, 0")")))#</code>.</p>
<p>More usefully, if a bit degenerately, every theory induces a collage in the following way. Given a theory #ccT#, we can build the collage #ccK_ccT : ccT ↛ ccT# where the bridges consist of the following. For each sort, #A#, of #ccT#, we have the following bridge: <code class="asciimath">#tt "id"_A : tt "inl"(A) -&gt; tt "inr"(A)#</code>. Then, for every function symbol, #ttf : A -&gt; B# in #ccT#, we have the following equation: <code class="asciimath">#tt "inl"(tt "f")(tt "id"_A(bbx_(tt "inl"(A)))) ~~ tt "id"_B(tt "inr"(tt "f")(bbx_(tt "inl"(A))))#</code>. We have <code class="asciimath">#ccK_ccT(tt "inl"(A), tt "inr"(B)) ~= ccT(A, B)#</code>.</p>
<p>You can think of a bridging term in a collage as a sequence of function symbols partitioned into two parts by a bridge. Naturally, we might consider partitioning into more than two parts by having more than one bridge. It’s easy to generalize the definition of collage to combine an arbitrary number of theories, but I’ll take a different, probably worse, route. Given collages #ccK_1 : ccT_1 ↛ ccT_2# and #ccK_2 : ccT_2 ↛ ccT_3#, we can make the collage #ccK_2 @ ccK_1 : ccT_1 ↛ ccT_3# by defining its bridges to be triples of a bridge of #ccK_1#, #k_1 : A_1 -&gt; A_2#, a term, #t : A_2 -&gt; B_2# of #ccT_2#, and a bridge of #ccK_2#, #k_2 : B_2 -&gt; B_3# which altogether will be a bridge of #ccK_2 @ ccK_1# going from #A_1 -&gt; B_3#. These triples essentially represent a term like <code class="asciimath">#k_2(t(k_1(bbx_(A_1))))#</code>. With this intuition we can formulate the equations. For each equation <code class="asciimath">#t'(k_1(t_1)) ~~ s'(k'_1(s_1))#</code> where #k_1# and <code class="asciimath">#k'_1#</code> are bridges of #ccK_1#, we have for every bridge #k_2# of #ccK_2# and term #t# of the appropriate sorts <code class="asciimath">#(k_2, t(t'(bbx)), k_1)(t_1) ~~ (k_2, t(s'(bbx)), k'_1)(s_1)#</code> and similarly for equations involving the bridges of #ccK_2#.</p>
<p>This composition is associative… almost. Furthermore, the collages generated by theories, #ccK_ccT#, behave like identities to this composition… almost. It turns out these statements are true, but only up to isomorphism of theories. That is, #(ccK_3 @ ccK_2) @ ccK_1 ~= ccK_3 @ (ccK_2 @ ccK_1)# but is not equal.</p>
<p>To talk about isomorphism of theories we need the notion of…</p>
<h2 id="interpretations">Interpretations</h2>
<p>An interpretation of a theory gives meaning to the syntax of a theory. There are two nearly identical notions of interpretation for us: interpretation (into sets) and interpretation into a theory. I’ll define them in parallel. An <strong>interpretation</strong> (<strong>into a theory</strong>), #ccI#, is a mapping, written #⟦-⟧^ccI# though the superscript will often be omitted, which maps sorts to sets (sorts) and function symbols to functions (terms). The mapping satisfies:</p>
<blockquote>
<p><code class="asciimath">#⟦"src"(f)⟧ = "src"(⟦f⟧)#</code> and <code class="asciimath">#⟦"tgt"(f)⟧ = "tgt"(⟦f⟧)#</code> where <code class="asciimath">#"src"#</code> and <code class="asciimath">#"tgt"#</code> on the right are the domain and codomain operations for an interpretation.</p>
</blockquote>
<p>We extend the mapping to a mapping on terms via:</p>
<blockquote>
<ul>
<li>#⟦bbx_A⟧ = x |-&gt; x#, i.e. the identity function, or, for interpretation into a theory, <code class="asciimath">#⟦bbx_A⟧ = bbx_{:⟦A⟧:}#</code></li>
<li><code class="asciimath">#⟦tt "f"(t)⟧ = ⟦tt "f"⟧ @ ⟦t⟧#</code> or, for interpretation into a theory, <code class="asciimath">#⟦tt "f"(t)⟧ = ⟦tt "f"⟧(⟦t⟧)#</code></li>
</ul>
</blockquote>
<p>and we require that for any equation of the theory, #l ~~ r#, #⟦l⟧ = ⟦r⟧#. (Technically, this is implicitly required for the extension of the mapping to terms to be well-defined, but it’s clearer to state it explicitly.) I’ll write <code class="asciimath">#ccI : ccT -&gt; bb "Set"#</code> when #ccI# is an interpretation of #ccT# into sets, and #ccI’ : ccT_1 -&gt; ccT_2# when #ccI’# is an interpretation of #ccT_1# into #ccT_2#.</p>
<p>An interpretation of the theory of isomorphisms produces a bijection between two specified sets. Spelling out a simple example where #bbbB# is the set of booleans:</p>
<blockquote>
<ul>
<li>#⟦A⟧ -= bbbB#</li>
<li>#⟦B⟧ -= bbbB#</li>
<li><code class="asciimath">#⟦tt "f"⟧ -= x |-&gt; not x#</code></li>
<li><code class="asciimath">#⟦tt "g"⟧ -= x |-&gt; not x#</code></li>
</ul>
</blockquote>
<p>plus the proof #not not x = x#.</p>
<p>As another simple example, we can interpret the theory of isomorphisms into itself slightly non-trivially.</p>
<blockquote>
<ul>
<li>#⟦A⟧ -= B#</li>
<li>#⟦B⟧ -= A#</li>
<li><code class="asciimath">#⟦tt "f"⟧ -= tt "g"(bbx_B)#</code></li>
<li><code class="asciimath">#⟦tt "g"⟧ -= tt "f"(bbx_A)#</code></li>
</ul>
</blockquote>
<p>As an (easy) exercise, you should define #pi_1 : ccT_1 xx ccT_2 -&gt; ccT_1# and similarly #pi_2#. If you defined #ccT_1 + ccT_2# before, you should define #iota_1 : ccT_1 -&gt; ccT_1 + ccT_2# and similarly for #iota_2#. As another easy exercise, show that an interpretation of <code class="asciimath">#ccT_{:~=:}#</code> is a bijection. In Haskell, an interpretation of #ccT_bbbN# would effectively be <code>foldNat</code>. Something very interesting happens when you consider what an interpretation of the collage generated by a theory, #ccK_ccT#, is. Spell it out. In a different vein, you can show that a collage #ccK : ccT_1 ↛ ccT_2# and an interpretation <code class="asciimath">#ccT_1^(op) xx ccT_2 -&gt; bb "Set"#</code> are essentially the same thing in the sense that each gives rise to the other.</p>
<p>Two theories are <strong>isomorphic</strong> if there exists interpretations <code class="asciimath">#ccI_1 : ccT_1 -&gt; ccT_2#</code> and <code class="asciimath">#ccI_2 : ccT_2 -&gt; ccT_1#</code> such that <code class="asciimath">#⟦⟦A⟧^(ccI_1)⟧^(ccI_2) = A#</code> and vice versa, and similarly for function symbols. In other words, each is interpretable in the other, and if you go from one interpretation and then back, you end up where you started. Yet another way to say this is that there is a one-to-one correspondence between sorts and terms of each theory, and this correspondence respects substitution.</p>
<p>As a crucially important example, the set of terms, #ccT(A, B)#, can be extended to an interpretation. In particular, for each sort #A#, <code class="asciimath">#ccT(A, -) : ccT -&gt; bb "Set"#</code>. It’s action on function symbols is the following:</p>
<blockquote>
<p><code class="asciimath">#⟦tt "f"⟧^(ccT(A, -)) -= t |-&gt; tt "f"(t)#</code></p>
</blockquote>
<p>We have, dually, <code class="asciimath">#ccT(-, A) : ccT^(op) -&gt; bb "Set"#</code> with the following action:</p>
<blockquote>
<p><code class="asciimath">#⟦tt "f"⟧^(ccT(-, A)) -= t |-&gt; t(tt "f"(bbx_B))#</code></p>
</blockquote>
<p>We can abstract from both parameters making <code class="asciimath">#ccT(-, =) : ccT^(op) xx ccT -&gt; bb "Set"#</code> which, by an early exercise, can be shown to correspond with the collage #ccK_ccT#.</p>
<p>Via an abuse of notation, I’ll identify #ccT^(op)(A, -)# with #ccT(-, A)#, though technically we only have an isomorphism between the interpretations, and to talk about isomorphisms between interpretations we need the notion of…</p>
<h2 id="homomorphisms">Homomorphisms</h2>
<p>The theories we’ve presented are (multi-sorted) universal algebra theories. Universal algebra allows us to specify a general notion of “homomorphism” that generalizes monoid homomorphism or group homomorphism or ring homomorphism or lattice homomorphism.</p>
<p>In universal algebra, the algebraic theory of groups consists of a single sort, a nullary operation, #1#, a binary operation, <code class="asciimath">#*#</code>, a unary operation, <code class="asciimath">#tt "inv"#</code>, and some equations which are unimportant for us. Operations correspond to our function symbols except that they’re are not restricted to being unary. A particular group is a particular interpretation of the algebraic theory of groups, i.e. it is a set and three functions into the set. A group homomorphism then is a function between those two groups, i.e. between the two interpretations, that preserves the operations. In a traditional presentation this would look like the following:</p>
<blockquote>
<p>Say #alpha : G -&gt; K# is a group homomorphism from the group #G# to the group #K# and #g, h in G# then:</p>
<ul>
<li>#alpha(1_G) = 1_K#</li>
<li><code class="asciimath">#alpha(g *_G h) = alpha(g) *_K alpha(h)#</code></li>
<li><code class="asciimath">#alpha(tt "inv"_G(g)) = tt "inv"_K(alpha(g))#</code></li>
</ul>
</blockquote>
<p>Using something more akin to our notation, it would look like:</p>
<blockquote>
<ul>
<li>#alpha(⟦1⟧^G) = ⟦1⟧^K#</li>
<li><code class="asciimath">#alpha(⟦*⟧^G(g,h)) = ⟦*⟧^K(alpha(g), alpha(h))#</code></li>
<li><code class="asciimath">#alpha(⟦tt "inv"⟧^G(g)) = ⟦tt "inv"⟧^K(alpha(g))#</code></li>
</ul>
</blockquote>
<p>The <code class="asciimath">#tt "inv"#</code> case is the most relevant for us as it is unary. However, for us, a function symbol #ttf# may have a different source and target and so we made need a different function on each side of the equation. E.g. for #ttf : A -&gt; B#, #alpha : ccI_1 -&gt; ccI_2#, and #a in ⟦A⟧^(ccI_1)# we’d have:</p>
<blockquote>
<p><code class="asciimath">#alpha_B(⟦tt "f"⟧^(ccI_1)(a)) = ⟦tt "f"⟧^(ccI_2)(alpha_A(a))#</code></p>
</blockquote>
<p>So a <strong>homomorphism</strong> <code class="asciimath">#alpha : ccI_1 -&gt; ccI_2 : ccT -&gt; bb "Set"#</code> is a <em>family</em> of functions, one for each sort of #ccT#, that satisfies the above equation for every function symbol of #ccT#. We call the individual functions making up #alpha# <strong>components</strong> of #alpha#, and we have <code class="asciimath">#alpha_A : ⟦A⟧^(ccI_1) -&gt; ⟦A⟧^(ccI_2)#</code>. The definition for an interpretation into a theory, #ccT_2#, is identical except the components of #alpha# are terms of #ccT_2# and #a# can be replaced with #bbx_(⟦A⟧^(ccI_1))#. Two interpretations are <strong>isomorphic</strong> if we have homomorphism #alpha : ccI_1 -&gt; ccI_2# such that each component is a bijection. This is the same as requiring a homomorphism #beta : ccI_2 -&gt; ccI_1# such that for each #A#, #alpha_A(beta_A(x)) = x# and #beta_A(alpha_A(x)) = x#. A similar statement can be made for interpretations into theories, just replace #x# with <code class="asciimath">#bbx_(⟦A⟧)#</code>.</p>
<p>Another way to look at homomorphisms is via collages. A homomorphism <code class="asciimath">#alpha : ccI_1 -&gt; ccI_2 : ccT -&gt; bb "Set"#</code> gives rise to an interpretation of the collage #ccK_ccT#. The interpretation <code class="asciimath">#ccI_alpha : ccK_ccT -&gt; bb "Set"#</code> is defined by:</p>
<blockquote>
<ul>
<li><code class="asciimath">#⟦tt "inl"(A)⟧^(ccI_alpha) -= ⟦A⟧^(ccI_1)#</code></li>
<li><code class="asciimath">#⟦tt "inr"(A)⟧^(ccI_alpha) -= ⟦A⟧^(ccI_2)#</code></li>
<li><code class="asciimath">#⟦tt "inl"(ttf)⟧^(ccI_alpha) -= ⟦ttf⟧^(ccI_1)#</code></li>
<li><code class="asciimath">#⟦tt "inr"(ttf)⟧^(ccI_alpha) -= ⟦ttf⟧^(ccI_2)#</code></li>
<li><code class="asciimath">#⟦tt "id"_A⟧^(ccI_alpha) -= alpha_A#</code></li>
</ul>
</blockquote>
<p>The homomorphism law guarantees that it satisfies the equation on <code class="asciimath">#tt "id"#</code>. Conversely, given an interpretation of #ccK_ccT#, we have the homomorphism, <code class="asciimath">#⟦tt "id"⟧ : ⟦tt "inl"(-)⟧ -&gt; ⟦tt "inr"(-)⟧ : ccT -&gt; bb "Set"#</code>. and the equation on <code class="asciimath">#tt "id"#</code> is exactly the homomorphism law.</p>
<h2 id="yoneda">Yoneda</h2>
<p>Consider a homomorphism #alpha : ccT(A, -) -&gt; ccI#. The #alpha# needs to satisfy for every sort #B# and #C#, every function symbol #ttf : C -&gt; D#, and every term #t : B -&gt; C#:</p>
<blockquote>
<p><code class="asciimath">#alpha_D(tt "f"(t)) = ⟦tt "f"⟧^ccI(alpha_C(t))#</code></p>
</blockquote>
<p>Looking at this equation, the possibility of viewing it as a recursive “definition” leaps out suggesting that the action of #alpha# is completely determined by it’s action on the variables. Something like this, for example:</p>
<blockquote>
<p><code class="asciimath">#alpha_D(tt "f"(tt "g"(tt "h"(bbx_A)))) = ⟦tt "f"⟧(alpha_C(tt "g"(tt "h"(bbx_A)))) = ⟦tt "f"⟧(⟦tt "g"⟧(alpha_B(tt "h"(bbx_A)))) = ⟦tt "f"⟧(⟦tt "g"⟧(⟦tt "h"⟧(alpha_A(bbx_A))))#</code></p>
</blockquote>
<p>We can easily establish that there’s a one-to-one correspondence between the set of homomorphisms #ccT(A, -) -&gt; ccI# and the elements of the set #⟦A⟧^ccI#. Given a homomorphism, #alpha#, we get an element of #⟦A⟧^ccI# via #alpha_A(bbx_A)#. Inversely, given an element #a in ⟦A⟧^ccI#, we can define a homomorphism <code class="asciimath">#a^**#</code> via:</p>
<blockquote>
<ul>
<li><code class="asciimath">#a_D^**(tt "f"(t)) -= ⟦tt "f"⟧^ccI(a_C^**(t))#</code></li>
<li><code class="asciimath">#a_A^**(bbx_A) -= a#</code></li>
</ul>
</blockquote>
<p>which clearly satisfies the condition on homomorphisms by definition. It’s easy to verify that <code class="asciimath">#(alpha_A(bbx_A))^** = alpha#</code> and immediately true that <code class="asciimath">#a^**(bbx_A) = a#</code> establishing the bijection.</p>
<p>We can state something stronger. Given any homomorphism #alpha : ccT(A, -) -&gt; ccI# and any function symbol #ttg : A -&gt; X#, we can make a new homomorphism #alpha * ttg : ccT(X, -) -&gt; ccI# via the following definition:</p>
<blockquote>
<p><code class="asciimath">#(alpha * ttg)(t) = alpha(t(tt "g"(bbx_A)))#</code></p>
</blockquote>
<p>Verifying that this is a homomorphism is straightforward:</p>
<blockquote>
<p><code class="asciimath">#(alpha * ttg)(tt "f"(t)) = alpha(tt "f"(t(tt "g"(bbx_A)))) = ⟦tt "f"⟧(alpha(t(tt "g"(bbx_A)))) = ⟦tt "f"⟧((alpha * ttg)(t))#</code></p>
</blockquote>
<p>and like any homomorphism of this form, as we’ve just established, it is completely determined by it’s action on variables, namely <code class="asciimath">#(alpha * ttg)_A(bbx_A) = alpha_X(tt "g"(bbx_A)) = ⟦tt "g"⟧(alpha_A(bbx_A))#</code>. In particular, if <code class="asciimath">#alpha = a^**#</code>, then we have <code class="asciimath">#a^** * ttg = (⟦tt "g"⟧(a))^**#</code>. Together these facts establish that we have an interpretation <code class="asciimath">#ccY : ccT -&gt; bb "Set"#</code> such that <code class="asciimath">#⟦A⟧^ccY -= (ccT(A, -) -&gt; ccI)#</code>, the <em>set</em> of homomorphisms, and <code class="asciimath">#⟦tt "g"⟧^ccY(alpha) -= alpha * tt "g"#</code>. The work we did before established that we have homomorphisms #(-)(bbx) : ccY -&gt; ccI# and <code class="asciimath">#(-)^** : ccI -&gt; ccY#</code> that are inverses. This is true for all theories and all interpretations as at no point did we use any particular facts about them. This statement is the (dual form of the) Yoneda lemma. To get the usual form simply replace #ccT# with #ccT^(op)#. A particularly important and useful case (so useful it’s usually used tacitly) occurs when we choose #ccI = ccT(B,-)#, we get #(ccT(A, -) -&gt; ccT(B, -)) ~= ccT(B, A)# or, choosing #ccT^(op)# everywhere, #(ccT(-, A) -&gt; ccT(-, B)) ~= ccT(A, B)# which states that a term from #A# to #B# is equivalent to a homomorphism from #ccT(-, A)# to #ccT(-, B)#.</p>
<p>There is another result, dual in a different way, called the co-Yoneda lemma. It turns out it is a corollary of the fact that for a collage #ccK : ccT_1 ↛ ccT_2#, <code class="asciimath">#ccK_(ccT_2) @ ccK ~= ccK#</code> and the dual is just the composition the other way. To get (closer to) the precise result, we need to be able to turn an interpretation into a collage. Given an interpretation, <code class="asciimath">#ccI : ccT -&gt; bb "Set"#</code>, we can define a collage #ccK_ccI : ccT_bb1 ↛ ccT# whose bridges from #1 -&gt; A# are the elements of #⟦A⟧^ccI#. Given this, the co-Yoneda lemma is the special case, #ccK_ccT @ ccK_ccI ~= ccK_ccI#.</p>
<p>Note, that the Yoneda and co-Yoneda lemmas only apply to interpretations into sets as #ccY# involves the <em>set</em> of homomorphisms.</p>
<h2 id="representability">Representability</h2>
<p>The Yoneda lemma suggests that the interpretations #ccT(A, -)# and #ccT(-, A)# are particularly important and this will be borne out as we continue.</p>
<p>We call an interpretation, <code class="asciimath">#ccI : ccT^(op) -&gt; bb "Set"#</code> <strong>representable</strong> if #ccI ~= ccT(-, X)# for some sort #X#. We then say that #X# <strong>represents</strong> #ccI#. What this states is that every term of sort #X# corresponds to an element in one of the sets that make up #ccI#, and these transform appropriately. There’s clearly a particularly important element, namely the image of #bbx_X# which corresponds to an element in #⟦X⟧^ccI#. This element is called the <strong>universal element</strong>. The dual concept is, for <code class="asciimath">#ccI : ccT -&gt; bb "Set"#</code>, #ccI# is <strong>co-representable</strong> if #ccI ~= ccT(X, -)#. We will also say #X# represents #ccI# in this case as it actually does when we view #ccI# as an interpretation of <code class="asciimath">#(ccT^(op))^(op)#</code>.</p>
<p>As a rather liberating exercise, you should establish the following result called <strong>parameterized representability</strong>. Assume we have theories #ccT_1# and #ccT_2#, and a family of sorts of #ccT_2#, #X#, and a family of interpretations of #ccT_2^(op)#, #ccI#, both indexed by sorts of #ccT_1#, such that for each <code class="asciimath">#A in bb "sort"(ccT_1)#</code>, #X_A# represents #ccI_A#, i.e. #ccI_A ~= ccT_2(-, X_A)#. Given all this, then there is a <em>unique</em> interpretation #ccX : ccT_1 -&gt; ccT_2# and <code class="asciimath">#ccI : ccT_1 xx ccT_2^(op) -&gt; bb "Set"#</code> where #⟦A⟧^(ccX) -= X_A# and <code class="asciimath">#"⟦("A, B")⟧"^ccI -= ⟦B⟧^(ccI_A)#</code> such that #ccI ~= ccT_2(=,⟦-⟧^ccX)#. To be a bit more clear, the right hand side means #(A, B) |-&gt; ccT_2(B, ⟦A⟧^ccX)#. Simply by choosing #ccT_1# to be a product of multiple theories, we can generalize this result to an arbitrary number of parameters. What makes this result liberating is that we just don’t need to worry about the parameters, they will automatically transform homomorphically. As a technical warning though, since two interpretations may have the same action on sorts but a different action on function symbols, if the family #X_A# was derived from an interpretation #ccJ#, i.e. #X_A -= ⟦A⟧^ccJ#, it may <em>not</em> be the case that #ccX = ccJ#.</p>
<p>Let’s look at some examples.</p>
<p>As a not-so-special case of representability, we can consider <code class="asciimath">#ccI -= ccK(tt "inl"(-), tt "inr"(Z))#</code> where #ccK : ccT_1 ↛ ccT_2#. Saying that #A# represents #ccI# in this case is saying that <em>bridging</em> terms of sort <code class="asciimath">#tt "inr"(Z)#</code>, i.e. sort #Z# in #ccT_2#, in #ccK#, correspond to terms of sort #A# in #ccT_1#. We’ll call the universal element of this representation the <strong>universal bridge</strong> (though technically it may be a bridging term, not a bridge). Let’s write #varepsilon# for this universal bridge. What representability states in this case is given <em>any</em> bridging term #k# of sort #Z#, there exists a unique term #|~ k ~|# of sort #A# such that #k = varepsilon(|~ k ~|)#. If we have an interpretation #ccX : ccT_2 -&gt; ccT_1# such that #⟦Z⟧^ccX# represents <code class="asciimath">#ccK(tt "inl"(-), tt "inr"(Z))#</code> for each sort #Z# of #ccT_2# we say we have a <strong>right representation</strong> of #ccK#. Note, that the universal bridges become a family <code class="asciimath">#varepsilon_Z : ⟦Z⟧^ccX -&gt; Z#</code>. Similarly, if <code class="asciimath">#ccK(tt "inl"(A), tt "inr"(-))#</code> is co-representable for each #A#, we say we have a <strong>left representation</strong> of #ccK#. The <strong>co-universal bridge</strong> is then a bridging term #eta_A : A -&gt; ⟦A⟧# such that for any bridging term #k# with source #A#, there exists a unique term <code class="asciimath">#|__ k __|#</code> in #ccT_2# such that <code class="asciimath">#k = |__ k __|(eta_A)#</code>. For reference, we’ll call these equations <strong>universal properties</strong> of the left/right representation. Parameterized representability implies that a left/right representation is essentially unique.</p>
<center>
<img src="/posts/raw/left-rep.svg" alt="" width="20%"></img> <img src="/posts/raw/right-rep.svg" alt="" width="20%"></img>
</center>
<p>Define #ccI_bb1# via #⟦A⟧^(ccI_bb1) -= bb1# where #bb1# is some one element set. #⟦ttf⟧^(ccI_bb1)# is the identity function for all function symbols #ttf#. We’ll say a theory #ccT# <strong>has a unit sort</strong> or <strong>has a terminal sort</strong> if there is a sort that we’ll also call #bb1# that represents #ccI_bb1#. Spelling out what that means, we first note that there is nothing notable about the universal element as it’s the only element. However, writing the homomorphism #! : ccI_bb1 -&gt; ccT(-, bb1)# and noting that since there’s only one element of #⟦A⟧^(ccI_bb1)# we can, with a slight abuse of notation, also write the term #!# picks out as #!# which gives the equation:</p>
<blockquote>
<p><code class="asciimath">#!_B(tt "g"(t)) = !_A(t)#</code> for any function symbol #ttg : A -&gt; B# and term, #t#, of sort #A#, note <code class="asciimath">#!_A : A -&gt; bb1#</code>.</p>
</blockquote>
<p>This equation states what the isomorphism also fairly directly states: there is exactly one term of sort #bb1# from any sort #A#, namely <code class="asciimath">#!_A(bbx_A)#</code>. The dual notion is called a <strong>void sort</strong> or an <strong>initial sort</strong> and will usually be notated #bb0#, the analog of #!# will be written as #0#. The resulting equation is:</p>
<blockquote>
<p><code class="asciimath">#tt "f"(0_A) = 0_B#</code> for any function symbol #ttf : A -&gt; B#, note #0_A : bb0 -&gt; A#.</p>
</blockquote>
<p>For the next example, I’ll leverage collages. Consider the collage #ccK_2 : ccT ↛ ccT xx ccT# whose bridges from #A -&gt; (B, C)# consist of pairs of terms #t_1 : A -&gt; B# and #t_2 : A -&gt; C#. #ccT# <strong>has pairs</strong> if #ccK_2# has a right representation. We’ll write #(B, C) |-&gt; B xx C# for the representing interpretation’s action on sorts. We’ll write the universal bridge as <code class="asciimath">#(tt "fst"(bbx_(B xx C)), tt "snd"(bbx_(B xx C)))#</code>. The universal property then looks like <code class="asciimath">#(tt "fst"(bbx_(B xx C)), tt "snd"(bbx_(B xx C)))((: t_1, t_2 :)) = (t_1, t_2)#</code> where #(: t_1, t_2 :) : A -&gt; B xx C# is the unique term induced by the bridge #(t_1, t_2)#. The universal property implies the following equations:</p>
<blockquote>
<ul>
<li><code class="asciimath">#(: tt "fst"(bbx_(B xx C)), tt "snd"(bbx_(B xx C))) = bbx_(B xx C)#</code></li>
<li><code class="asciimath">#tt "fst"((: t_1, t_2 :)) = t_1#</code></li>
<li><code class="asciimath">#tt "snd"((: t_1, t_2 :)) = t_2#</code></li>
</ul>
</blockquote>
<center>
<img src="/posts/raw/right-rep-pairs.svg" alt="" width="20%"></img>
</center>
<p>One aspect of note is regardless of whether #ccK_2# has a right representation, i.e. regardless of whether #ccT# has pairs, it always has a left representation. The co-universal bridge is #(bbx_A, bbx_A)# and the unique term <code class="asciimath">#|__(t_1, t_2)__|#</code> is <code class="asciimath">#tt "inl"(t_1, bbx_A)(tt "inr"(bbx_A, t_2)(bbx_("("A,A")")))#</code>.</p>
<center>
<img src="/posts/raw/left-rep-pairs.svg" alt="" width="20%"></img>
</center>
<p>Define an interpretation #Delta : ccT -&gt; ccT xx ccT# so that #⟦A⟧^Delta -= (A,A)# and similarly for function symbols. #Delta# left represents #ccK_2#. If the interpretation #(B,C) |-&gt; B xx C# right represents #ccK_2#, then we say we have an adjunction between #Delta# and #(- xx =)#, written #Delta ⊣ (- xx =)#, and that #Delta# is left adjoint to #(- xx =)#, and conversely #(- xx =)# is right adjoint #Delta#.</p>
<center>
<img src="/posts/raw/left-right-rep-pairs.svg" alt="" width="20%"></img>
</center>
<p>More generally, whenever we have the situation <code class="asciimath">#ccT_1(⟦-⟧^(ccI_1), =) ~= ccT_2(-, ⟦=⟧^(ccI_2))#</code> we say that #ccI_1 : ccT_2 -&gt; ccT_1# is <strong>left adjoint</strong> to #ccI_2 : ccT_1 -&gt; ccT_2# or conversely that #ccI_2# is <strong>right adjoint</strong> to #ccI_1#. We call this arrangement an <strong>adjunction</strong> and write #ccI_1 ⊣ ccI_2#. Note that we will always have this situation if #ccI_1# left represents and #ccI_2# right represents the same collage. As we noted above, parameterized representability actually determines one adjoint given (its action on sorts and) the other adjoint. With this we can show that adjoints are unique up to isomorphism, that is, given two left adjoints to an interpretation, they must be isomorphic. Similarly for right adjoints. This means that stating something is a left or right adjoint to some other known interpretation essentially completely characterizes it. One issue with adjunctions is that they tend to be wholesale. Let’s say the pair sort #A xx B# existed but no other pair sorts existed, then the (no longer parameterized) representability approach would work just fine, but the adjunction would no longer exist.</p>
<p>Here’s a few of exercises using this. First, a moderately challenging one (until you catch the pattern): spell out the details to the left adjoint to #Delta#. We say a theory <strong>has sums</strong> and write those sums as #A + B# if #(- + =) ⊣ Delta#. Recast void and unit sorts using adjunctions and/or left/right representations. As a terminological note, we say a theory <strong>has finite products</strong> if it has unit sorts and pairs. Similarly, a theory <strong>has finite sums</strong> or <strong>has finite coproducts</strong> if it has void sorts and sums. An even more challenging exercise is the following: a theory <strong>has exponentials</strong> if it has pairs and for every sort #A#, #(A xx -) ⊣ (A =&gt; -)# (note, parameterized representability applies to #A#). Spell out the equations characterizing #A =&gt; B#.</p>
<h2 id="finite-product-theories">Finite Product Theories</h2>
<p>Finite products start to lift us off the ground. So far the theories we’ve been working with have been extremely basic: a language with only unary functions, all terms being just a sequence of applications of function symbols. It shouldn’t be underestimated though. It’s more than enough to do monoid and group theory. A good amount of graph theory can be done with just this. And obviously we were able to establish several general results assuming only this structure. Nevertheless, while we can talk about specific groups, say, we can’t talk about the <em>theory</em> of groups. Finite products change this.</p>
<p>A theory with finite products allows us to talk about multi-ary function symbols and terms by considering unary function symbols from products. This allows us to do all of universal algebra. For example, the <strong>theory of groups</strong>, <code class="asciimath">#ccT_(bb "Grp")#</code>, consists of a sort #S# and all it’s products which we’ll abbreviate as #S^n# with #S^0 -= bb1# and #S^(n+1) -= S xx S^n#. It has three function symbols #tte : bb1 -&gt; S#, #ttm : S^2 -&gt; S#, and #tti : S -&gt; S# plus the ones that having finite products requires. In fact, instead of just heaping an infinite number of sorts and function symbols into our theory — and we haven’t even gotten to equations — let’s define a compact set of data from which we can generate all this data.</p>
<p>A <strong>signature</strong>, #Sigma#, consists of a collection of sorts, #sigma#, a collection of <em>multi-ary</em> function symbols, and a collection of equations. Equations still remain pairs of terms, but we need to now extend our definition of terms for this context. A <strong>term (in a signature)</strong> is either a variable, <code class="asciimath">#bbx_i^[A_0,A_1,...,A_n]#</code> where #A_i# are sorts and #0 &lt;= i &lt;= n#, the operators <code class="asciimath">#tt "fst"#</code> or <code class="asciimath">#tt "snd"#</code> applied to a term, the unit term written #(::)^A# with sort #A#, a pair of terms written #(: t_1, t_2 :)#, or the (arity correct) application of a multi-ary function symbol to a series of terms, e.g. <code class="asciimath">#tt "f"(t_1, t_2, t_3)#</code>. As a Haskell data declaration, it might look like:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">data</span> <span class="dt">SigTerm</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="ot">=</span> <span class="dt">SigVar</span> [<span class="dt">Sort</span>] <span class="dt">Int</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>    <span class="op">|</span> <span class="dt">Fst</span> <span class="dt">SigTerm</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>    <span class="op">|</span> <span class="dt">Snd</span> <span class="dt">SigTerm</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>    <span class="op">|</span> <span class="dt">Unit</span> <span class="dt">Sort</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>    <span class="op">|</span> <span class="dt">Pair</span> <span class="dt">SigTerm</span> <span class="dt">SigTerm</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>    <span class="op">|</span> <span class="dt">SigApply</span> <span class="dt">FunctionSymbol</span> [<span class="dt">SigTerm</span>]</span></code></pre></div>
<p>At this point, sorting (i.e. typing) the terms is no longer trivial, though it is still pretty straightforward. Sorts are either #bb1#, or #A xx B# for sorts #A# and #B#, or a sort #A in sigma#. The source of function symbols or terms are <em>lists</em> of sorts.</p>
<blockquote>
<ul>
<li><code class="asciimath">#bbx_i^[A_0, A_1, ..., A_n] : [A_0, A_1, ..., A_n] -&gt; A_i#</code></li>
<li><code class="asciimath">#(::)^A : [A] -&gt; bb1#</code></li>
<li><code class="asciimath">#(: t_1, t_2 :) : bar S -&gt; T_1 xx T_2#</code> where #t_i : bar S -&gt; T_i#</li>
<li><code class="asciimath">#tt "fst"(t) : bar S -&gt; T_1#</code> where #t : bar S -&gt; T_1 xx T_2#</li>
<li><code class="asciimath">#tt "snd"(t) : bar S -&gt; T_2#</code> where #t : bar S -&gt; T_1 xx T_2#</li>
<li><code class="asciimath">#tt "f"(t_1, ..., t_n) : bar S -&gt; T#</code> where #t_i : bar S -&gt; T_i# and <code class="asciimath">#ttf : [T_1,...,T_n] -&gt; T#</code></li>
</ul>
</blockquote>
<p>The point of a signature was to represent a theory so we can compile a term of a signature into a term of a theory with finite products. The <strong>theory generated from a signature</strong> #Sigma# has the same sorts as #Sigma#. The equations will be equations of #Sigma#, with the terms compiled as will be described momentarily, plus for every pair of sorts the equations that describe pairs and the equations for #!#. Finally, we need to describe how to take a term of the signature and make a function symbol of the theory, but before we do that we need to explain how to convert those sources of the terms which are lists. That’s just a conversion to right nested pairs, <code class="asciimath">#[A_0,...,A_n] |-&gt; A_0 xx (... xx (A_n xx bb1) ... )#</code>. The compilation of a term #t#, which we’ll write as <code class="asciimath">#ccC[t]#</code>, is defined as follows:</p>
<blockquote>
<ul>
<li><code class="asciimath">#ccC[bbx_i^[A_0, A_1, ..., A_n]] = tt "snd"^i(tt "fst"(bbx_(A_i xx(...))))#</code> where <code class="asciimath">#tt "snd"^i#</code> means the #i#-fold application of <code class="asciimath">#tt "snd"#</code></li>
<li><code class="asciimath">#ccC[(::)^A] = !_A#</code></li>
<li><code class="asciimath">#ccC[(: t_1, t_2 :)] = (: ccC[t_1], ccC[t_2] :)#</code></li>
<li><code class="asciimath">#ccC[tt "fst"(t)] = tt "fst"(ccC[t])#</code></li>
<li><code class="asciimath">#ccC[tt "snd"(t)] = tt "snd"(ccC[t])#</code></li>
<li><code class="asciimath">#ccC[tt "f"(t_1, ..., t_n)] = tt "f"((: ccC[t_1], (: ... , (: ccC[t_n], ! :) ... :) :))#</code></li>
</ul>
</blockquote>
<p>As you may have noticed, the generated theory will have an infinite number of sorts, an infinite number of function symbols, and an infinite number of equations no matter what the signature is — even an empty one! Having an infinite number of things isn’t a problem as long as we can algorithmically describe them and this is what the signature provides. Of course, if you’re a (typical) mathematician you nominally don’t care about an algorithmic description. Besides being compact, signatures present a nicer term language. The theories are like a core or assembly language. We could define a slightly nicer variation where we keep a context and manage named variables leading to terms-in-context like:</p>
<blockquote>
<p><code class="asciimath">#x:A, y:B |-- tt "f"(x, x, y)#</code></p>
</blockquote>
<p>which is</p>
<blockquote>
<p><code class="asciimath">#tt "f"(bbx_0^[A,B], bbx_0^[A,B], bbx_1^[A,B])#</code></p>
</blockquote>
<p>for our current term language for signatures. Of course, compilation will be (slightly) trickier for the nicer language.</p>
<p>The benefit of having compiled the signature to a theory, in addition to being able to reuse the results we’ve established for theories, is we only need to define operations on the theory, which is simpler since we only need to deal with pairs and unary function symbols. One example of this is we’d like to extend our notion of interpretation to one that respects the structure of the signature, and we can do that by defining an interpretation of theories that respects finite products.</p>
<p>A <strong>finite product preserving interpretation (into a finite product theory)</strong>, #ccI#, is an interpretation (into a finite product theory) that additionally satisfies:</p>
<blockquote>
<ul>
<li><code class="asciimath">#⟦bb1⟧^ccI ~~ bb1#</code></li>
<li><code class="asciimath">#⟦A xx B⟧^ccI ~~ ⟦A⟧^ccI xx ⟦B⟧^ccI#</code></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><code class="asciimath">#⟦!_A⟧^ccI = !_(⟦A⟧^ccI)#</code></li>
<li><code class="asciimath">#⟦tt "fst"(t)⟧^ccI = tt "fst"(⟦t⟧^ccI)#</code></li>
<li><code class="asciimath">#⟦tt "snd"(t)⟧^ccI = tt "snd"(⟦t⟧^ccI)#</code></li>
<li><code class="asciimath">#⟦(: t_1, t_2 :)⟧^ccI = (: ⟦t_1⟧^ccI, ⟦t_2⟧^ccI :)#</code></li>
</ul>
</blockquote>
<p>where, for <code class="asciimath">#bb "Set"#</code>, #bb1 -= {{}}#, #xx# is the cartesian product, <code class="asciimath">#tt "fst"#</code> and <code class="asciimath">#tt "snd"#</code> are the projections, <code class="asciimath">#!_A -= x |-&gt; \{\}#</code>, and #(: f, g :) -= x |-&gt; (: f(x), g(x) :)#.</p>
<p>With signatures, we can return to our theory, now signature, of groups. <code class="asciimath">#Sigma_bb "Grp"#</code> has a single sort #S#, three function symbols <code class="asciimath">#tte : [bb1] -&gt; S#</code>, <code class="asciimath">#tti : [S] -&gt; S#</code>, and <code class="asciimath">#ttm : [S, S] -&gt; S#</code>, with the following equations:</p>
<blockquote>
<ul>
<li><code class="asciimath">#tt "m"(tt "e"((::)^S), bbx_0^S) ~~ bbx_0^S#</code></li>
<li><code class="asciimath">#tt "m"(tt "i"(bbx_0^S), bbx_0^S) ~~ tt "e"((::)^S)#</code></li>
<li><code class="asciimath">#tt "m"(tt "m"(bbx_0^[S,S,S], bbx_1^[S,S,S]), bbx_2^[S,S,S]) ~~ tt "m"(bbx_0^[S,S,S], tt "m"(bbx_1^[S,S,S], bbx_2^[S,S,S]))#</code></li>
</ul>
</blockquote>
<p>or using the nicer syntax:</p>
<blockquote>
<ul>
<li><code class="asciimath">#x:S |-- tt "m"(tt "e"(), x) ~~ x#</code></li>
<li><code class="asciimath">#x:S |-- tt "m"(tt "i"(x), x) ~~ tt "e"()#</code></li>
<li><code class="asciimath">#x:S, y:S, z:S |-- tt "m"(tt "m"(x, y), z) ~~ tt "m"(x, tt "m"(y, z))#</code></li>
</ul>
</blockquote>
<p>An actual group is then just a finite product preserving interpretation of (the theory generated by) this signature. All of universal algebra and much of abstract algebra can be formulated this way.</p>
<h2 id="the-simply-typed-lambda-calculus-and-beyond">The Simply Typed Lambda Calculus and Beyond</h2>
<p>We can consider additionally assuming that our theory has exponentials. I left articulating exactly what that means as an exercise, but the upshot is we have the following two operations:</p>
<p>For any term #t : A xx B -&gt; C#, we have the term <code class="asciimath">#tt "curry"(t) : A -&gt; C^B#</code>. We also have the homomorphism <code class="asciimath">#tt "app"_(AB) : B^A xx A -&gt; B#</code>. They satisfy:</p>
<blockquote>
<ul>
<li><code class="asciimath">#tt "curry"(tt "app"(bbx_(B^A xx A))) = bbx_(B^A)#</code></li>
<li><code class="asciimath">#tt "app"((: tt "curry"(t_1), t_2 :)) = t_1((: bbx_A, t_2 :))#</code> where #t_1 : A xx B -&gt; C# and #t_2 : A -&gt; B#.</li>
</ul>
</blockquote>
<p>We can view these, together with the the product operations, as combinators, and it turns out we can compile the simply typed lambda calculus into the above theory. This is exactly what the <a href="https://en.wikipedia.org/wiki/Categorical_abstract_machine">Categorical Abstract Machine</a> did. The “Caml” in “O’Caml” stands for “Categorical Abstract Machine Language”, though O’Caml no longer uses the CAM. Conversely, every term of the theory can be expressed as a simply typed lambda term. This means we can view the simply typed lambda calculus as just a different presentation of the theory.</p>
<p>At this point, this presentation of category theory starts to connect to the mainstream categorical literature on <a href="https://ncatlab.org/nlab/show/algebraic+theory">universal algebra</a>, <a href="https://ncatlab.org/nlab/show/type+theory#TheInternalLanguageOfACategory">internal languages</a>, <a href="https://ncatlab.org/nlab/show/sketch">sketches</a>, and <a href="https://ncatlab.org/nlab/show/internal+logic">internal logic</a>. This page gives a synopsis of the <a href="https://ncatlab.org/nlab/show/relation+between+type+theory+and+category+theory">relationship between type theory and category theory</a>. For some reason, it is unusual to talk about the internal language of a plain category, but that is exactly what we’ve done here.</p>
<p>I haven’t talked about finite limits or colimits beyond products and coproducts, nor have I talked about even the infinitary versions of products and coproducts, let alone arbitrary limits and colimits. These can be handled the same way as products and coproducts. Formulating a language like signatures or the simply typed lambda calculus is a bit more complicated, but not that hard. I may make a follow-up article covering this among other things. I also have a side project (don’t hold your breath), that implements the internal language of a category with finite limits. The result looks roughly like a simple version of an algebraic specification language like the <a href="https://cseweb.ucsd.edu/~goguen/sys/obj.html">OBJ family</a>. The <code>RING</code> theory described in the <a href="http://maude.lcc.uma.es/manual271/maude-manualch6.html#x41-880006.3">Maude manual</a> gives an idea of what it would look like. In fact, here’s an example of the current actual syntax I’m using.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<pre><code>theory Categories
    type O
    type A
    given src : A -&gt; O
    given tgt : A -&gt; O

    given id : O -&gt; A
    satisfying o:O | src (id o) = o, tgt (id o) = o

    given c : { f:A, g:A | src f = tgt g } -&gt; A
    satisfying (f, g):{ f:A, g:A | src f = tgt g }
        | tgt (c (f, g)) = tgt f, src (c (f, g)) = src g
    satisfying &quot;left unit&quot; (o, f):{ o:O, f:A | tgt f = o }
        | c (id o, f) = f
    satisfying &quot;right unit&quot; (o, f):{ o:O, f:A | src f = o }
        | c (f, id o) = f
    satisfying &quot;associativity&quot; (f, g, h):{ f:A, g:A, h:A | src f = tgt g, src g = tgt h }
        | c (c (f, g), h) = c (f, c (g, h))
endtheory</code></pre>
<p>It turns out this is a particularly interesting spot in the design space. The fact that the theory of theories with finite limits is itself a theory with finite limits has interesting consequences. It is still relatively weak though. For example, it’s not possible to describe the theory of fields in this language.</p>
<p>There are other directions one could go. For example, the internal logic of monoidal categories is (a fragment of) ordered linear logic. You can cross this bridge either way. You can look at different languages and consider what categorical structure is needed to support the features of the language, or you can add features to the category and see how that impacts the internal language. The relationship is similar to the source language and a core/intermediate language in a compiler, e.g. GHC Haskell and <a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/FC">System Fc</a>.</p>
<h2 id="decoder">Decoder</h2>
<p>If you’ve looked at category theory at all, you can probably make most of the connections without me telling you. The table below outlines the mapping, but there are some subtleties. First, as a somewhat technical detail, my definition of a theory corresponds to a <em>small</em> category, i.e. a category which has a set of objects and a set of arrows. For more programmer types, you should think of “set” as <code>Set</code> in Agda, i.e. similar to the <code>*</code> kind in Haskell. Usually “category” means “locally small category” which may have a proper class of objects and between any two objects a set of arrows (though the union of all those sets may be a proper class). Again, for programmers, the distinction between “class” and “set” is basically the difference between <code>Set</code> and <code>Set1</code> in Agda.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> To make my definition of theory closer to this, all that is necessary is instead of having a set of function symbols, have a family of sets indexed by pairs of objects. Here’s what a partial definition in Agda of the two scenarios would look like:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode agda"><code class="sourceCode agda"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">-- Small category (the definition I used)</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="kw">record</span> SmallCategory <span class="ot">:</span> <span class="dt">Set1</span> <span class="kw">where</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>    <span class="kw">field</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>        objects <span class="ot">:</span> <span class="dt">Set</span></span>
<span id="cb5-5"><a href="#cb5-5"></a>        arrows <span class="ot">:</span> <span class="dt">Set</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>        src <span class="ot">:</span> arrows <span class="ot">-&gt;</span> objects</span>
<span id="cb5-7"><a href="#cb5-7"></a>        tgt <span class="ot">:</span> arrows <span class="ot">-&gt;</span> objects</span>
<span id="cb5-8"><a href="#cb5-8"></a>        <span class="ot">...</span></span>
<span id="cb5-9"><a href="#cb5-9"></a></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="co">-- Locally small category</span></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="kw">record</span> LocallySmallCategory <span class="ot">:</span> <span class="dt">Set2</span> <span class="kw">where</span></span>
<span id="cb5-12"><a href="#cb5-12"></a>    <span class="kw">field</span></span>
<span id="cb5-13"><a href="#cb5-13"></a>        objects <span class="ot">:</span> <span class="dt">Set1</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>        hom <span class="ot">:</span> objects <span class="ot">-&gt;</span> objects <span class="ot">-&gt;</span> <span class="dt">Set</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>        <span class="ot">...</span></span>
<span id="cb5-16"><a href="#cb5-16"></a></span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="co">-- Different presentation of a small category</span></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="kw">record</span> SmallCategory&#39; <span class="ot">:</span> <span class="dt">Set1</span> <span class="kw">where</span></span>
<span id="cb5-19"><a href="#cb5-19"></a>    <span class="kw">field</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>        objects <span class="ot">:</span> <span class="dt">Set</span></span>
<span id="cb5-21"><a href="#cb5-21"></a>        hom <span class="ot">:</span> objects <span class="ot">-&gt;</span> objects <span class="ot">-&gt;</span> <span class="dt">Set</span></span>
<span id="cb5-22"><a href="#cb5-22"></a>        <span class="ot">...</span></span></code></pre></div>
<p>The benefit of the notion of locally small category is that <code>Set</code> itself is a locally small category. The distinction I was making between interpretations into theories and interpretations into <strong>Set</strong> was due to the fact that <strong>Set</strong> wasn’t a theory. If I used a definition theory corresponding to a locally small category, I could have combined the notions of interpretation by making <strong>Set</strong> a theory. The notion of a small category, though, is still useful. Also, an interpretation into <strong>Set</strong> corresponds to the usual notion of a model or semantics, while interpretations into other theories was a less emphasized concept in traditional model theory and universal algebra.</p>
<p>A less technical and more significant difference is that my definition of a theory doesn’t correspond to a category, but rather to a <em>presentation</em> of a category, from which a category can be generated. The analog of arrows in a category is <em>terms</em>, not function symbols. This is a bit more natural route from the model theory/universal algebra/programming side. Similarly, having an explicit collection of equations, rather than just an equivalence relation on terms is part of the presentation of the category but not part of the category itself.</p>
<table class="table table-striped table-sm">
<thead>
<tr>
<th>
model theory
</th>
<th>
category theory
</th>
</thead>
<tbody>
<tr>
<td>
sort
</td>
<td>
object
</td>
</tr>
<tr>
<td>
term
</td>
<td>
arrow
</td>
</tr>
<tr>
<td>
function symbol
</td>
<td>
generating arrow
</td>
</tr>
<tr>
<td>
theory
</td>
<td>
presentation of a (small) category
</td>
</tr>
<tr>
<td>
collage
</td>
<td>
collage, <a href="https://ncatlab.org/nlab/show/cograph+of+a+profunctor">cograph of a profunctor</a>
</td>
</tr>
<tr>
<td>
bridge
</td>
<td>
<a href="https://ncatlab.org/nlab/show/heteromorphism">heteromorphism</a>
</td>
</tr>
<tr>
<td>
signature
</td>
<td>
presentation of a (small) category with finite products
</td>
</tr>
<tr>
<td>
interpretation into sets, aka models
</td>
<td>
a functor into <strong>Set</strong>, a (co)presheaf
</td>
</tr>
<tr>
<td>
interpretation into a theory
</td>
<td>
functor
</td>
</tr>
<tr>
<td>
homomorphism
</td>
<td>
natural transformation
</td>
</tr>
<tr>
<td>
simply typed lambda calculus (with products)
</td>
<td>
a cartesian closed category
</td>
</tr>
</tbody>
</table>
<h2 id="conclusion">Conclusion</h2>
<p>In some ways I’ve stopped just when things were about to get good. I may do a follow-up to elaborate on this good stuff. Some examples are: if I expand the definition so that <strong>Set</strong> becomes a “theory”, then interpretations also form such a “theory”, and these are often what we’re really interested in. The category of finite-product preserving interpretations of the theory of groups essentially <em>is</em> the category of groups. In fact, universal algebra is, in categorical terms, just the study of categories with finite products and finite-product preserving functors from them, particularly into <strong>Set</strong>. It’s easy to generalize this in many directions. It’s also easy to make very general definitions, like a general definition of a free algebraic structure. In general, we’re usually more interested in the interpretations of a theory than the theory itself.</p>
<p>While I often do advocate thinking in terms of internal languages of categories, I’m not sure that it is a preferable perspective for the very basics of category theory. Nevertheless, there are a few reasons for why I wrote this. First, this very syntactical approach is, I think, more accessible to someone coming from a programming background. From this view, a category is a very simple programming language. Adding structure to the category corresponds to adding features to this programming language. Interpretations are denotational semantics.</p>
<p>Another aspect about this presentation that is quite different is the use and emphasis on collages. Collages correspond to profunctors, a crucially important and enabling concept that is rarely covered in categorical introductions. The characterization of profunctors as collages in Vaughn Pratt’s paper (not using that name) was one of the things I enjoyed about that paper and part of what prompted me to start writing this. In earlier drafts of this article, I was unable to incorporate collages in a meaningful way as I was trying to start from profunctors. This approach just didn’t add value. Collages just looked like a bizarre curio and weren’t integrated into the narrative at all. For other reasons, though, I ended up revisiting the idea of a heteromorphism. My (fairly superficial) opinion is that once you have the notion of functors and natural transformations, adding the notion of heteromorphisms has a low power-to-weight ratio, though it does make some things a bit nicer. Nevertheless, in thinking of how best to fit them into this context, it was clear that collages provided the perfect mechanism (which isn’t a big surprise), and the result works rather nicely. When I realized a fact that can be cryptically but compactly represented as #ccK_ccT ≃ bbbI xx ccT# where #bbbI# is the interval category, i.e. two objects with a single arrow joining them, I realized that this is actually an interesting perspective. Since most of this article was written at that point, I wove collages into the narrative replacing some things. If, though, I had started with this perspective from the beginning I suspect I would have made a significantly different article, though the latter sections would likely be similar.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>It’s actually better to organize this as a <em>family</em> of collections of function symbols indexed by pairs of sorts.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Instead of having equations that generate an equivalence relation on (raw) terms, we could simply require an equivalence relation on (raw) terms be directly provided.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Collaging is actually quite natural in this context. I already intend to support one theory importing another. A collage is just a theory that imports two others and then adds function symbols between them.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>For programmers familiar with Agda, at least, if you haven’t made this connection, this might help you understand and appreciate what a “class” is versus a “set” and what “size issues” are, which is typically handled extremely vaguely in a lot of the literature.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Constant-time Binary Logarithm</title>
    <link href="https://derekelkins.github.io/posts/find-first-set.html" />
    <id>https://derekelkins.github.io/posts/find-first-set.html</id>
    <published>2016-11-09 23:48:33-08:00</published>
    <updated>2016-11-10T07:48:33Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I’ve been watching the <a href="https://courses.csail.mit.edu/6.851/spring14/lectures/">Spring 2012 lectures for MIT 6.851 Advanced Data Structures</a> with Prof. Erik Demaine. In lecture 12, “Fusion Trees”, it mentions a constant time algorithm for finding the index of the first most significant 1 bit in a word, i.e. the binary logarithm. Assuming word operations are constant time, i.e. in the Word RAM model, the below algorithm takes 27 word operations (not counting copying). When I compiled it with GHC 8.0.1 -O2 the core of the algorithm was 44 straight-line instructions. The theoretically interesting thing is, other than changing the constants, the same algorithm works for any word size that’s an even power of 2. Odd powers of two need a slight tweak. This is demonstrated for <code>Word64</code>, <code>Word32</code>, and <code>Word16</code>. It should be possible to do this for any arbitrary word size <code>w</code>.</p>
<p>The <code>clz</code> instruction <a href="https://hackage.haskell.org/package/base-4.8.2.0/docs/Data-Bits.html#v:countLeadingZeros">can be used to implement</a> this function, but this is a potential simulation if that or a similar instruction wasn’t available. It’s probably not the fastest way. Similarly, find first set and count trailing zeros <a href="https://en.wikipedia.org/wiki/Find_first_set">can be implemented</a> in terms of this operation.</p>
<h2 id="code">Code</h2>
<p>Below is the complete code. You can also download it <a href="/posts/raw/FFS.hs">here</a>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1"></a><span class="ot">{-# LANGUAGE BangPatterns #-}</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="kw">import</span> <span class="dt">Data.Word</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="kw">import</span> <span class="dt">Data.Bits</span></span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co">-- Returns 0-based bit index of most significant bit that is 1. Assumes input is non-zero.</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">-- That is, 2^indexOfMostSignificant1 x &lt;= x &lt; 2^(indexOfMostSignificant1 x + 1)</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">-- From Erik Demaine&#39;s presentation in Spring 2012 lectures of MIT 6.851, particularly &quot;Lecture 12: Fusion Trees&quot;.</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">-- Takes 26 (source-level) straight-line word operations.</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="ot">indexOfMostSignificant1 ::</span> <span class="dt">Word64</span> <span class="ot">-&gt;</span> <span class="dt">Word64</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>indexOfMostSignificant1 w <span class="ot">=</span> idxMsbyte <span class="op">.|.</span> idxMsbit</span>
<span id="cb1-11"><a href="#cb1-11"></a>    <span class="kw">where</span></span>
<span id="cb1-12"><a href="#cb1-12"></a>        <span class="co">-- top bits of each byte</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>        <span class="op">!</span>wtbs <span class="ot">=</span> w <span class="op">.&amp;.</span> <span class="bn">0x8080808080808080</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>        </span>
<span id="cb1-15"><a href="#cb1-15"></a>        <span class="co">-- all but top bits of each byte producing 8 7-bit chunks</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>        <span class="op">!</span>wbbs <span class="ot">=</span> w <span class="op">.&amp;.</span> <span class="bn">0x7F7F7F7F7F7F7F7F</span>              </span>
<span id="cb1-17"><a href="#cb1-17"></a></span>
<span id="cb1-18"><a href="#cb1-18"></a>        <span class="co">-- parallel compare of each 7-bit chunk to 0, top bit set in result if 7-bit chunk was not 0</span></span>
<span id="cb1-19"><a href="#cb1-19"></a>        <span class="op">!</span>pc <span class="ot">=</span> parallelCompare <span class="bn">0x8080808080808080</span> wbbs</span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a>        <span class="co">-- top bit of each byte set if the byte has any bits set in w</span></span>
<span id="cb1-22"><a href="#cb1-22"></a>        <span class="op">!</span>ne <span class="ot">=</span> wtbs <span class="op">.|.</span> pc                             </span>
<span id="cb1-23"><a href="#cb1-23"></a></span>
<span id="cb1-24"><a href="#cb1-24"></a>        <span class="co">-- a summary of which bytes (except the first) are non-zero as a 7-bit bitfield, i.e. top bits collected into bottom byte</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>        <span class="op">!</span>summary <span class="ot">=</span> sketch ne <span class="ot">`unsafeShiftR`</span> <span class="dv">1</span></span>
<span id="cb1-26"><a href="#cb1-26"></a></span>
<span id="cb1-27"><a href="#cb1-27"></a>        <span class="co">-- parallel compare summary to powers of two</span></span>
<span id="cb1-28"><a href="#cb1-28"></a>        <span class="op">!</span>cmpp2 <span class="ot">=</span> parallelCompare <span class="bn">0xFFBF9F8F87838180</span> (<span class="bn">0x0101010101010101</span> <span class="op">*</span> summary)</span>
<span id="cb1-29"><a href="#cb1-29"></a>        </span>
<span id="cb1-30"><a href="#cb1-30"></a>        <span class="co">-- index of most significant non-zero byte * 8</span></span>
<span id="cb1-31"><a href="#cb1-31"></a>        <span class="op">!</span>idxMsbyte <span class="ot">=</span> sumTopBits8 cmpp2                </span>
<span id="cb1-32"><a href="#cb1-32"></a></span>
<span id="cb1-33"><a href="#cb1-33"></a>        <span class="co">-- most significant 7-bits of most significant non-zero byte</span></span>
<span id="cb1-34"><a href="#cb1-34"></a>        <span class="op">!</span>msbyte <span class="ot">=</span> ((w <span class="ot">`unsafeShiftR`</span> (<span class="fu">fromIntegral</span> idxMsbyte)) <span class="op">.&amp;.</span> <span class="bn">0xFF</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">1</span></span>
<span id="cb1-35"><a href="#cb1-35"></a></span>
<span id="cb1-36"><a href="#cb1-36"></a>        <span class="co">-- parallel compare msbyte to powers of two</span></span>
<span id="cb1-37"><a href="#cb1-37"></a>        <span class="op">!</span>cmpp2&#39; <span class="ot">=</span> parallelCompare <span class="bn">0xFFBF9F8F87838180</span> (<span class="bn">0x0101010101010101</span> <span class="op">*</span> msbyte)</span>
<span id="cb1-38"><a href="#cb1-38"></a></span>
<span id="cb1-39"><a href="#cb1-39"></a>        <span class="co">-- index of most significant non-zero bit in msbyte</span></span>
<span id="cb1-40"><a href="#cb1-40"></a>        <span class="op">!</span>idxMsbit <span class="ot">=</span> sumTopBits cmpp2&#39; </span>
<span id="cb1-41"><a href="#cb1-41"></a></span>
<span id="cb1-42"><a href="#cb1-42"></a>        <span class="co">-- Maps top bits of each byte into lower byte assuming all other bits are 0.</span></span>
<span id="cb1-43"><a href="#cb1-43"></a>        <span class="co">-- 0x2040810204081 = sum [2^j | j &lt;- map (\i -&gt; 49 - 7*i) [0..7]]</span></span>
<span id="cb1-44"><a href="#cb1-44"></a>        <span class="co">-- In general if w = 2^(2*k+p) and p = 0 or 1 the formula is:</span></span>
<span id="cb1-45"><a href="#cb1-45"></a>        <span class="co">-- sum [2^j | j &lt;- map (\i -&gt; w-(2^k-1) - 2^(k+p) - (2^(k+p) - 1)*i) [0..2^k-1]]</span></span>
<span id="cb1-46"><a href="#cb1-46"></a>        <span class="co">-- Followed by shifting right by w - 2^k</span></span>
<span id="cb1-47"><a href="#cb1-47"></a>        sketch w <span class="ot">=</span> (w <span class="op">*</span> <span class="bn">0x2040810204081</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">56</span></span>
<span id="cb1-48"><a href="#cb1-48"></a></span>
<span id="cb1-49"><a href="#cb1-49"></a>        parallelCompare w1 w2 <span class="ot">=</span> complement (w1 <span class="op">-</span> w2) <span class="op">.&amp;.</span> <span class="bn">0x8080808080808080</span></span>
<span id="cb1-50"><a href="#cb1-50"></a>        sumTopBits w <span class="ot">=</span> ((w <span class="ot">`unsafeShiftR`</span> <span class="dv">7</span>) <span class="op">*</span> <span class="bn">0x0101010101010101</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">56</span></span>
<span id="cb1-51"><a href="#cb1-51"></a>        sumTopBits8 w <span class="ot">=</span> ((w <span class="ot">`unsafeShiftR`</span> <span class="dv">7</span>) <span class="op">*</span> <span class="bn">0x0808080808080808</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">56</span></span>
<span id="cb1-52"><a href="#cb1-52"></a></span>
<span id="cb1-53"><a href="#cb1-53"></a><span class="ot">indexOfMostSignificant1_w32 ::</span> <span class="dt">Word32</span> <span class="ot">-&gt;</span> <span class="dt">Word32</span></span>
<span id="cb1-54"><a href="#cb1-54"></a>indexOfMostSignificant1_w32 w <span class="ot">=</span> idxMsbyte <span class="op">.|.</span> idxMsbit</span>
<span id="cb1-55"><a href="#cb1-55"></a>    <span class="kw">where</span> <span class="op">!</span>wtbs <span class="ot">=</span> w <span class="op">.&amp;.</span> <span class="bn">0x80808080</span></span>
<span id="cb1-56"><a href="#cb1-56"></a>          <span class="op">!</span>wbbs <span class="ot">=</span> w <span class="op">.&amp;.</span> <span class="bn">0x7F7F7F7F</span></span>
<span id="cb1-57"><a href="#cb1-57"></a>          <span class="op">!</span>pc <span class="ot">=</span> parallelCompare <span class="bn">0x80808080</span> wbbs</span>
<span id="cb1-58"><a href="#cb1-58"></a>          <span class="op">!</span>ne <span class="ot">=</span> wtbs <span class="op">.|.</span> pc</span>
<span id="cb1-59"><a href="#cb1-59"></a>          <span class="op">!</span>summary <span class="ot">=</span> sketch ne <span class="ot">`unsafeShiftR`</span> <span class="dv">1</span></span>
<span id="cb1-60"><a href="#cb1-60"></a>          <span class="op">!</span>cmpp2 <span class="ot">=</span> parallelCompare <span class="bn">0xFF838180</span> (<span class="bn">0x01010101</span> <span class="op">*</span> summary)</span>
<span id="cb1-61"><a href="#cb1-61"></a>          <span class="op">!</span>idxMsbyte <span class="ot">=</span> sumTopBits8 cmpp2</span>
<span id="cb1-62"><a href="#cb1-62"></a>          <span class="op">!</span>msbyte <span class="ot">=</span> ((w <span class="ot">`unsafeShiftR`</span> (<span class="fu">fromIntegral</span> idxMsbyte)) <span class="op">.&amp;.</span> <span class="bn">0xFF</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">1</span></span>
<span id="cb1-63"><a href="#cb1-63"></a>          <span class="op">!</span>cmpp2&#39; <span class="ot">=</span> parallelCompare <span class="bn">0x87838180</span> (<span class="bn">0x01010101</span> <span class="op">*</span> msbyte)</span>
<span id="cb1-64"><a href="#cb1-64"></a></span>
<span id="cb1-65"><a href="#cb1-65"></a>          <span class="co">-- extra step when w is not an even power of two</span></span>
<span id="cb1-66"><a href="#cb1-66"></a>          <span class="op">!</span>cmpp2&#39;&#39; <span class="ot">=</span> parallelCompare <span class="bn">0xFFBF9F8F</span> (<span class="bn">0x01010101</span> <span class="op">*</span> msbyte)</span>
<span id="cb1-67"><a href="#cb1-67"></a>          <span class="op">!</span>idxMsbit <span class="ot">=</span> sumTopBits cmpp2&#39; <span class="op">+</span> sumTopBits cmpp2&#39;&#39;</span>
<span id="cb1-68"><a href="#cb1-68"></a></span>
<span id="cb1-69"><a href="#cb1-69"></a>          sketch w <span class="ot">=</span> (w <span class="op">*</span> <span class="bn">0x204081</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">28</span></span>
<span id="cb1-70"><a href="#cb1-70"></a>          parallelCompare w1 w2 <span class="ot">=</span> complement (w1 <span class="op">-</span> w2) <span class="op">.&amp;.</span> <span class="bn">0x80808080</span></span>
<span id="cb1-71"><a href="#cb1-71"></a>          sumTopBits w <span class="ot">=</span> ((w <span class="ot">`unsafeShiftR`</span> <span class="dv">7</span>) <span class="op">*</span> <span class="bn">0x01010101</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">24</span></span>
<span id="cb1-72"><a href="#cb1-72"></a>          sumTopBits8 w <span class="ot">=</span> ((w <span class="ot">`unsafeShiftR`</span> <span class="dv">7</span>) <span class="op">*</span> <span class="bn">0x08080808</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">24</span></span>
<span id="cb1-73"><a href="#cb1-73"></a></span>
<span id="cb1-74"><a href="#cb1-74"></a><span class="ot">indexOfMostSignificant1_w16 ::</span> <span class="dt">Word16</span> <span class="ot">-&gt;</span> <span class="dt">Word16</span></span>
<span id="cb1-75"><a href="#cb1-75"></a>indexOfMostSignificant1_w16 w <span class="ot">=</span> idxMsnibble <span class="op">.|.</span> idxMsbit</span>
<span id="cb1-76"><a href="#cb1-76"></a>    <span class="kw">where</span> <span class="op">!</span>wtbs <span class="ot">=</span> w <span class="op">.&amp;.</span> <span class="bn">0x8888</span></span>
<span id="cb1-77"><a href="#cb1-77"></a>          <span class="op">!</span>wbbs <span class="ot">=</span> w <span class="op">.&amp;.</span> <span class="bn">0x7777</span></span>
<span id="cb1-78"><a href="#cb1-78"></a>          <span class="op">!</span>pc <span class="ot">=</span> parallelCompare <span class="bn">0x8888</span> wbbs</span>
<span id="cb1-79"><a href="#cb1-79"></a>          <span class="op">!</span>ne <span class="ot">=</span> wtbs <span class="op">.|.</span> pc</span>
<span id="cb1-80"><a href="#cb1-80"></a>          <span class="op">!</span>summary <span class="ot">=</span> sketch ne <span class="ot">`unsafeShiftR`</span> <span class="dv">1</span></span>
<span id="cb1-81"><a href="#cb1-81"></a>          <span class="op">!</span>cmpp2 <span class="ot">=</span> parallelCompare <span class="bn">0xFB98</span> (<span class="bn">0x1111</span> <span class="op">*</span> summary)</span>
<span id="cb1-82"><a href="#cb1-82"></a>          <span class="op">!</span>idxMsnibble <span class="ot">=</span> sumTopBits4 cmpp2</span>
<span id="cb1-83"><a href="#cb1-83"></a>          <span class="op">!</span>msnibble <span class="ot">=</span> ((w <span class="ot">`unsafeShiftR`</span> (<span class="fu">fromIntegral</span> idxMsnibble)) <span class="op">.&amp;.</span> <span class="bn">0xF</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">1</span></span>
<span id="cb1-84"><a href="#cb1-84"></a>          <span class="op">!</span>cmpp2&#39; <span class="ot">=</span> parallelCompare <span class="bn">0xFB98</span> (<span class="bn">0x1111</span> <span class="op">*</span> msnibble)</span>
<span id="cb1-85"><a href="#cb1-85"></a>          <span class="op">!</span>idxMsbit <span class="ot">=</span> sumTopBits cmpp2&#39;</span>
<span id="cb1-86"><a href="#cb1-86"></a></span>
<span id="cb1-87"><a href="#cb1-87"></a>          sketch w <span class="ot">=</span> (w <span class="op">*</span> <span class="bn">0x249</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">12</span></span>
<span id="cb1-88"><a href="#cb1-88"></a>          parallelCompare w1 w2 <span class="ot">=</span> complement (w1 <span class="op">-</span> w2) <span class="op">.&amp;.</span> <span class="bn">0x8888</span></span>
<span id="cb1-89"><a href="#cb1-89"></a>          sumTopBits w <span class="ot">=</span> ((w <span class="ot">`unsafeShiftR`</span> <span class="dv">3</span>) <span class="op">*</span> <span class="bn">0x1111</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">12</span></span>
<span id="cb1-90"><a href="#cb1-90"></a>          sumTopBits4 w <span class="ot">=</span> ((w <span class="ot">`unsafeShiftR`</span> <span class="dv">3</span>) <span class="op">*</span> <span class="bn">0x4444</span>) <span class="ot">`unsafeShiftR`</span> <span class="dv">12</span></span></code></pre></div>]]></summary>
</entry>
<entry>
    <title>Quotient Types for Programmers</title>
    <link href="https://derekelkins.github.io/posts/quotient-types-for-programmers.html" />
    <id>https://derekelkins.github.io/posts/quotient-types-for-programmers.html</id>
    <published>2016-09-22 22:21:35-07:00</published>
    <updated>2016-09-23T05:21:35Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Programmers in typed languages with higher order functions and algebraic data types are already comfortable with most of the basic constructions of set/type theory. In categorical terms, those programmers are familiar with finite products and coproducts and (monoidal/cartesian) closed structure. The main omissions are subset types (equalizers/pullbacks) and quotient types (coequalizers/pushouts) which would round out limits and colimits. Not having a good grasp on either of these constructions dramatically shrinks the world of mathematics that is understandable, but while subset types are fairly straightforward, quotient types are quite a bit less intuitive.</p>
<p>See <a href="https://stackoverflow.com/questions/23596225/how-can-quotient-types-help-safely-expose-module-internals/39765173#39765173">this StackOverflow answer</a> by me which provides a more software engineering perspective.</p>
<h2 id="subset-types">Subset Types</h2>
<p>In my opinion, most programmers can more or less immediately understand the notion of a subset type at an intuitive level.<br />
A <strong>subset type</strong> is just a type combined with a predicate on that type that specifies which values of the type we want. For example, we may have something like <code>{ n:Nat | n /= 0 }</code> meaning the type of naturals not equal to #0#. We may use this in the type of the division function for the denominator. Consuming a value of a subset type is easy, a natural not equal to #0# is still just a natural, and we can treat it as such. The difficult part is producing a value of a subset type. To do this, we must, of course, produce a value of the underlying type — <code>Nat</code> in our example — but then we must further convince the type checker that the predicate holds (e.g. that the value does not equal #0#). Most languages provide no mechanism to prove potentially arbitrary facts about code, and this is why they do not support subset types. Dependently typed languages do provide such mechanisms and thus either have or can encode subset types. Outside of dependently typed languages the typical solution is to use an abstract data type and use a runtime check when values of that abstract data type are created.</p>
<h2 id="quotient-types">Quotient Types</h2>
<p>The dual of subset types are quotient types. My impression is that this construction is the most difficult basic construction for people to understand. Further, programmers aren’t much better off, because they have little to which to connect the idea. Before I give a definition, I want to provide the example with which most people are familiar: modular (or clock) arithmetic. A typical way this is first presented is as a system where the numbers “wrap-around”. For example, in arithmetic mod #3#, we count #0#, #1#, #2#, and then wrap back around to #0#. Programmers are well aware that it’s not necessary to guarantee that an input to addition, subtraction, or multiplication mod #3# is either #0#, #1#, or #2#. Instead, the operation can be done and the <code>mod</code> function can be applied at the end. This will give the same result as applying the <code>mod</code> function to each argument at the beginning. For example, #4+7 = 11# and #11 mod 3 = 2#, and #4 mod 3 = 1# and #7 mod 3 = 1# and #1+1 = 2 = 11 mod 3#.</p>
<p>For mathematicians, the type of integers mod #n# is represented by the quotient type #ZZ//n ZZ#. The idea is that the values of #ZZ // n ZZ# are integers except that we agree that any two integers #a# and #b# are treated as equal if #a - b = kn# for some integer #k#. For #ZZ // 3 ZZ#, #… -6 = -3 = 0 = 3 = 6 = …# and #… = -5 = -2 = 1 = 4 = 7 = …# and #… = -4 = -1 = 2 = 5 = 8 = …#.</p>
<h2 id="equivalence-relations">Equivalence Relations</h2>
<p>To start to formalize this, we need the notion of an equivalence relation. An <strong>equivalence relation</strong> is a binary relation <code class="asciimath">#(~~)#</code> which is <strong>reflexive</strong> (#x ~~ x# for all #x#), <strong>symmetric</strong> (if <code class="asciimath">#x ~~ y#</code> then <code class="asciimath">#y ~~ x#</code>), and <strong>transitive</strong> (if <code class="asciimath">#x ~~ y#</code> and <code class="asciimath">#y ~~ z#</code> then <code class="asciimath">#x ~~ z#</code>). We can check that “#a ~~ b# iff there exists an integer #k# such that #a-b = kn#” defines an equivalence relation on the integers for any given #n#. For reflexivity we have #a - a = 0n#. For symmetry we have if #a - b = kn# then #b - a = -kn#. Finally, for transitivity we have if #a - b = k_1 n# and #b - c = k_2 n# then #a - c = (k_1 + k_2)n# which we get by adding the preceding two equations.</p>
<p>Any relation can be extended to an equivalence relation. This is called the reflexive-, symmetric-, transitive-closure of the relation. For an arbitrary binary relation #R# we can define the equivalence relation #(~~_R)# via "#a ~~_R b# iff #a = b# or #R(a, b)# or #b ~~_R a# or #a ~~_R c and c ~~_R b# for some #c#". To be precise, #~~_R# is the smallest relation satisfying those constraints. In Datalog syntax, this looks like:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode prolog"><code class="sourceCode prolog"><span id="cb1-1"><a href="#cb1-1"></a>eq_r(<span class="dt">A</span><span class="kw">,</span> <span class="dt">A</span>)<span class="kw">.</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>eq_r(<span class="dt">A</span><span class="kw">,</span> <span class="dt">B</span>) <span class="kw">:-</span> r(<span class="dt">A</span><span class="kw">,</span> <span class="dt">B</span>)<span class="kw">.</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>eq_r(<span class="dt">A</span><span class="kw">,</span> <span class="dt">B</span>) <span class="kw">:-</span> eq_r(<span class="dt">B</span><span class="kw">,</span> <span class="dt">A</span>)<span class="kw">.</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>eq_r(<span class="dt">A</span><span class="kw">,</span> <span class="dt">B</span>) <span class="kw">:-</span> eq_r(<span class="dt">A</span><span class="kw">,</span> <span class="dt">C</span>)<span class="kw">,</span> eq_r(<span class="dt">C</span><span class="kw">,</span> <span class="dt">B</span>)<span class="kw">.</span></span></code></pre></div>
<h2 id="quotient-types-the-type-theory-view">Quotient Types: the Type Theory view</h2>
<p>If #T# is a type, and <code class="asciimath">#(~~)#</code> is an equivalence relation, we use #T // ~~# as the notation for the <strong>quotient type</strong>, which we read as “#T# quotiented by the equivalence relation <code class="asciimath">#(~~)#</code>”. We call #T# the <strong>underlying type</strong> of the quotient type. We then say #a = b# at type #T // ~~# iff #a ~~ b#. Dual to subset types, to produce a value of a quotient type is easy. Any value of the underlying type is a value of the quotient type. (In type theory, this produces the perhaps surprising result that #ZZ# is a <em>subtype</em> of #ZZ // n ZZ#.) As expected, consuming a value of a quotient type is more complicated. To explain this, we need to explain what a function #f : T // ~~ -&gt; X# is for some type #X#. A function #f : T // ~~ -&gt; X# is a function #g : T -&gt; X# which satisfies #g(a) = g(b)# for all #a# and #b# such that #a ~~ b#. We call #f# (or #g#, they are often conflated) <strong>well-defined</strong> if #g# satisfies this condition. In other words, any well-defined function that consumes a quotient type isn’t allowed to produce an output that distinguishes between equivalent inputs. A better way to understand this is that quotient types allow us to change what the notion of equality is for a type. From this perspective, a function being well-defined just means that it is a function. Taking equal inputs to equal outputs is one of the defining characteristics of a function.</p>
<p>Sometimes we can finesse needing to check the side condition. Any function #h : T -&gt; B# gives rise to an equivalence relation on #T# via #a ~~ b# iff #h(a) = h(b)#. In this case, any function #g : B -&gt; X# gives rise to a function #f : T // ~~ -&gt; X# via #f = g @ h#. In particular, when #B = T# we are guaranteed to have a suitable #g# for any function #f : T // ~~ -&gt; X#. In this case, we can implement quotient types in a manner quite similar subset types, namely we make an abstract type and we normalize with the #h# function as we either produce or consume values of the abstract type. A common example of this is rational numbers. We can reduce a rational number to lowest terms either when it’s produced or when the numerator or denominator get accessed, so that we don’t accidentally write functions which distinguish between #1/2# and #2/4#. For modular arithmetic, the mod by #n# function is a suitable #h#.</p>
<h2 id="quotient-types-the-set-theory-view">Quotient Types: the Set Theory view</h2>
<p>In set theory such an #h# function can always be made by mapping the elements of #T# to the equivalence classes that contain them, i.e. #a# gets mapped to #{b | a ~~ b}# which is called the <strong>equivalence class</strong> of #a#. In fact, in set theory, #T // ~~# is usually defined to <em>be</em> the set of equivalence classes of <code class="asciimath">#(~~)#</code>. So, for the example of #ZZ // 3 ZZ#, in set theory, it is a set of exactly three elements: the elements are #{ 3n+k | n in ZZ}# for #k = 0, 1, 2#. Equivalence classes are also called <strong>partitions</strong> and are said to partition the underlying set. Elements of these equivalence classes are called <strong>representatives</strong> of the equivalence class. Often a notation like #[a]# is used for the equivalence class of #a#.</p>
<h2 id="more-examples">More Examples</h2>
<p>Here is a quick run-through of some significant applications of quotient types. I’ll give the underlying type and the equivalence relation and what the quotient type produces. I’ll leave it as an exercise to verify that the equivalence relations really are equivalence relations, i.e. reflexive, symmetric, and transitive. I’ll start with more basic examples. You should work through them to be sure you understand how they work.</p>
<h3 id="integers">Integers</h3>
<p>Integers can be presented as pairs of naturals #(n, m)# with the idea being that the pair represents “#n - m#”. Of course, #1 - 2# should be the same as #2 - 3#. This is expressed as #(n_1, m_1) ~~ (n_2, m_2)# iff #n_1 + m_2 = n_2 + m_1#. Note how this definition only relies on operations on natural numbers. You can explore how to define addition, subtraction, multiplication, and other operations on this representation in a well-defined manner.</p>
<h3 id="rationals">Rationals</h3>
<p>Rationals can be presented very similarly to integers, only with multiplication instead of addition. We also have pairs #(n, d)#, usually written #n/d#, in this case of an integer #n# and a non-zero natural #d#. The equivalence relation is #(n_1, d_1) ~~ (n_2, d_2)# iff #n_1 d_2 = n_2 d_1#.</p>
<h3 id="topological-circles">(Topological) Circles</h3>
<p>We can extend the integers mod #n# to the continuous case. Consider the real numbers with the equivalence relation #r ~~ s# iff #r - s = k# for some integer #k#. You could call this the reals mod #1#. Topologically, this is a circle. If you walk along it far enough, you end up back at a point equivalent to where you started. Occasionally this is written as #RR//ZZ#.</p>
<h3 id="torii">Torii</h3>
<p>Doing the previous example in 2D gives a torus. Specifically, we have pairs of real numbers and the equivalence relation #(x_1, y_1) ~~ (x_2, y_2)# iff #x_1 - x_2 = k# and #y_1 - y_2 = l# for some integers #k# and #l#. Quite a bit of topology relies on similar constructions as will be expanded upon on the section on gluing.</p>
<h3 id="unordered-pairs">Unordered pairs</h3>
<p>Here’s an example that’s a bit closer to programming. Consider the following equivalence relation on arbitrary pairs: #(a_1, b_1) ~~ (a_2, b_2)# iff #a_1 = a_2 and b_1 = b_2# or #a_1 = b_2 and b_1 = a_2#. This just says that a pair is equivalent to either itself, or a swapped version of itself. It’s interesting to consider what a well-defined function is on this type.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<h3 id="gluing-pushouts">Gluing / Pushouts</h3>
<p>Returning to topology and doing a bit more involved construction, we arrive at gluing or pushouts. In topology, we often want to take two topological spaces and glue them together in some specified way. For example, we may want to take two discs and glue their boundaries together. This gives a sphere. We can combine two spaces into one with the disjoint sum (or coproduct, i.e. Haskell’s <code>Either</code> type.) This produces a space that contains both the input spaces, but they don’t interact in any way. You can visualize them as sitting next to each other but not touching. We now want to say that certain pairs of points, one from each of the spaces, are really the same point. That is, we want to quotient by an equivalence relation that would identify those points. We need some mechanism to specify which points we want to identify. One way to accomplish this is to have a pair of functions, #f : C -&gt; A# and #g : C -&gt; B#, where #A# and #B# are the spaces we want to glue together. We can then define a relation #R# on the disjoint sum via #R(a, b)# iff there’s a #c : C# such that <code class="asciimath">#a = tt "inl"(f(c)) and b = tt "inr"(g(c))#</code>. This is not an equivalence relation, but we can extend it to one. The quotient we get is then the gluing of #A# and #B# specified by #C# (or really by #f# and #g#). For our example of two discs, #f# and #g# are the same function, namely the inclusion of the boundary of the disc into the disc. We can also glue a space to itself. Just drop the disjoint sum part. Indeed, the circle and torus are examples.</p>
<h3 id="polynomial-ring-ideals">Polynomial ring ideals</h3>
<p>We write #RR[X]# for the type of polynomials with one indeterminate #X# with real coefficients. For two indeterminates, we write #RR[X, Y]#. Values of these types are just polynomials such as #X^2 + 1# or #X^2 + Y^2#. We can consider quotienting these types by equivalence relations generated from identifications like #X^2 + 1 ~~ 0# or #X^2 - Y ~~ 0#, but we want more than just the reflexive-, symmetric-, transitive-closure. We want this equivalence relation to also respect the operations we have on polynomials, in particular, addition and multiplication. More precisely, we want if #a ~~ b# and #c ~~ d# then #ac ~~ bd# and similarly for addition. An equivalence relation that respects all operations is called a <strong>congruence</strong>. The standard notation for the quotient of #RR[X, Y]# by a congruence generated by both of the previous identifications is #RR[X, Y]//(X^2 + 1, X^2 - Y)#. Now if #X^2 + 1 = 0# in #RR[X, Y]//(X^2 + 1, X^2 - Y)#, then for <em>any</em> polynomial #P(X, Y)#, we have #P(X, Y)(X^2 + 1) = 0# because #0# times anything is #0#. Similarly, for any polynomial #Q(X, Y)#, #Q(X, Y)(X^2 - Y) = 0#. Of course, #0 + 0 = 0#, so it must be the case that #P(X, Y)(X^2 + 1) + Q(X, Y)(X^2 - Y) = 0# for all polynomials #P# and #Q#. In fact, we can show that all elements in the equivalence class of #0# are of this form. You’ve now motivated the concrete definition of a ring ideal and given it’s significance. An <strong>ideal</strong> is an equivalence class of #0# with respect to some congruence. Let’s work out what #RR[X, Y]//(X^2 + 1, X^2 - Y)# looks like concretely. First, since #X^2 - Y = 0#, we have #Y = X^2# and so we see that values of #RR[X, Y]//(X^2 + 1, X^2 - Y)# will be polynomials in only one indeterminate because we can replace all #Y#s with #X^2#s. Since #X^2 = -1#, we can see that all those polynomials will be linear (i.e. of degree 1) because we can just keep replacing #X^2#s with #-1#s, i.e. #X^(n+2) = X^n X^2 = -X^n#. The end result is that an arbitrary polynomial in #RR[X, Y]//(X^2 + 1, X^2 - Y)# looks like #a + bX# for real numbers #a# and #b# and we have #X^2 = -1#. In other words, #RR[X, Y]//(X^2 + 1, X^2 - Y)# is isomorphic to the complex numbers, #CC#.</p>
<p>As a reasonably simple exercise, given a polynomial #P(X) : RR[X]#, what does it get mapped to when embedded into #RR[X]//(X - 3)#, i.e. what is #[P(X)] : RR[X]//(X - 3)#?<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<h3 id="free-algebras-modulo-an-equational-theory">Free algebras modulo an equational theory</h3>
<p>Moving much closer to programming, we have a rather broad and important example that a mathematician might describe as free algebras modulo an equational theory. This example covers several of the preceding examples. In programmer-speak, a free algebra is just a type of abstract syntax trees for some language. We’ll call a specific abstract syntax tree a <strong>term</strong>. An equational theory is just a collection of pairs of terms with the idea being that we’d like these terms to be considered equal. To be a bit more precise, we will actually allow terms to contain (meta)variables. An example equation for an expression language might be <code>Add(</code>#x#<code>,</code>#x#<code>) = Mul(2,</code>#x#<code>)</code>. We call a term with no variables a <strong>ground term</strong>. We say a ground term <strong>matches</strong> another term if there is a consistent substitution for the variables that makes the latter term syntactically equal to the ground term. E.g. <code>Add(3, 3)</code> matches <code>Add(</code>#x#<code>,</code>#x#<code>)</code> via the substitution #x |-&gt;#<code>3</code>. Now, the equations of our equational theory gives rise to a relation on ground terms #R(t_1, t_2)# iff there exists an equation #l = r# such that #t_1# matches #l# and #t_2# matches #r#. This relation can be extended to an equivalence relation on ground terms, and we can then quotient by that equivalence relation.</p>
<p>Let’s consider a worked example. We can consider the theory of monoids. We have two operations (types of AST nodes): <code>Mul(</code>#x#<code>,</code>#y#<code>)</code> and <code>1</code>. We have the following three equations: <code>Mul(1,</code>#x#<code>) =</code>#x#, <code>Mul(</code>#x#<code>, 1) =</code>#x#, and <code>Mul(Mul(</code>#x#<code>,</code>#y#<code>),</code>#z#<code>) = Mul(</code>#x#<code>, Mul(</code>#y#<code>,</code>#z#<code>))</code>. We additionally have a bunch of constants subject to no equations. In this case, it turns out we can define a normalization function, what I called #h# far above, and that the quotient type is isomorphic to lists of constants. Now, we can extend this theory to the theory of groups by adding a new operation, <code>Inv(</code>#x#<code>)</code>, and new equations: <code>Inv(Inv(</code>#x#<code>)) =</code>#x#, <code>Inv(Mul(</code>#x#<code>,</code>#y#<code>)) = Mul(Inv(</code>#y#<code>), Inv(</code>#x#<code>))</code>, and <code>Mul(Inv(</code>#x#<code>),</code>#x#<code>) = 1</code>. If we ignore the last of these equations, you can show that we can normalize to a form that is isomorphic to a list of a disjoint sum of the constants, i.e. <code>[Either Const Const]</code> in Haskell if <code>Const</code> were the type of the constant terms. Quotienting this type by the equivalence relation extended with that final equality corresponds to adding the rule that a <code>Left c</code> cancels out <code>Right c</code> in the list whenever they are adjacent.</p>
<p>This overall example is a fairly profound one. Almost all of abstract algebra can be viewed as an instance of this or a closely related variation. When you hear about things defined in terms of “generators and relators”, it is an example of this sort. Indeed, those “relators” are used to define a relation that will be extended to an equivalence relation. Being defined in this way is arguably what it <em>means</em> for something to be “algebraic”.</p>
<h2 id="postscript">Postscript</h2>
<p>The <a href="http://www.nuprl.org/book/Introduction_Type_Theory.html">Introduction to Type Theory</a> section of the NuPRL book provides a more comprehensive and somewhat more formal presentation of these and related concepts. While the quotient <em>type</em> view of quotients is conceptually different from the standard set theoretic presentation, it is much more amenable to computation as the #ZZ // n ZZ# example begins to illustrate.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>It’s a commutative function.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>It gets mapped to it’s value at #3#, i.e. #P(3)#.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Constructivist Motto</title>
    <link href="https://derekelkins.github.io/posts/constructivist-motto.html" />
    <id>https://derekelkins.github.io/posts/constructivist-motto.html</id>
    <published>2016-05-07 11:12:29-07:00</published>
    <updated>2016-05-07T18:12:29Z</updated>
    <summary type="html"><![CDATA[<blockquote>
<p>I don’t believe classical logic is false; I just believe that it is not true.</p>
</blockquote>]]></summary>
</entry>

</feed>
