<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="A blog mostly on math, physics, and computer science.">
    <meta name="author" content="Derek Elkins">
    <!--<link rel="icon" href="/images/favicon.ico">-->

    <title>Complex-Step Differentiation</title>
    <!-- Bootstrap core CSS -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <!-- Custom styles for this template -->
    <link href="../css/blog.css" rel="stylesheet">
    <link href="../css/syntax.css" rel="stylesheet">
  </head>

  <body>
    <div class="blog-masthead">
      <div class="container">
        <nav class="blog-nav">
          <a class="blog-nav-item " href="../">Home</a>
          <a class="blog-nav-item " href="../about.html">About</a>
          <a class="blog-nav-item " href="../contact.html">Contact</a>
          <a class="blog-nav-item " href="../readinglist.html">Reading List</a>
          <a class="blog-nav-item " href="../archive.html">Archive</a>
          <a class="blog-nav-item pull-right" href="../rss.xml">RSS</a>
          <a class="blog-nav-item pull-right" href="../atom.xml">Atom</a>
        </nav>
      </div>
    </div>

    <div class="container">
      
      
      <div class="row">
        
        <div class="col-sm-12 blog-main">
        
          <div class="blog-post">
    <h2 class="blog-post-title" style="margin-top: 30px;">Complex-Step Differentiation</h2>
    <p class="blog-post-meta">August  9, 2020 05:28 UTC 
        
        (Last updated on December 13, 2021 21:27 UTC)
    </p>
    <h6 class="blog-post-meta">
        
        Tags: <a title="All pages tagged 'math'." href="../tags/math.html">math</a>, <a title="All pages tagged 'programming'." href="../tags/programming.html">programming</a>
        
    </h6>

    <h2 id="introduction">Introduction</h2>
<p>Complex-step differentiation is a simple and effective technique for numerically differentiating a(n analytic) function.
Discussing it is a neat combination of complex analysis, numerical analysis, and ring theory. We’ll see that it is very
closely connected to forward-mode automatic differentiation (FAD). For better or worse, while widely applicable, the scenarios
where complex-step differentiation is the <em>best</em> solution are a bit rare. To apply complex-step differentiation, you need
a version of your desired function that operates on complex numbers. If you have that, then you can apply complex-step
differentiation immediately. Otherwise, you need to adapt the function to complex arguments. This can be done essentially
automatically using the same techniques as automatic differentiation, but at that point you might as well use automatic
differentiation. Adapting the code to complex numbers or AD takes about the same amount of effort, however, the AD version
will be more efficient, more accurate, and easier to use.</p>
<p>Nevertheless, this serves as a simple example to illustrate several theoretical and practical ideas.</p>
<!--more-->
<h2 id="numerical-differentiation">Numerical Differentiation</h2>
<p>The problem we’re solving is given a function |f : \mathbb R \to \mathbb R| which is differentiable around a point |x_0|,
we’d like to compute its derivative |f’| at |x_0|. In many cases, |f| is <a href="https://en.wikipedia.org/wiki/Analytic_function">real analytic</a>
at the point |x_0| meaning |f| has a <a href="https://en.wikipedia.org/wiki/Taylor_series#Definition">Taylor series</a> which converges
to |f| in some open interval containing |x_0|.</p>
<p>The most obvious way of numerically differentiating |f| is to approximate the limit in the definition of
the derivative, \[f’(x) = \lim_{h\to 0} [f(x + h) - f(x)] / h\] by simply choosing a small value for |h| rather
than taking the limit. When |f| is real analytic at |x|, we can analyze the quality of this approximation by expanding |f(x + h)|
in a Taylor series at |x|. This produces \[[f(x + h) - f(x)]/h = f’(x) + O(h)\] A slight tweak produces a better result with the
same number of evaluations of |f|. Specifically, the Taylor series of |f(x + h) - f(x - h)| at |x| is equal to the
odd part the Taylor series of |f(x + h)| at |x|. This leads to the <strong>Central Differences</strong> formula:</p>
<p><span class="math display">$$f'(x) + O(h^2) = \frac{f(x + h) - f(x - h)}{2h}$$</span></p>
<p>The following interactively illustrates this using the function
<select style="width: 2em;" id="functionSelector">
<option value="0" selected>1</option>
<option value="1">2</option>
<option value="2">3</option>
<option value="3">4</option>
<option value="4">5</option>
</select> <span id="ex1">|f(x) = x^{9/2}|</span><span id="sin" style="display: none;">|f(x) = \sin(x)|</span><span id="exp" style="display: none;">|f(x) = e^x|</span><span id="ex2" style="display: none;">|f(x) = e^x/(\sin(x)^3 + \cos(x)^3)|</span> evaluated at |x_0 =| <span class="point"></span>.
The correct answer to |17| digits is |f’(|<span class="point"></span>|) {}={}|<span id="correct"></span>. The slider ranges from |h=10^{-2}| to |h=10^{-20}|.</p>
<p><input id="realInput" type="range" min="2" max="20"><br />
|h|: <span id="realH"></span><br />
|f’(|<span class="point"></span>|)|: <span id="real"></span><br />
error: <span id="realError"></span></p>
<p>If you play with the slider using the first example, you’ll see that the error decreases until around |10^{-5}| after which it starts
increasing until |10^{-15}| where it is off by more than |1|. At |10^{-16}| the estimated derivative is |0| which is, of course,
completely incorrect. Even at |10^{-5}| the error is on the order of |10^{-9}| which is much higher than the
double precision floating point machine epsilon of approximately |10^{-16}|.</p>
<p>There are two issues here. First, we have the issue that if |x_0 \neq 0|, then |x_0 + h = x_0| for sufficiently small |h|.
This happens when |x_0/h| has a magnitude of around |10^{16}| or more.</p>
<p>The second issue here is known as catastrophic cancellation. For simplicity, let’s say |f(x)=1|. (It’s actually about |6.2| for the
first example.) Let’s further say for some small value of |h|, |f(x+h) = 1.00000000000020404346|. The value we care about is the
|0.00000000000020404346|, but given limited precision, we might have |f(x + h) = 1.000000000000204|, meaning we only
have three digits of precision for the value we care about. Indeed, as |h| becomes smaller we’ll lose more and more
precision in our desired value until we lose all precision which happens when |f(x + h) = f(x)|. It is generally
a bad idea numerically to calculate a small value by subtracting two larger values for this reason.</p>
<p>We would not have the first issue if |x_0 = 0| as in the second and fourth examples (|f(x) = e^x|).
We would not have the second issue if |f(x) = 0| as in the second and third examples (|f(x) = \sin(x)| with |x_0 = \pi|).
We have neither issue in the second example of |f(x) = \sin(x)| with |x_0 = 0|.
This will become important later.</p>
<p>We have a dilemma. For the theory, we want as small a value of |h| as possible without being zero. In practice, we start
losing precision as |h| gets smaller, and generally larger values of |h| are going to be less impacted by this.</p>
<p>Let’s set this aside for now and look at other ways of numerically computing the derivative in the hopes that
we can avoid this problem.</p>
<h2 id="cauchys-residue-theorem">Cauchy’s Residue Theorem</h2>
<p>If we talk about functions |f : \mathbb C \to \mathbb C|, the analogue of real analyticity
is <a href="https://en.wikipedia.org/wiki/Holomorphic_function">holomorphicity</a> or <a href="https://en.wikipedia.org/wiki/Holomorphic_functions_are_analytic">complex analyticity</a>.
A complex function is <strong>holomorphic</strong> if it satisfies the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations">Cauchy-Riemann equations</a>.
(See the <a href="#appendix">Appendix</a> for more details about where the Cauchy-Riemann equations come from.)
A complex function is <strong>complex analytic</strong> if it has a Taylor series which converges to the function. It can be proven
that these two descriptions are equivalent, though this isn’t a trivial fact. We can also talk about functions that
are holomorphic or complex analytic on an open subset of |\mathbb C| and at a point by considering an open subset around
that point. The typical choice of open subset is some suitably small open disk in the complex plane about the point.
(Other common domains are ellipses, infinite strips, and areas bounded by <a href="https://en.wikipedia.org/wiki/Hankel_contour">Hankel contours</a>
and variations such as sideways opening parabolas.)</p>
<p>A major fact about holomorphic functions is the <a href="https://en.wikipedia.org/wiki/Cauchy%27s_integral_theorem">Cauchy integral theorem</a>.
If |f| is a holomorphic function inside a (suitably nice) closed curve |\Gamma| in the complex plane, then |\oint_\Gamma f(z)\mathrm dz = 0|.
Again, |\Gamma| will typically be chosen to be some circle. (Integrals like this in the complex plane are often called
<strong>contour integrals</strong> and the curves we’re integrating along are called <strong>contours</strong>.)</p>
<p>Things get really interesting when we generalize to <a href="https://en.wikipedia.org/wiki/Meromorphic_function">meromorphic functions</a>
which are complex functions that are holomorphic except at an isolated set of points. These take the form of <a href="https://en.wikipedia.org/wiki/Pole_(complex_analysis)"><strong>poles</strong></a>
which are points |z_0| such that |1/f(z_0) = 0|, i.e. poles are where a function goes to infinity as, e.g., |1/z| does at |0|.
The generalization of Cauchy’s integral theorem is <a href="https://en.wikipedia.org/wiki/Residue_theorem">Cauchy’s Residue Theorem</a>. <em>This theorem
is surprising and is one of the most useful theorems in all of mathematics both theoretically and practically</em>.</p>
<p>We’ll only need a common special case of it. Let |f| be a holomorphic function, then |f(z)/(z - z_0)^n| is a meromorphic function
with a single pole of order |n| at |z_0|. If |\Gamma| is a positively oriented, simple closed curve containing |z_0|,
then <span class="math display">$$f^{(n-1)}(z_0) = \frac{(n-1)!}{2 \pi i}\oint_{\Gamma} \frac{f(z)\mathrm dz}{(z - z_0)^n}$$</span> In this case, |f^{(n-1)}(z_0)/(n-1)!|
is the <strong>residue</strong> of |f(z)/(z - z_0)^n| at |z_0|. More generally, if there are multiple poles in the area bounded by |\Gamma|, then we will
sum up their residues.</p>
<p>This formula provides us a means of calculating the |(n-1)|-st Taylor coefficient of a complex analytic function at any point.
For our particular purposes, we’ll only need the |n=2| case, \[f’(z_0) = \frac{1}{2 \pi i}\oint_{\Gamma} \frac{f(z)\mathrm dz}{(z - z_0)^2}\]</p>
<p>For the remainder of this section I want to give some examples of how Cauchy’s Residue Theorem is used both theoretically
and practically. This whole article will itself be another practical example of Cauchy’s Residue Theorem. This is not exhaustive
by any means.</p>
<p>To start illustrating some of the surprising properties of this theorem, we can take the |n=1| case which states that we can evaluate
a holomorphic function at any point via |f(z_0) = \frac{1}{2 \pi i}\oint_{\Gamma} \frac{f(z)\mathrm dz}{z - z_0}| where |\Gamma|
is any contour which bounds an area containing |z_0|. This leads to an interesting discreteness. Not only can we evaluate
a (holomorphic) function (or any of its derivatives) at a point via the values of the function on a contour, the only significant
constraint on that contour is that it bound an area containing the desired point. In other words, no matter how we deform the contour the
integral is constant except when we deform the contour so as not to bound an area containing the point being evaluated, at which point
the integral’s value is |0|<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. It may seem odd to use an integral to evaluate
a function at a point, but it can be useful when there are numerical issues with evaluating the function near the desired point<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. In fact, these results show that if we know the values of a holomorphic function on the boundary of a given open subset of
the complex plane, then we know the value of the function <em>everywhere</em>. In this sense, holomorphic functions (and analytic
functions in general) are extremely rigid.</p>
<p>This leads to the notion of <a href="https://en.wikipedia.org/wiki/Analytic_continuation">analytic continuation</a>
where we try to compute an analytic function beyond its overt domain of definition. This is the basis of most “sums of divergent series”.
For example, there is the first-year calculus fact that the sum of the infinite series |\sum_{n=0}^\infty x^n| is |1/(1-x)| converging
on the interval |x \in (-1,1)|. In fact, the proof of convergence only needs |\|x\| &lt; 1| so we can readily generalize to
complex |z| with |\|z\| &lt; 1|, i.e. |z| contained in the open unit disk. However, |1/(1-z)| is a meromorphic function that is holomorphic
everywhere except for |z=1|, therefore there is a <em>unique</em> analytic function defined everywhere except |z=1| that agrees with
the infinite sum on the unit disk, namely |1/(1-z)| itself. Choosing |z=-1| leads to the common example of “summing a divergent series”
with “|\sum_{n=0}^\infty (-1)^n = 1/2|” which really means “the value at |-1| of the unique complex analytic function which agrees
with this infinite series when it converges”.</p>
<p>Sticking with just evaluation, applying the Cauchy Residue theorem to quadrature, i.e. numerical integration, leads to an interesting
connection to a rational approximation problem. Say we want to compute |\int_{-1}^1 f(x) \mathrm dx|, we can use the Cauchy
integral to evaluate |f(x)| leading to <span class="math display">$$\int_{-1}^1 f(x) \mathrm dx
= \int_{-1}^1 \frac{1}{2\pi i}\oint \frac{f(z)\mathrm dz}{z - x}\mathrm dx
= \frac{1}{2\pi i}\oint f(z)\int_{-1}^1 \frac{\mathrm dx}{z - x}\mathrm dz
= \frac{1}{2\pi i}\oint f(z)\log\left(\frac{z+1}{z-1}\right)\mathrm dz$$</span>
A quadrature formula looks like |\int_{-1}^1 f(x) \mathrm dx \approx \sum_{k=1}^N w_k f(x_k)|. The sum
can be written as a Cauchy integral of |\oint f(z)\sum_{k=1}^N \frac{1}{2\pi i}\frac{w_k\mathrm dz}{z - x_k}|. We thus have
<span class="math display">$$\left|\frac{1}{2\pi i}\oint f(z)\left[\log\left(\frac{z+1}{z-1}\right) - \sum_{k=1}^N \frac{w_k}{z - x_k}\right]\mathrm dz\right|$$</span> as the error of
the approximation. The sum is a rational function (in partial fraction form) and thus the error is minimized by points (|x_k|)
and weights (|w_k|) that lead to better rational approximations of |\log((z+1)/(z-1))|<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>The ability to calculate coefficients of the Taylor series of a holomorphic function is, by itself, already a valuable
tool in both theory and practice. In particular, the coefficients of a <a href="https://en.wikipedia.org/wiki/Generating_function">generating function</a>
or a <a href="https://en.wikipedia.org/wiki/Z-transform">Z-transform</a> can be computed with Cauchy integrals. This has applications
in probability theory, statistics, finance, combinatorics, recurrences, differential equations, and signal processing.
Indeed, when |z_0 = 0| and |\Gamma| is the unit circle, then the Cauchy integral is a component of the <a href="https://en.wikipedia.org/wiki/Fourier_series#Complex-valued_functions">Fourier series</a>
of |f|. Approximating these integrals with the Trapezoid Rule (which we’ll discuss in a bit) produces the <a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform">Discrete Fourier Transform</a>.</p>
<p>Let |p| be a polynomial and, for simplicity, assume all its zeroes are of multiplicity one. Then |1/p(z)| is a meromorphic function
that’s holomorphic everywhere except for the roots of |p|. The Cauchy integral |\frac{1}{2\pi i}\oint_{\Gamma} \frac{p’(z)\mathrm dz}{p(z)}|
counts the number of roots of |p| contained in the area bounded by |\Gamma|. If we know there is only one root of |p| within
the area bounded by |\Gamma|, then we can compute that root with |\frac{1}{2\pi i}\oint_{\Gamma} \frac{z p’(z)\mathrm dz}{p(z)}|.
A better approach is to use the formulas |\left(\oint \frac{z\mathrm dz}{p(z)}\right)/\left(\oint \frac{\mathrm dz}{p(z)}\right)|.
Similar ideas can be used to adapt this to counting and finding multiple roots. See
<a href="https://doi.org/10.1137/130931035">Numerical Algorithms based on Analytic Function Values at Roots of Unity</a> by Austin, Kravanja, and
Trefethen (2014) which is a good survey in general.</p>
<p>Another very common use of Cauchy’s Residue Theorem is to sum (convergent) infinite series. |\tan(\pi z)/\pi| has a zero at
|z = k| for each integer |k| and a non-zero derivative at those points. In fact, the derivative is |1|. Alternatively,
we could use |\sin(\pi z)/\pi| which has a zero at |z = k| for each integer |k| but has derivative |(-1)^k| at those points.
Therefore, |\pi\cot(\pi z) = \pi/\tan(\pi z)| has a (first-order) pole at |z = k| for each integer |k|
with residue |1|. In particular, if |f| is a holomorphic function (at least near the real axis), then the value of the
Cauchy integral of |f(z)\pi\cot(\pi z)| along a Hankel contour will be |2\pi i \sum_{k=0}^\infty f(k)|. Along an infinite strip
around the real axis we’d get |2 \pi i \sum_{k=-\infty}^\infty f(k)|. As an example, we can consider the
<a href="https://en.wikipedia.org/wiki/Basel_problem">famous sum</a>, |\sum_{k=1}^\infty 1/k^2|. It can be shown that if |f| is a meromorphic
function whose poles are not located at integers and |\vert zf(z)\vert| is bounded for sufficiently large |\vert z\vert|, then |\oint f(z)\pi \cot(\pi z)\mathrm dz = 0|. We thus have
that \[\sum_{k=-\infty}^{\infty} f(k) = -\sum_j \mathrm{Res}(f(z)\pi\cot(\pi z); z_j)\] where |z_j| are the poles of |f|.
In particular, |f(z) = \frac{1}{z^2 + a^2}| has (first-order) poles at |\pm ai|. This gives us simply
<span class="math display">$$\sum_{k=-\infty}^{\infty} \frac{1}{k^2 + a^2} = -\pi\frac{\cot(\pi a i)-\cot(-\pi a i)}{2ai} = \frac{\pi}{a}\coth(\pi a)$$</span>
where I’ve used |\coth(x) = i\cot(xi)| and the fact that |\coth| is an odd function. Exploiting the symmetry of the sum
gives us <span class="math display">$$\sum_{k=1}^{\infty} \frac{1}{k^2 + a^2} = \frac{\pi}{2a}\coth(\pi a) - \frac{1}{2a^2}$$</span> By expanding |\coth|
in a Laurent series, we see that the limit of the right-hand side as |a| approaches |0| is |\frac{\pi^2}{6}|. While contour
integration is quite effective for coming up with analytic solutions to infinite sums, numerically integrating the contour
integrals is also highly effective as illustrated in <a href="https://doi.org/10.1007/s10543-006-0077-9">Talbot quadratures and rational approximations</a>
by Trefethen, Weideman, and Schmelzer (2006), for example.</p>
<h2 id="computing-the-integrals">Computing the Integrals</h2>
<p>We’ve seen in the previous section that |f’(z_0) = \frac{1}{2\pi i}\oint_{\Gamma} \frac{f(z)\mathrm dz}{(z-z_0)^2}|.
This doesn’t much help us if we don’t have a way to compute the integrals. From this point forward, fix |\Gamma|
as a circle of radius |h| centered on |z_0|.</p>
<p>Before that, let’s consider numerical integration in general. Say we want to integrate the real function |f| from |0| to |b|,
i.e. we want to calculate |\int_0^b f(x)\mathrm dx|. The most obvious way to go about it is to approximate the Riemann
sums that define the (Riemann) integral. This would produce a formula like
|\int_0^b f(x)\mathrm dx \approx \frac{b}{N}\sum_{k=0}^{N-1} f(bk/N)| corresponding to summing the areas of rectangles
whose left points are the values of |f|. As before with central differences, relatively minor tweaks will give better approximations. In particular,
we get the two roughly equivalent approximations of the <a href="https://en.wikipedia.org/wiki/Midpoint_rule#Midpoint_rule"><strong>Midpoint Rule</strong></a>
\[\int_0^b f(x)\mathrm dx \approx \frac{b}{N}\sum_{k=0}^{N-1} f\left(\frac{b(k+(1/2))}{N}\right)\] where we take the midpoint rather than the
left or right point, and the <a href="https://en.wikipedia.org/wiki/Trapezoidal_rule"><strong>Trapezoid Rule</strong></a>
\[\int_0^b f(x)\mathrm dx \approx \frac{b}{2N}\sum_{k=0}^{N-1}[f(b(k+1)/N) + f(bk/N)]\] where we average the left and right Riemann
sums. While both of these perform substantially better than the left/right Riemann sums, they are still rather basic
quadrature rules; the error decreases as |O(1/N^2)|.</p>
<p>Something special happens when |f| is a periodic function. First, the Trapezoid rule reduces to
|\frac{b}{N}\sum_{k=0}^{N-1} f(bk/N)|. More importantly, the Midpoint rule and the Trapezoid rule both start converging
geometrically rather than quadratically. Furthermore, for the particular case we’re interested in, namely integrating analytic
functions along a circle in the complex plane, these quadrature rules are optimal. Let |\zeta| be the |2N|-th root of unity.
The Trapezoid rule corresponds to sum the values of |f| at the even powers of |\zeta| scaled by the radius |h| and translated
by |z_0|, and the Midpoint rule corresponds to the sum of the odd powers.</p>
<p>We now have two parameters for approximating a Cauchy integral via the Trapezoid or Midpoint rules: the radius |h| and the
number of points |N|.</p>
<p>Complex-Step Differentiation corresponds to approximating the Cauchy integral for the derivative using the extreme case of
the Midpoint rule with |N=2| and very small radii (i.e. values of |h|). Meanwhile, Central Differences corresponds to the extreme case
of using the Trapezoid rule with |N=2| and very small radii. To spell this out a bit more, we perform the substitution
|z - z_0 = he^{\theta i}| which leads to |\mathrm dz = hie^{\theta i}\mathrm d\theta| and
<span class="math display">$$\frac{1}{2\pi i}\oint_{|z - z_0| = h} \frac{f(z)\mathrm dz}{(z - z_0)^2} = \frac{1}{2 \pi h}\int_0^{2\pi} f(z_0 + he^{\theta i})e^{-\theta i}\mathrm d\theta$$</span></p>
<p>Applying the Trapezoid rule to the right hand side of this corresponds to picking |\theta = 0, \pi|, while applying the
Midpoint rule corresponds to picking |\theta = \pm \pi/2|. |e^{\theta i} = \pm 1| for |\theta = 0, \pi|, and |e^{\theta i} = \pm i|
for |\theta = \pm \pi/2|. For the Trapezoid rule, this leads to \[f’(z_0) \approx \frac{1}{2h}[f(z_0 + h) - f(z_0 - h)]\] which is
Central Differences. For the Midpoint rule, this leads to \[f’(z_0) \approx \frac{1}{2hi}[f(z_0 + hi) - f(z_0 - hi)]\] This
is Complex-Step Differentiation when |z_0| is real.</p>
<h2 id="complex-step-differentiation">Complex-Step Differentiation</h2>
<p>As just calculated, <strong>Complex-Step Differentiation</strong> computes the derivative at the <em>real</em> number |x_0| via the formula:
<span class="math display">$$f'(x_0) \approx \frac{1}{2hi} [f(x_0 + hi) - f(x_0 - hi)]$$</span> Another perspective on this formula is that it is just the
Central Differences formula along the imaginary axis instead of the real axis.</p>
<p>When |f| is complex analytic and real-valued on real arguments, then we have
|f(\overline z) = \overline{f(z)}| where |\overline z| is the complex conjugate of |z|, i.e. it maps |a + bi| to |a - bi|
or |re^{\theta i}| to |re^{-\theta i}|. This leads to
|f(x_0 + hi) - f(\overline{x_0 + hi}) = f(x_0 + hi) - \overline{f(x_0 + hi)} = 2i\operatorname{Im}(f(x_0 + hi))|. This lets us simplify
Complex-Step Differentiation to |f’(x_0) \approx \operatorname{Im}(f(x_0 + h))/h|.</p>
<p>Here is the earlier interactive example but now using Complex-Step Differentiation. As |h| decreases in magnitude, the error
steadily decreases until there is no error at all.</p>
<p><input id="complexInput" type="range" min="2" max="20"><br />
|h|: <span id="complexH"></span><br />
|f’(|<span class="point"></span>|)|: <span id="complex"></span><br />
error: <span id="complexError"></span></p>
<p>This formula using |\operatorname{Im}| avoids catastrophic cancellation simply by not doing a subtraction. However, it turns out
for real |x_0| (which is necessary to derive the simplified formula), there isn’t a problem either way. Using the first form of
the Complex-Step Differentiation formula is also numerically stable. The key here is that the imaginary part of |x_0| and |f(x_0)| are
both |0| and so we don’t get catastrophic cancellation for the same reason we wouldn’t get it with Central Differences if |f(x_0) = 0|.
This suggests that if we wanted to evaluate |f’| at some non-zero point on the imaginary axis, Complex-Step Differentiation would
perform poorly while Central Differences would perform well. Further, if we wanted to evaluate |f’| at some point not on either
the real or imaginary axes, neither approach would perform well. In this case, choosing different values for |N| and the radius
would be necessary<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>A third perspective on Complex-Step Differentiation comes when we think about which value of |h| should we use. The smaller |f’(x_0)|
is, the smaller we’d want |h| to be. Unlike Central Differences, there is little stopping us from having |h| be <em>very</em> small and
values like |h=10^{-100}| are typical. In fact, around |h=10^{-155}| in double precision floating point arithmetic, |h| gets the
theoretically useful property that |h^2 = 0| due to underflow. In this case, |x_0 + hi| behaves like |x_0 + \varepsilon| where
|\varepsilon^2 = 0|. This is the defining property of the ring of <a href="https://en.wikipedia.org/wiki/Dual_number">dual numbers</a>.
Dual numbers are exactly what are used in forward-mode automatic differentiation.</p>
<h2 id="forward-mode-automatic-differentiation">Forward-Mode Automatic Differentiation</h2>
<p>The ring of dual numbers has numbers of the form |a + b\varepsilon| where |a, b \in \mathbb R|. This behaves just like
the complex numbers except that instead of |i^2 = -1| we have |\varepsilon^2 = 0|. The utility of dual numbers for
our purposes can be seen by expanding |f(x_0 + \varepsilon)| in a Taylor series about |x_0|. We get
|f(x_0 + \varepsilon) = f(x_0) + f’(x_0)\varepsilon|. All higher power terms of the Taylor series are zero because |\varepsilon^2 = 0|.
We can thus get the derivative of |f| simply by computing |f(x + \varepsilon)| and then looking at the coefficient of |\varepsilon|
in the result.</p>
<p>In this example there is no interactivity as we are not estimating the derivative in the AD case but instead calculating it in parallel.
There is no |h| parameter.</p>
<p>|f’(|<span class="point"></span>|)|: <span id="fad"></span><br />
error: <span id="fadError"></span></p>
<p>As the end of the previous section indicated, Complex-Step Differentiation approximates this (often exactly) by using |hi| as |\varepsilon|.
Nevertheless, this is not ideal. Often the complex versions of a function will be more costly than their dual number counterparts.
For example, |(a + bi)(c + di) = (ac - bd) + (ad + bc)i| involves four real multiplications and two additions.
|(a + b\varepsilon)(c + d\varepsilon) = ac + (ad + bc)\varepsilon| involves three real multiplications and one addition on
the other hand.</p>
<h2 id="references">References</h2>
<p><a href="https://doi.org/10.1137/S003614459631241X">Using Complex Variables to Estimate Derivatives of Real Functions</a> by Squire and Trapp (1998)
is the first(?) published paper <em>specifically</em> about the idea of complex-step differentiation. It’s a three page paper and the authors
are not claiming any originality but just demonstrating the effectiveness of ideas from the ’60s that the authors found to be underappreciated.</p>
<p><a href="https://doi.org/10.1145/838250.838251">The Complex-Step Derivative Approximation</a> by Martins, Sturdza, and Alonso (2003) does
a much deeper dive into the theory behind complex-step differentiation and its connections to automatic differentiation.</p>
<p>You may have noticed the name “Trefethen” in many of the papers cited. Nick Trefethen and his collaborators have been doing amazing
work for the past couple of decades, most notably in the <a href="https://www.chebfun.org/about/">Chebfun project</a>. Looking at
Trefethen’s book <a href="http://www.chebfun.org/ATAP/">Approximation Theory and Approximation Practice</a> (and <a href="https://people.maths.ox.ac.uk/trefethen/atapvideos.html">lectures</a>)
recently reintroduced me to <a href="https://people.maths.ox.ac.uk/trefethen/papers.html">Trefethen’s work</a>. This particular article was
prompted by a footnote in the paper <a href="https://doi.org/10.1137/130932132">The Exponentially Convergent Trapezoidal Rule</a> which I highly
recommend. In fact, I highly recommend Chebfun as well as nearly all of Trefethen’s work. It is routinely compelling, interesting, and
well presented.</p>
<h2 id="appendix">Appendix</h2>
<p>Using the language of <a href="http://geocalc.clas.asu.edu/">Geometric Calculus</a>, we can write a very general form of the Fundamental
Theorem of Calculus. Namely, \[\int_{\mathcal M} \mathrm d^m\mathbf x \cdot \nabla f(\mathbf x) = \oint_{\partial \mathcal M}\mathrm d^{m-1}\mathbf x f(\mathbf x)\] where
|\mathcal M| is an |m|-dimensional manifold. Here |f| is a multivector-valued vector function. If |m=2| and |\nabla f = 0|,
then this would produce a formula very similar to the Cauchy integral formula.</p>
<p>Writing |f(x + yi) = u(x, y) + v(x, y)i|, the Cauchy-Riemann equations are |\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}|
and |\frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x}|. However, |\nabla f = 0| leads to the slightly
different equations |\frac{\partial u}{\partial x} = -\frac{\partial v}{\partial y}| and |\frac{\partial u}{\partial y} = \frac{\partial v}{\partial x}|.</p>
<p>The resolution of this discrepancy is found by recognizing that we don’t want |f| to be a vector-valued vector function but rather
a spinor-valued spinor function. It is most natural to identify complex numbers with the even subalgebra of the 2D geometric
algebra. If |\mathbf e_1| and |\mathbf e_2| are the two orthonormal basis vectors of the 2D space, then the pseudoscalar
|I \equiv \mathbf e_1\wedge \mathbf e_2 = \mathbf e_1 \mathbf e_2| satisfies |I^2 = -1|. For the 2D case, a spinor
is a multivector of the form |a + bI|.</p>
<p>We can generalize the vector derivative, |\nabla|, to a multivector derivative |\nabla_X| where |X| is a multivector variable
by using the generic formula for the directional derivative in a linear space and then defining |\nabla_X| to be a linear
combination of directional derivatives. Given any |\mathbb R|-linear space |V| and an element |v \in V|, we can
define the directional derivative of |f : V \to V| in the direction |v| via
|\frac{\partial f}{\partial v}(x) \equiv \frac{\mathrm d f(x + \tau v)}{\mathrm d\tau}|. In our case,
we have the basis vectors |\{1, \mathbf e_1, \mathbf e_2, I\}| though we only care about the even subalgebra
corresponding to the basis vectors |\{1, I\}|. Define |\partial_1 f(x) \equiv \frac{\mathrm d f(x + \tau)}{\mathrm d \tau}|
and |\partial_I f(x) \equiv \frac{\mathrm d f(x + \tau I)}{\mathrm d \tau}| assuming |f| is a spinor-valued function<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.
We can then define |\nabla_{\mathbf z} \equiv \partial_1 + I\partial_I|. We now have |\nabla_{\mathbf z} f = 0| is
the equivalent to the Cauchy-Riemann equations where |f| is now a spinor-valued spinor function, i.e. a function of |\mathbf z|.</p>
See <a href="https://doi.org/10.1016/0022-247X(68)90002-4">Multivector Functions</a> by Hestenes for more about this.
<script>
var Num = (function () {
    function Num(real) {
        this.real = real;
    }
    Num.prototype.add = function (z) {
        return new Num(this.real + z.real);
    };
    Num.prototype.addReal = function (x) {
        return new Num(this.real + x);
    };
    Num.prototype.sub = function (z) {
        return new Num(this.real - z.real);
    };
    Num.prototype.scale = function (a) {
        return new Num(a * this.real);
    };
    Num.prototype.mul = function (z) {
        return new Num(this.real * z.real);
    };
    Num.prototype.div = function (z) {
        return new Num(this.real / z.real);
    };
    Num.prototype.exp = function () {
        return new Num(Math.exp(this.real));
    };
    Num.prototype.sin = function () {
        return new Num(Math.sin(this.real));
    };
    Num.prototype.cos = function () {
        return new Num(Math.cos(this.real));
    };
    Num.prototype.powN = function (n) {
        return new Num(Math.pow(this.real, n));
    };
    return Num;
}());

var Complex = (function () {
    function Complex(real, imag) {
        this.real = real;
        this.imag = imag;
    }
    Complex.cis = function (theta) {
        return new Complex(Math.cos(theta), Math.sin(theta));
    };
    Complex.prototype.normSquared = function () {
        var r = this.real;
        var i = this.imag;
        return r * r + i * i;
    };
    Complex.prototype.norm = function () {
        return Math.sqrt(this.normSquared());
    };
    Complex.prototype.arg = function () {
        return Math.atan2(this.imag, this.real);
    };
    Complex.prototype.add = function (z) {
        return new Complex(this.real + z.real, this.imag + z.imag);
    };
    Complex.prototype.addReal = function (x) {
        return new Complex(this.real + x, this.imag);
    };
    Complex.prototype.sub = function (z) {
        return new Complex(this.real - z.real, this.imag - z.imag);
    };
    Complex.prototype.scale = function (a) {
        return new Complex(a * this.real, a * this.imag);
    };
    Complex.prototype.mul = function (z) {
        var r = this.real;
        var i = this.imag;
        var zr = z.real;
        var zi = z.imag;
        return new Complex(r * zr - i * zi, r * zi + i * zr);
    };
    Complex.prototype.div = function (z) {
        return this.mul(z.powN(-1));
    };
    Complex.prototype.exp = function () {
        return Complex.cis(this.imag).scale(Math.exp(this.real));
    };
    Complex.prototype.cos = function () {
        var r = this.real;
        var i = this.imag;
        return new Complex(Math.cos(r)*Math.cosh(i), -Math.sin(r)*Math.sinh(i));
    };
    Complex.prototype.sin = function () {
        var r = this.real;
        var i = this.imag;
        return new Complex(Math.sin(r)*Math.cosh(i), Math.cos(r)*Math.sinh(i));
    };
    Complex.prototype.powN = function (n) {
        var r2 = this.normSquared();
        var theta = this.arg();
        return Complex.cis(n * theta).scale(Math.pow(r2, 0.5 * n));
    };
    return Complex;
}());

var Dual = (function () {
    function Dual(real, eps) {
        this.real = real;
        this.eps = eps;
    }
    Dual.prototype.add = function (z) {
        return new Dual(this.real + z.real, this.eps + z.eps);
    };
    Dual.prototype.addReal = function (x) {
        return new Dual(this.real + x, this.eps);
    };
    Dual.prototype.sub = function (z) {
        return new Dual(this.real - z.real, this.eps - z.eps);
    };
    Dual.prototype.scale = function (a) {
        return new Dual(a * this.real, a * this.eps);
    };
    Dual.prototype.mul = function (z) {
        var r = this.real;
        var e = this.eps;
        var zr = z.real;
        var ze = z.eps;
        return new Dual(r * zr, r * ze + e * zr);
    };
    Dual.prototype.div = function (z) {
        return this.mul(z.powN(-1));
    };
    Dual.prototype.exp = function () {
        return new Dual(Math.exp(this.real), this.eps * Math.exp(this.real));
    };
    Dual.prototype.sin = function () {
        return new Dual(Math.sin(this.real), this.eps * Math.cos(this.real));
    };
    Dual.prototype.cos = function () {
        return new Dual(Math.cos(this.real), this.eps * -Math.sin(this.real));
    };
    Dual.prototype.powN = function (n) {
        return new Dual(Math.pow(this.real, n), n * this.eps * Math.pow(this.real, n - 1));
    };
    return Dual;
}());

var functions = [
    function (x) { return x.powN(9 / 2); },
    function (x) { return x.sin(); },
    function (x) { return x.sin(); },
    function (x) { return x.exp(); },
    function (x) { return x.exp().div(x.sin().powN(3).add(x.cos().powN(3))); }];
var functionSpans = [
    document.getElementById('ex1'),
    document.getElementById('sin'),
    document.getElementById('sin'),
    document.getElementById('exp'),
    document.getElementById('ex2')];
var points = [1.5, 0, Math.PI, 0, 1.5];
var correct = ["18.600812734259759", "1.0000000000000000", "-1.0000000000000000", "1.0000000000000000", "3.6220337007163260"]

// Midpoint method
function midpoint(f, x, h) {
    return f(new Num(x + h)).sub(f(new Num(x - h))).scale(0.5 / h).real;
}

// Complex-step differentiation
function complexStep(f, x, h) {
    // return f(new Complex(x, h)).sub(f(new Complex(x, -h))).scale(0.5 / h).imag;
    return f(new Complex(x, h)).imag / h;
}

// Forward Automatic Differentiation
function FAD(f, x) {
    return f(new Dual(x, 1)).eps;
}

var correctText = document.getElementById('correct');
var functionSelector = document.getElementById('functionSelector');

var realH = document.getElementById('realH');
var real = document.getElementById('real');
var realError = document.getElementById('realError');
var realInput = document.getElementById('realInput');
realInput.addEventListener('input', function() {
    update(functionSelector.value);
});

var complexH = document.getElementById('complexH');
var complex = document.getElementById('complex');
var complexError = document.getElementById('complexError');
var complexInput = document.getElementById('complexInput');
complexInput.addEventListener('input', function() {
    update(functionSelector.value);
});

var fad = document.getElementById('fad');
var fadError = document.getElementById('fadError');

function update(i) {
    var f = functions[i];
    var x = points[i];
    document.querySelectorAll('span.point').forEach(function (pointText) {
        pointText.textContent = points[i];
    });
    correctText.textContent = correct[i];
    var correctValue = parseFloat(correct[i]);

    var rv = parseFloat(realInput.value);
    var rh = Math.pow(10, -rv);
    var ry = midpoint(f, x, rh);
    realH.textContent = rh;
    real.textContent = ry;
    realError.textContent = Math.abs(ry - correctValue);

    var cv = parseFloat(complexInput.value);
    var ch = Math.pow(10, -cv);
    var cy = complexStep(f, x, ch);
    complexH.textContent = ch;
    complex.textContent = cy;
    complexError.textContent = Math.abs(cy - correctValue);

    fad.textContent = FAD(f, x);
    fadError.textContent = Math.abs(FAD(f, x) - correctValue);

    functionSpans.forEach(function (span) {
        span.style = "display: none;";
    });
    functionSpans[i].style = "display: inline;";
}

update(functionSelector.value);

functionSelector.addEventListener('input', function() {
    update(functionSelector.value);
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>In terms of the general theory of partial differential equations, we are saying that |z^{-1}| is
a <a href="https://en.wikipedia.org/wiki/Green's_function">Green’s function</a> for |\nabla|. We can then understand everything that is happening
here in terms of general results. In particular, it is the two-dimensional case of the results described in
<a href="https://doi.org/10.1016/0022-247X(68)90002-4">Multivector Functions</a> by Hestenes.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>See
<a href="https://doi.org/10.1137/130931035">Numerical Algorithms based on Analytic Function Values at Roots of Unity</a> by Austin, Kravanja, and Trefethen (2014)
for an example. Also, with some minor tweaks, we can have that “point” be a matrix and these integrals can be used to calculate functions
of matrices, e.g. the square root, exponent, inverse, and log of a matrix. See
<a href="https://doi.org/10.1137/070700607">Computing |A^\alpha|, |\log(A)|, and Related Matrix Functions by Contour Integrals</a> by Hale, Higham, and Trefethen (2009)
for details.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>See
<a href="https://doi.org/10.1137/060659831">Is Gauss Quadrature Better than Clenshaw-Curtis?</a> by Trefethen (2008) for more details.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>While focused on issues that mostly come up with very high-order derivatives, e.g. |100|-th derivatives and higher,
<a href="https://arxiv.org/abs/0910.1841">Accuracy and Stability of Computing High-Order Derivatives of Analytic Functions by Cauchy Integrals</a>
by Bornemann (2009) nevertheless has a good discussions of the concerns here.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>If
we allowed arbitrary multivector-valued functions, then we’d need to add a projection producing the tangential derivative.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

    <div id="disqus_thread"></div>
    <script type="text/javascript">
        function loadDisqus() {
        const disqus_shortname = 'hedonisticlearning';
        const disqus_identifier = '5827b4b6-5400-4368-8861-53ecf33bf1ad';
        const disqus_title = 'Complex-Step Differentiation';

        (function() {
            const dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
        }
        const commentsBtn = document.createElement("button");
        commentsBtn.textContent = "Click to show comments";
        commentsBtn.addEventListener("click", () => loadDisqus());
        const disqusDiv = document.getElementById("disqus_thread");
        disqusDiv.append(commentsBtn);
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript> 
</div>

        </div><!-- /.blog-main -->
        
        
      </div><!-- /.row -->
    </div><!-- /.container -->

    <footer class="blog-footer">
      <p>Site generated by <a href="http://jaspervdj.be/hakyll">Hakyll</a></p>
      <p>
        <a href="#">Back to top</a>
      </p>
    </footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script type="text/javascript" src="../js/jquery.min.js"></script>
    <script type="text/javascript" src="../js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!--<script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>-->
    
    
    <script>
        MathJax = {
            loader: {
                load: ['ui/lazy', '[custom]/xypic.js'],
                paths: { custom: '/js' }
            },
            tex: {
                packages: { '[+]': ['xypic'] },
                inlineMath: [['|','|']],
                macros: window.extraMacros || {}
            }
        };
    </script>
    <script type="text/javascript" src="../js/MathJax-3/es5/tex-mml-chtml.js"></script>
  </body>
</html>
