<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Derek Elkins' Blog</title>
        <link>https://derekelkins.github.io</link>
        <description><![CDATA[Mostly math, physics, and CS]]></description>
        <atom:link href="https://derekelkins.github.io/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Wed, 30 Sep 2015 05:45:11 UT</lastBuildDate>
        <item>
    <title>The three styles of category theory</title>
    <link>https://derekelkins.github.io/posts/styles-of-category-theory.html</link>
    <description><![CDATA[<h3 id="introduction">Introduction</h3>
<p>Arguably (1-)category theory is the study of universal properties. There are three standard formulations of the notion of universal property which are all equivalent. Most introductions to category theory will cover all three and how they relate. By systematically preferring one formulation above the others, three styles of doing category theory arise with distinctive constructions and proof styles.</p>
<p>I noticed this trichotomy many years ago, but I haven’t heard anyone explicitly identify and compare these styles. Furthermore, in my opinion, one of these styles, the one I call <strong>Set</strong>-category theory, is significantly easier to use than the others, but seems poorly represented in introductions to category theory.</p>
<p>For myself, it wasn’t until I read <a href="http://www.tac.mta.ca/tac/reprints/articles/10/tr10abs.html">Basic Concepts of Enriched Category Theory</a> by G. M. Kelly and was introduced to indexed (co)limits (aka weighted (co)limits), that I began to recognize and appreciate <strong>Set</strong>-category theory. Indexed (co)limits are virtually absent from category theory introductions, and the closely related notion of (co)ends are also very weakly represented despite being far more useful than (conical) (co)limits. I don’t know why this is. Below I’ll be more focused on comparing the styles than illustrating the usefulness of indexed (co)limits. I may write an article about that in the future; in the mean time you can read “Basic Concepts of Enriched Category Theory” and just ignore the enriched aspect. Admittedly, it is definitely <em>not</em> an introduction to category theory.</p>
<!--more-->
<h3 id="universality">Universality</h3>
<p>What follows are three equivalent formulations of the notion of universal property including proofs of equivalence.</p>
<h4 id="initiality">Initiality</h4>
<p>Initiality is by far the simplest formulation of universal properties to understand because it is a very special case, albeit a special case that can be finagled to cover the general case.</p>
<p>An object, typically denoted |0|, is <strong>initial</strong> iff for every object in the category there exists a unique arrow from |0|. If |A| is an object in the category, we can write |(:A:) : 0 -&gt; A| for the unique arrow.</p>
<h4 id="representability">Representability</h4>
<p>Representability isn’t as simple or intuitive as initiality, but it is still pretty simple.</p>
<p>A functor |F : C^(op) -&gt; Set| is <strong>representable</strong> iff |F ~= Hom(-,Y)| for some |Y|. Note that this is an isomorphism between functors, i.e. a natural isomorphism. We say |Y| <strong>represents</strong> the functor |F|. A functor |G : C -&gt; Set| is <strong>co-representable</strong> iff |G ~= Hom(X,-)| for some |X|. Similarly we say |X| <strong>(co-)represents</strong> the functor |G|. Representable and co-representable aren’t actually different; if we set |D = C^(op)| then we could say |F| is co-representable as a functor from |D| and it would mean the same thing.</p>
<p>Now we need to prove initiality and (co-)representability equivalent. One direction is easy, |0| co-represents the functor |lambda C . {**}|.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> It’s easy to show that this says exactly that |0| is initial.</p>
<p>I’ll hold off on the other direction and instead prove the circular implication: <strong>Representability</strong> |=&gt;| <strong>Initiality</strong> |=&gt;| <strong>Universal Arrow</strong> |=&gt;| <strong>Representability</strong>. Right now we’ve done the first step; we can recast initiality as representability. Next we’ll show that we can recast universal arrows as initiality and representability as a universal arrow and we’ll have shown them all equivalent.</p>
<h4 id="universal-arrow">Universal arrow</h4>
<p>Let |U : C -&gt; D| be a functor. |eta : A -&gt; UX| is a <strong>universal arrow</strong> iff for all |f : A -&gt; UY| there exists a unique arrow |(:f:) : X -&gt; Y| such that |f = U(:f:) @ eta|. Dually, let |F : C -&gt; D| be a functor, then |epsilon : FX -&gt; A| is a <strong>co-universal arrow</strong> iff for all |g : FY -&gt; A| there exists a unique arrow |(:g:) : Y -&gt; X| such that |g = epsilon @ F(:g:)|. Again, these are equivalent notions. A co-universal arrow is just a universal arrow in the opposite category.</p>
<p>Now to prove <strong>Initiality</strong> |=&gt;| <strong>Universal Arrow</strong>. Consider the (comma) category whose objects are pairs |(f, Y)| where |f : A -&gt; UY| and whose arrows |k : (f, Y) -&gt; (g, Z)| are arrows |k : Y -&gt; Z| such that |g = Uk @ f|. |eta| is a universal arrow iff |(eta, X)| is initial in this category. This is easy to check.</p>
<p>Finally, <strong>Universal Arrow</strong> |=&gt;| <strong>Representability</strong>. Let |G : C -&gt; Set|. The claim is there exists a universal arrow <code class="asciimath">|eta : {**} -&gt; GX|</code> iff |X| co-represents |G|. Doing the backwards direction first, if |X| co-represents |G| then there is an element of |GX| that corresponds to |id : X -&gt; X|. Have |eta| pick out that element. Given another element |y : {**} -&gt; GY| we need to find a unique arrow |(:y:) : X -&gt; Y| such that |G(:y:)(eta) = y|. Naturality of the isomorphism making |X| co-represent |G| says: |f @ phi_A(a) = phi_B(Gf(a))| where |phi : G ~= Hom(X,-)|, |f : A -&gt; B|, and |a in GA|. Applying |phi_Y| to both sides of the earlier equation and using |phi_X(eta) = id| we get: |(:y:) = phi_Y(y)| showing that |(:y:)| exists and is unique and thus |eta| is a universal arrow.</p>
<p>Next we need to show that |eta : {**} -&gt; GX| being a universal arrow implies that |X| co-represents |G|. We immediately have a parameterized bijection namely |(: - :)_Y : GY ~= Hom(X,Y)|. All we need to do is show that it is natural in |Y| which is to say |f @ (:a:)_A = (:Gf(a):)_B| where |f : A -&gt; B|. We have |Gf(a) = Gf(G(:a:)(eta)) = (Gf @ G(:a:))(eta) = G(f @ (:a:))(eta)|. Using |f = (:Gf(eta):)_B| which is a consequence of |eta| being universal, we get |(:Gf(a):)_B = f @ (:a:)_A| as desired.</p>
<h3 id="initiality-1">Initiality</h3>
<p>In the initiality style of doing category theory, the main exercise is defining an appropriate category. This is pretty difficult from both a creativity perspective and a notational perspective. Once an appropriate category is defined, though, the proofs are typically not too hard. Perhaps the main tool is the comma category as that allows the compact and flexible specification of many categories, though it can often take a touch of creativity to recognize a category as a comma category. Technically, the proof above implies that for any universal property there’s an appropriate comma category in which the universal object is the initial object.</p>
<p>This style is often seen in introductions to category theory. If you’ve ever read about categories of (co)cones or wedges, then you’ve seen an example of this style with an ad-hoc definition of the relevant categories. In practice, the categories needed to characterize a specific universal property are of little interest on their own so the effort spent defining them isn’t repaid. The main exception is the category of |F|-algebras for some functor |F|. (Incidentally, the category of |F|-algebras is a full subcategory of a comma category.)</p>
<p>The style of proof is to leverage the uniqueness of the mediating morphism to equate two seemingly different things. Lambek’s lemma is an excellent illustration of this proof style.</p>
<h3 id="set-category-theory"><strong>Set</strong>-category theory</h3>
<p>The core of this style of category theory is representability and its parameterized variant. Representability alone is already very powerful and ergonomic. Every limit or colimit in a category represents a limit of homsets. This means all the tools and intuitions of set theory are available. This is the basis for my naming of this style: we elucidate the structure of a category by relating it to the structure of the category of sets.</p>
<p>The main tools of this style are (co)ends, indexed (co)limits, and adjunctions in the form |Hom(FA,B) ~= Hom(A,UB)|. In particular, the intimate connection between ends/indexed limits and sets of natural transformations significantly eases moving up and down functor categories.</p>
<p>By far the best part of this approach is the style of proof. By leveraging continuity properties, characterizations in terms of (parameterized) representability, some general isomorphisms, and occasionally some hand-crafted isomorphisms, nearly any categorical theorem can be proven in a dozen or less isomorphisms. The presentation looks and feels very much like solving an algebraic equation. The proofs are easy to read and easy to write.</p>
<p>The main drawback of this style is that most of the properties above weaken and some disappear when working in enriched or higher categories.</p>
<h3 id="cat-category-theory"><strong>Cat</strong>-category theory</h3>
<p>Arguably, this style should be based on universal arrows. While it technically is, in practice, it’s rare that one talks about universal arrows and instead the parameterized version, namely adjunctions in their unit/counit formulation are the main tool. After adjunctions would be Kan extensions. All of these are concepts that live naturally at the level of 2-categories which makes them much easier to generalize.</p>
<p>The main style of proof is diagram chasing, i.e. proving typically several equalities. On the one hand, proving an equation is straight-forward equational reasoning; on the other hand, there tend to be several equations that need to be proven at a time, they tend to be fine-grained and thus detailed and tedious, and often general results don’t obviously apply. The solution to most of these problems is to use string diagrams instead. Many of the bureaucratic details completely disappear and the proofs become crystal clear. A wonderful example of this style of proof is proving that an adjunction gives rise to a monad. With normal diagram chasing it’s mildly tedious. With string diagrams it proves itself.</p>
<p>One issue that comes up in this style is the elegant definitions tend to be wholesale. For example, there’s a nice adjunction that characterizes all limits of any given shape. Unfortunately, the adjunction only exists if all the limits of that shape exists. If they don’t all exist, then you need to use some other method to talk about the limits that do exist. Technically, universal arrows could handle this, but usually representability is used instead.</p>
<!-- ### Yoneda-->
<h3 id="conclusion">Conclusion</h3>
<p>Ultimately, one should become familiar with all these approaches. There are definitely problems that fit nicer in some than others. However, when it comes to universal properties the <strong>Set</strong>-category theory wins hands down. <strong>Cat</strong>-category theory tends to be more useful when working with structures that aren’t characterized by a universal property such as monoidal structure. Initiality has its niche with initial algebras. I don’t have any ideas on where it might also be a profitable perspective. In fact, I’m not completely sure it’s a good thing that we think about initial algebras as initial algebras. There are interesting interactions between initial algebras and adjunctions for example and it’s not clear to me that they wouldn’t be more accessible with a different presentation of initial algebras.</p>
<h3 id="texts-illustrating-these-approaches">Texts illustrating these approaches</h3>
<ul>
<li>Representatives of initiality:
<ul>
<li><a href="http://maartenfokkinga.github.io/utwente/mmf92b.pdf">A Gentle Introduction to Category Theory – the calculational approach –</a> by Maarten Fokkinga</li>
</ul></li>
<li>Representatives of <strong>Set</strong>-category theory:
<ul>
<li><a href="http://www.cs.ox.ac.uk/ralf.hinze/publications/WGP10.pdf">Reason Isomorphically!</a> by Ralf Hinze</li>
<li><a href="http://arxiv.org/abs/1501.02503">This is the (co)end, my only (co)friend</a> by Fosco Loregian</li>
<li><a href="http://www.tac.mta.ca/tac/reprints/articles/10/tr10abs.html">Basic Concepts of Enriched Category Theory</a> by G. M. Kelly</li>
</ul></li>
<li>Representatives of <strong>Cat</strong>-category theory:
<ul>
<li><a href="http://www.dcs.ed.ac.uk/home/dt/CT/categories.pdf">Category Theory Lecture Notes</a> by Daniele Turi</li>
</ul></li>
</ul>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>|{**}| is just some singleton set.<a href="#fnref1">↩</a></p></li>
</ol>
</div>]]></description>
    <pubDate>Wed, 30 Sep 2015 05:45:11 UT</pubDate>
    <guid>https://derekelkins.github.io/posts/styles-of-category-theory.html</guid>
    <dc:creator>Derek Elkins</dc:creator>
</item>
<item>
    <title>Differentiation under the integral</title>
    <link>https://derekelkins.github.io/posts/differentiation-under-the-integral.html</link>
    <description><![CDATA[<p><a href="../raw/DiffUnderIntegral.pdf">Here</a> is a PDF with an informal proof of a very general form of <a href="https://en.wikipedia.org/wiki/Differentiation_under_the_integral_sign">differentiation under the integral</a>.</p>
<p>It’s formulated using geometric algebra and provides a simple demonstration of using some of the basic identities in geometric calculus.</p>
<p>The end result is:</p>
<p><img src="../raw/diff-under-integral.svg" alt="d/(dt) int_(D(t)) L_t(x; d^m x) = int_(D(t)) dot L_t(x; (d^m x ^^ (del x)/(del t)) * dot grad_x) + int_(del D(t)) L_t(x; d^(m-1) x ^^ (del x)/(del t)) + int_(D(t))(del L_t(x; d^m x))/(del t)"></img></p>
<!--
> `#d/(dt) int_(cc"D"(t)) bb"L"_t(bb"x"; d^m bb"x") 
>    = int_(cc"D"(t)) dot bb"L"_t(bb"x"; (d^m bb"x" ^^ (del x)/(del t)) * dot grad_(bb"x"))#
>  # + int_(del cc"D"(t)) bb"L"_t(bb"x"; d^(m-1) bb"x" ^^ (del x)/(del t))#
>  # + int_(cc"D"(t))(del bb"L"_t(bb"x"; d^m bb"x"))/(del t)#`{.asciimath}
-->]]></description>
    <pubDate>Mon, 14 Sep 2015 07:53:29 UT</pubDate>
    <guid>https://derekelkins.github.io/posts/differentiation-under-the-integral.html</guid>
    <dc:creator>Derek Elkins</dc:creator>
</item>
<item>
    <title>A better way to write convolution</title>
    <link>https://derekelkins.github.io/posts/convolution.html</link>
    <description><![CDATA[<p>Normally discrete convolution is written as follows:</p>
<blockquote>
<p><code class="asciimath">#(f ** g)(n) = sum_(k=0)^n f(k)g(n-k)#</code></p>
</blockquote>
<p>It is not immediately obvious from this that #f ** g = g ** f#. Consider this alternate representation:</p>
<blockquote>
<p><code class="asciimath">#(f ** g)(n) = sum_(j+k=n) f(j)g(k)#</code></p>
</blockquote>
<p>This formula is obviously symmetric in #f# and #g# and it clarifies an important invariant. For example, to multiply two polynomials (or formal power series) we convolve their coefficients. This looks like:</p>
<p>Let <code class="asciimath">#P(x) = sum_k a_k x^k# and #Q(x) = sum_k b_k x^k#</code> then</p>
<blockquote>
<p><code class="asciimath">#P(x)Q(x) = sum_(j+k=n) a_j b_k x^n#</code></p>
</blockquote>
<p>This emphasizes that the #n#th coefficient is the product of the coefficients whose degrees sum to #n#. (You may recognize this as the convolution theorem.)</p>
<!--more-->
<p>There is one subtle difference. In this alternate representation, it very much matters what the domain of #j# and #k# are. If #j# and #k# are naturals, we get what we started with. But if they are integers, then we have:</p>
<blockquote>
<p><code class="asciimath">#sum_(j+k=n) f(j)g(k) = sum_(k=-oo)^oo f(k)g(n-k)#</code></p>
</blockquote>
<p>In many combinatorial situations this makes no difference and may even be preferable. If, however, you are doing something like a short-time Fourier transform, then you probably don’t want to include an infinite number of entries (but you probably aren’t indexing by #bbbZ#). Personally, I view this as a feature; you should be clear about the domain of your variables.</p>
<p>Let’s prove that #**# is associative, i.e. <code class="asciimath">#((f ** g) ** h)(n) = (f ** (g ** h))(n)#</code>. Expanding the left hand side we get:</p>
<blockquote>
<p><code class="asciimath">#sum_(m+k=n) sum_(i+j=m) f(i)g(j)h(k)# #= sum_(i+j+k=n) f(i)g(j)h(k)# #= sum_(i+m=n) sum_(j+k=m) f(i)g(j)h(k)#</code></p>
</blockquote>
<p>and we’re done. It’s clear the associativity of convolution comes from the associativity of addition and distributivity. In fact, the commutativity also came from the commutativity of addition. This suggests a simple but broad generalization. Simply replace the #+# in the indexing with any binary operation, i.e.:</p>
<blockquote>
<p><code class="asciimath">#(f ** g)(n) = sum_(j o+ k=n) f(j)g(k)#</code></p>
</blockquote>
<p>Usually #o+# is taken to be an operation of a group, and this makes sense when you need to support the equivalent of #n-k#, but there’s really nothing keeping it from being a monoid, and the unit isn’t doing anything so why not a semigroup, and we only really need associativity if we want the convolution to be associative, and nothing requires #n# to be the same type as #j# or #k# or for them to be the same type as each other for that matter… Of course having a group structure reduces the often sticky problem of factoring #n# into all the pairs of #j# and #k# such that #j o+ k = n# to the problem of “simply” enumerating the group.</p>
<p>Here’s some code for the free monoid (i.e. lists) case:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">import </span><span class="dt">Data.Monoid</span>
<span class="kw">import </span><span class="dt">Data.List</span>

<span class="kw">class</span> <span class="dt">Monoid</span> m <span class="ot">=&gt;</span> <span class="dt">CommutativeMonoid</span> m
<span class="kw">instance</span> <span class="dt">Num</span> a <span class="ot">=&gt;</span> <span class="dt">CommutativeMonoid</span> (<span class="dt">Sum</span> a)

<span class="co">-- Overly generalized convolve</span>
<span class="ot">ogconvolve ::</span> (<span class="dt">CommutativeMonoid</span> m) <span class="ot">=&gt;</span> (a <span class="ot">-&gt;</span> b <span class="ot">-&gt;</span> m) <span class="ot">-&gt;</span> (n <span class="ot">-&gt;</span> [(j,k)]) <span class="ot">-&gt;</span> (j <span class="ot">-&gt;</span> a) <span class="ot">-&gt;</span> (k <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> n <span class="ot">-&gt;</span> m
ogconvolve mul factor f g n <span class="fu">=</span> mconcat (map (uncurry (\j k <span class="ot">-&gt;</span> mul (f j) (g k))) (factor n))

<span class="ot">gconvolve ::</span> <span class="dt">Num</span> m <span class="ot">=&gt;</span> (n <span class="ot">-&gt;</span> [(n,n)]) <span class="ot">-&gt;</span> (n <span class="ot">-&gt;</span> m) <span class="ot">-&gt;</span> (n <span class="ot">-&gt;</span> m) <span class="ot">-&gt;</span> n <span class="ot">-&gt;</span> m
gconvolve factor f g n <span class="fu">=</span> getSum (ogconvolve (\a b <span class="ot">-&gt;</span> <span class="dt">Sum</span> (a<span class="fu">*</span>b)) factor f g n)

<span class="ot">lconvolve ::</span> <span class="dt">Num</span> m <span class="ot">=&gt;</span> ([a] <span class="ot">-&gt;</span> m) <span class="ot">-&gt;</span> ([a] <span class="ot">-&gt;</span> m) <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> m
lconvolve <span class="fu">=</span> gconvolve (\as <span class="ot">-&gt;</span> zip (inits as) (tails as))</code></pre></div>
<p>You may have noticed that we never actually use #o+# anywhere. It’s only significance is to define the valid factorizations. I.e. there’s an implicit constraint in the above code that</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">all (\(j,k) <span class="ot">-&gt;</span> n <span class="fu">==</span> j <span class="ot">`oplus`</span> k) (factor n)</code></pre></div>
<p>and also that</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">factor n <span class="fu">==</span> nub (factor n)</code></pre></div>
<p>i.e. no duplicates. (Conceptually, factor spits out a set, indeed the graph of the relation #(j,k)|-&gt;j o+ k=n#.)</p>
<p>The commutative monoid constraint and <code>mul</code> function were just for the heck of it, but why a commutative monoid and not an arbitrary monoid? Well, we don’t want the result to depend on how <code>factor</code> spits out the factors. In other words, if it actually did return a set then we can’t depend on the order of the elements in the set if we want a well-defined function.</p>
<p>Here’s where I tell you that <code>lconvolve</code> is an important function for something. I suspect it or a variant probably is, but I don’t know. Here’s another monoid case, commutative this time, that definitely is important and interesting: <a href="https://en.wikipedia.org/wiki/Dirichlet_convolution">Dirichlet convolution</a>.</p>
<p>Here’s the typical bad definition:</p>
<blockquote>
<p><code class="asciimath">#(f *** g)(n) = sum_(d|n) f(d)g(n/d)#</code> where #d|n# means #d# evenly divides #n#.</p>
</blockquote>
<p>These are the coefficients of the product of two Dirichlet series. For example,</p>
<blockquote>
<p><code class="asciimath">#F(s) = sum_(n=1)^oo f(n)n^(-s) \quad G(s) = sum_(n=1)^oo g(n)n^(-s)#</code> <code class="asciimath">#\quad F(s)G(s) = sum_(n=1)^oo (f *** g)(n)n^(-s)#</code></p>
</blockquote>
<p>We again see a situation where the correspondence doesn’t just jump out at you (or at least it didn’t to me originally) until you realize the above sum can be written:</p>
<blockquote>
<p><code class="asciimath">#(f *** g)(n) = sum_(ab=n) f(a)g(b)#</code></p>
</blockquote>
<p>then it’s pretty easy to see that it is right. We can start doing a lot of number theory with the above combined with the <a href="https://en.wikipedia.org/wiki/Euler_product">Euler product</a> formula:</p>
<blockquote>
<p><code class="asciimath">#sum_(n=1)^oo f(n)n^(-s) = prod_(p\ &quot;prime&quot;) sum_(n=0)^oo f(p^n)p^(-ns)#</code></p>
</blockquote>
<p>where #f# is multiplicative, which means #f(ab) = f(a)f(b)# when #a# and #b# are coprime. (The real significance of #f# being multiplicative is that it is equivalent to #f# being completely determined by its values on prime powers.) It turns out it is surprisingly easy and fun to derive various <a href="https://en.wikipedia.org/wiki/Arithmetic_function">arithmetic functions</a> and identities between them. Here’s a brief teaser.</p>
<p>#1(n) = 1# is a multiplicative function and this gives:</p>
<blockquote>
<p><code class="asciimath">#zeta(s) = sum_(n=1)^oo n^(-s) = prod_(p\ &quot;prime&quot;) 1/(1-p^(-s))#</code></p>
</blockquote>
<p>where we’ve used the formula for the sum of a geometric series.</p>
<p>So <code class="asciimath">#1/(zeta(s)) = prod_(p\ &quot;prime&quot;) 1-p^(-s) = sum_(n=1)^oo mu(n)n^(-s)#</code>. The question is now, what is #mu#? Well, all we need to do is say what it is on prime powers and comparing the Euler product formula to the above we see, for prime #p#, #mu(p) = -1# and #mu(p^n) = 0# for #n &gt; 1#. This is the <a href="https://en.wikipedia.org/wiki/M%C3%B6bius_function">Möbius function</a>. Because #zeta(s) * (1/(zeta(s))) = 1# we have #(1 *** mu)(n) = sum_(d|n) mu(d) = 0# for #n &gt; 1#. Continuing in this vein leads to analytic number theory and the Riemann hypothesis, though you may want to pick up the <a href="https://en.wikipedia.org/wiki/Mellin_transform">Mellin transform</a> along the way.</p>]]></description>
    <pubDate>Mon, 14 Sep 2015 06:12:26 UT</pubDate>
    <guid>https://derekelkins.github.io/posts/convolution.html</guid>
    <dc:creator>Derek Elkins</dc:creator>
</item>
<item>
    <title>First post</title>
    <link>https://derekelkins.github.io/posts/first-post.html</link>
    <description><![CDATA[<p>I finally made a blog. We’ll see how it goes. Right now I’m in a kind of mathy mood so the coming posts will probably be pretty theoretical/mathematical. I do have a list of topics I’d like to write about, so I should be producing something for a while. If there’s something you’d like me to talk about, email me.</p>]]></description>
    <pubDate>Mon, 14 Sep 2015 06:12:11 UT</pubDate>
    <guid>https://derekelkins.github.io/posts/first-post.html</guid>
    <dc:creator>Derek Elkins</dc:creator>
</item>

    </channel>
</rss>
