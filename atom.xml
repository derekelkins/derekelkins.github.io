<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Hedonistic Learning</title>
    <link href="https://derekelkins.github.io/atom.xml" rel="self" />
    <link href="https://derekelkins.github.io" />
    <id>https://derekelkins.github.io/atom.xml</id>
    <author>
        <name>Derek Elkins</name>
        <email>derek.a.elkins+blog@gmail.com</email>
    </author>
    <updated>2020-07-06T05:03:27Z</updated>
    <entry>
    <title>Enriched Indexed Categories, Syntactically</title>
    <link href="https://derekelkins.github.io/posts/enriched-indexed-categories-syntactically.html" />
    <id>https://derekelkins.github.io/posts/enriched-indexed-categories-syntactically.html</id>
    <published>2020-07-05 22:03:27-07:00</published>
    <updated>2020-07-06T05:03:27Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>This is part 3 in a series. See <a href="/posts/internal-language-of-indexed-monoidal-categories.html">the previous part about internal languages for indexed monoidal categories</a> upon which this part heavily depends.</p>
<p>In category theory, the hom-sets between two objects can often be equipped with some extra structure which is respected by identities and composition. For example, the set of group homomorphisms between two abelian groups is itself an abelian group by defining the operations pointwise. Similarly, the set of monotonic functions between two partially ordered sets (posets) is a poset again by defining the ordering pointwise. Linear functions between vector spaces form a vector space. The set of functors between small categories is a small category. Of course, the structure on the hom-sets can be different than the objects. Trivially, with the earlier examples a vector space is an abelian group, so we could say that linear functions form an abelian group instead of a vector space. Likewise groups are monoids. Less trivially, the set of relations between two sets is a partially ordered set via inclusion. There are many cases where instead of hom-sets we have hom-objects that aren’t naturally thought of as sets. For example, we can have hom-objects be non-negative (extended) real numbers from which the category laws become the laws of a generalized metric space. We can identify posets with categories who hom-objects are elements of a two element set or, even better, a two element poset with one element less than or equal to the other.</p>
<p>This general process is called <a href="https://ncatlab.org/nlab/show/enriched+category+theory">enriching</a> a category in some other category which is almost always called |\newcommand{\V}{\mathcal V}\V| in the generic case. We then talk about having |\V|-categories and |\V|-functors, etc. In a specific case, it will be something like |\mathbf{Ab}|-categories for an |\mathbf{Ab}|-enriched category, where |\mathbf{Ab}| is the category of abelian groups. Unsurprisingly, not just <em>any</em> category will do for |\V|. However, it turns out very little structure is needed to define a notion of |\V|-category, |\V|-functor, |\V|-natural transformation, and |\V|-profunctor. The usual “baseline” is that |\V| is a <a href="https://ncatlab.org/nlab/show/monoidal+category">monoidal category</a>. As mentioned in the previous post, paraphrasing Bénabou, notions of “families of objects/arrows” are ubiquitous and fundamental in category theory. It is useful for our purposes to make this structure explicit. For very little cost, this will also provide a vastly more general notion that will readily capture enriched categories, <a href="https://ncatlab.org/nlab/show/indexed+category">indexed categories</a>, and categories that are simultaneously indexed and enriched, of which <a href="https://ncatlab.org/nlab/show/internal+category">internal categories</a> are an example. The tool for this is a <a href="https://ncatlab.org/nlab/show/Grothendieck+fibration">(Grothendieck) fibration</a> aka a fibered category or the mostly equivalent concept of an indexed category.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>To that end, instead of just a monoidal category, we’ll be using <a href="https://ncatlab.org/nlab/show/indexed+monoidal+category">indexed monoidal categories</a>. Typically, to get an experience as much like ordinary category theory as possible, additional structure is assumed on |\V|. In particular, it is assumed to be an <a href="https://ncatlab.org/nlab/show/cosmos#indexed_bnabou_cosmoi">(indexed) cosmos</a> which means that it is an indexed symmetric monoidally closed category with indexed coproducts preserved by |\otimes| and indexed products and fiberwise finite limits and colimits (preserved by the indexed structure). This is quite a lot more structure which I’ll introduce in later parts. In this part, I’ll make no assumptions beyond having an indexed monoidal category.</p>
<!--more-->
<h2 id="basic-category-theory-in-indexed-monoidal-categories">Basic Category Theory in Indexed Monoidal Categories</h2>
<p>The purpose of the machinery of the previous posts is to make this section seem boring and pedestrian. Other than being a little more explicit and formal, for most of the following concepts it will look like we’re restating the usual definitions of categories, functors, and natural transformations. The main exception is profunctors which will be presented in a quite different manner, though still in a manner that is easy to connect to the usual presentation. (We will see how to recover the usual presentation in later parts.)</p>
<p>While I’ll start by being rather explicit about indexes and such, I will start to suppress that detail over time as most of it is inferrable. One big exception is that right from the start I’ll omit the explicit dependence of primitive terms on indexes. For example, while I’ll write |\mathsf F(\mathsf{id}) = \mathsf{id}| for the first functor law, what the syntax of the previous posts says I should be writing is |\mathsf F(s, s; \mathsf{id}(s)) = \mathsf{id}(s)|.</p>
<p>To start, I want to introduce two different notions of |\V|-category, small |\V|-categories and large |\V|-categories, and talk about what this distinction actually means. I will proceed afterwards with the “large” notions, e.g. |\V|-functors between large |\V|-categories, as the small case will be an easy special case.</p>
<h3 id="small-v-categories">Small |\V|-categories</h3>
<p>The <strong>theory of a small |\V|-category</strong> consists of:</p>
<ul>
<li>an index type |\newcommand{\O}{\mathsf O}\O|,</li>
<li>a linear type |\newcommand{\A}{\mathsf A}s, t : \O \vdash \A(t, s)|,</li>
<li>a linear term |s : \O; \vdash \mathsf{id} : \A(s, s)|, and</li>
<li>a linear term |s, u, t : \O; g : \A(t, u), f : \A(u, s) \vdash g \circ f : \A(t, s)|</li>
</ul>
<p>satisfying <br /><span class="math display">$$\begin{gather}
s, t : \O; f : \A(t, s) \vdash \mathsf{id} \circ f = f = f \circ \mathsf{id} : \A(t, s)\qquad \text{and} \\ \\
s, u, v, t : \O; h : \A(t, v), g : \A(v, u), f : \A(u, s) \vdash h \circ (g \circ f) = (h \circ g) \circ f : \A(t, s)
\end{gather}$$</span><br /></p>
<p>In the notation of the previous posts, I’m saying |\O : \mathsf{IxType}|, |\A : (\O, \O) \to \mathsf{Type}|, |\mathsf{id} : (s : \O;) \to \A(s, s)|, and |\circ : (s, u, t : \O; \A(t, u), \A(u, s)) \to \A(t, s)| are primitives added to the signature of the theory. I’ll continue to use the earlier, more pointwise presentation above to describe the signature.</p>
<p>A <strong>small |\V|-category</strong> for an |\mathbf S|-indexed monoidal category |\V| is then an interpretation of this theory. That is, an object |O| of |\mathbf S| as the interpretation of |\O|, and an object |A| of |\V^{O\times O}| as the interpretation of |\A|. The interpretation of |\mathsf{id}| is an arrow |I_O \to \Delta_O^* A| of |\V^O|, where |\Delta_O : O \to O\times O| is the diagonal arrow |\langle id, id\rangle| in |\mathbf S|. The interpretation of |\circ| is an arrow |\pi_{23}^* A \otimes \pi_{12}^* A \to \pi_{13}^* A| of |\V^{O\times O \times O}| where |\pi_{ij} : X_1 \times X_2 \times X_3 \to X_i \times X_j| are the appropriate projections.</p>
<p>Since we can prove in the internal language that the choice of |()| for |\O|, |s, t : () \vdash I| for |\A|, |s, t : (); x : I \vdash x : I| for |\mathsf{id}|, and |s, u, t : (); f : I, g: I \vdash \mathsf{match}\ f\ \mathsf{as}\ *\ \mathsf{in}\ g : I| for |\circ| satisfies the laws of the theory of a small |\V|-category, we know we have a |\V|-category which I’ll call |\mathbb I| for any |\V|.</p>
<p>For |\V = \mathcal Fam(\mathbf V)|, |O| is a set of objects. |A| is an |(O\times O)|-indexed family of objects of |\mathbf V| which we can write |\{A(t,s)\}_{s,t\in O}|. The interpretation of |\mathsf{id}| is an |O|-indexed family of arrows of |\mathbf V|, |\{ id_s : I_s \to A(s, s) \}_{s\in O}|. Finally, the interpretation of |\circ| is a family of arrows of |\mathbf V|, |\{ \circ_{s,u,t} : A(t, u)\otimes A(u, s) \to A(t, s) \}_{s,u,t \in O}|. This is exactly the data of a (small) |\mathbf V|-enriched category. One example is when |\mathbf V = \mathbf{Cat}| which produces (strict) |2|-categories.</p>
<p>For |\V = \mathcal Self(\mathbf S)|, |O| is an object of |\mathbf S|. |A| is an arrow of |\mathbf S| into |O\times O|, i.e. an object of |\mathbf S/O\times O|. I’ll write the object part of this as |A| as well, i.e. |A : A \to O\times O|. The idea is that the two projections are the target and source of the arrow. The interpretation of |\mathsf{id}| is an arrow |ids : O \to A| such that |A \circ ids = \Delta_O|, i.e. the arrow produced by |ids| should have the same target and source. Finally, the interpretation of |\circ| is an arrow |c| from the pullback of |\pi_2 \circ A| and |\pi_1 \circ A| to |A|. The source of this arrow is the object of composable pairs of arrows. We further require |c| to produce an arrow with the appropriate target and source of the composable pair. This is exactly the data for a category internal to |\mathbf S|. An interesting case for contrast with the previous paragraph is that a category internal to |\mathbf{Cat}| is a double category.</p>
<p>For |\V = \mathcal Const(\mathbf V)|, the above data is exactly the data of a <a href="https://ncatlab.org/nlab/show/monoid+object">monoid object</a> in |\mathbf V|. This is a formal illustration that a (|\mathbf V|-enriched) category is just an “indexed monoid”. Indeed, |\mathcal Const(\mathbf V)|-functors will be monoid homomorphisms and |\mathcal Const(\mathbf V)|-profunctors will be double-sided monoid actions. In particular, when |\mathbf V = \mathbf{Ab}|, we get rings, ring homomorphisms, and bimodules of rings. The intuitions here are the guiding ones for the construction we’re realizing.</p>
<p>One aspect of working in a not-necessarily-symmetric (indexed) monoidal category is the choice of the standard order of composition or diagrammatic order is not so trivial since it is not possible to even state what it means for them to be the equivalent. To be clear, this definition isn’t really taking a stance on the issue. We can interpret |\A(t, s)| as the type of arrows |s \to t| and then |\circ| will be the standard order of composition, or as the type of arrows |t \to s| and then |\circ| will be the diagrammatic order. In fact, there’s nothing in this definition that stops us from having |\A(t, s)| being the type of arrows |t \to s| while still having |\circ| be standard order composition as usual. The issue comes up only once we consider |\V|-profunctors as we will see.</p>
<h3 id="large-v-categories">Large |\V|-categories</h3>
<p>A <strong>large |\V|-category</strong> is a model of a theory of the following form. There is</p>
<ul>
<li>a collection of index types |\O_x|,</li>
<li>for each pair of index types |\O_x| and |\O_y|, a linear type |s : \O_x, t : \O_y \vdash \A_{yx}(t, s)|,</li>
<li>for each index type |\O_x|, a linear term |s : \O_x; \vdash \mathsf{id}_x : \A_{xx}(s, s)|, and</li>
<li>for each triple of index types, |\O_x|, |\O_y|, and |\O_z|, a linear term |s: \O_x, u : \O_y, t : \O_z; g : \A_{zy}(t, u), f : \A_{yx}(u, s) \vdash g \circ_{xyz} f : \A_{zx}(t, s)|</li>
</ul>
<p>satisfying the same laws as small |\V|-categories, just with some extra subscripts. Clearly, a small |\V|-category is just a large |\V|-category where the collection of index types consists of just a single index type.</p>
<h3 id="small-versus-large">Small versus Large</h3>
<p>The typical way of describing the difference between small and large (|\V|-)categories would be to say something like: “By having a collection of index types in a large |\V|-category, we can have a proper class of them. In a small |\V|-category, the index type of objects is interpreted as an object in a category, and a proper class can’t be an object of a category<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.” However, for us, there’s a more directly relevant distinction. Namely, while we had a single theory of small |\V|-categories, there is no single theory of large |\V|-categories. Different large |\V|-categories correspond to models of (potentially) different theories. In other words, the notion of a small |\V|-category is able to be captured by our notion of theory but not the concept of a large |\V|-category. This extends to |\V|-functors, |\V|-natural transformations, and |\V|-profunctors. In the small case, we can define a single theory which captures each of these concepts, but that isn’t possible in the large case. In general, notions of “large” and “small” are about what we can internalize within the relevant object language, usually a set theory. Arguably, the only reason we speak of “size” and of proper classes being “large” is that the <a href="https://en.wikipedia.org/wiki/Axiom_schema_of_specification">Axiom of Specification</a> outright states that any subclass of a set is a set, so proper classes in <strong>ZFC</strong> can’t be subsets of any set. As I’ve mentioned <a href="/posts/finite.html">elsewhere</a>, you can definitely have set theories with proper classes that are contained in even finite sets, so the issue isn’t one of “bigness”.</p>
<p>The above discussion also explains the hand-wavy word “collection”. The collection is a collection in the meta-language in which we’re discussing/formalizing the notion of theory. When working <em>within</em> the theory of a particular large |\V|-category, all the various types and terms are just available ab initio and are independent. There is no notion of “collection of types” within the theory and nothing indicating that some types are part of a “collection” with others.</p>
<p>Another perspective on this distinction between large and small |\V|-categories is that small |\V|-categories have a <em>family</em> of arrows, identities, and compositions with respect to the notion of “family” represented by our internal language. If we hadn’t wanted to bother with formulating the internal language of an <em>indexed</em> monoidal category, we could have still defined the notion of |\V|-category with respect to the internal language of a (non-indexed) monoidal category. It’s just that all such |\V|-categories (except for monoid objects) would have to be large |\V|-categories. That is, the indexing and notion of “family” would be at a meta-level. Since most of the |\V|-categories of interest will be large (though, generally a special case called a |\V|-fibration which reins in the size a bit), it may seem that there was no real benefit to the indexing stuff. Where it comes in, or rather where small |\V|-categories come in, is that our notion of (co)complete means “has all (co)limits of <em>small</em> diagrams” and small diagrams are |\V|-functors from small |\V|-categories.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> There are several other places, e.g. the notion of presheaf, where we implicitly depend on what we mean by “small |\V|-category”. So while we won’t usually be focused on small |\V|-categories, which |\V|-categories are small impacts the structure of the whole theory.</p>
<h3 id="v-functors">|\V|-functors</h3>
<p>The formulation of |\V|-functors is straightforward. As mentioned before, I’ll only present the “large” version.</p>
<p>Formally, we can’t formulate a theory of just a |\V|-functor, but rather we need to formulate a theory of “a pair of |\V|-categories and a |\V|-functor between them”.</p>
<p>A <strong>|\V|-functor between (large) |\V|-categories |\mathcal C| and |\mathcal D|</strong> is a model of a theory consisting of a theory of a large |\V|-category, of which |\mathcal C| is a model, and a theory of a large |\V|-category which I’ll write with primes, of which |\mathcal D| is a model, and model of the following additional data:</p>
<ul>
<li>for each index type |\O_x|, an index type |\O’_{F_x}| and an index term, |s : \O_x \vdash \mathsf F_x(s) : \O’_{F_x}|, and</li>
<li>for each pair of index types, |\O_x| and |\O_y|, a linear term |s : \O_x, t : \O_y; f : \A_{yx}(t, s) \vdash \mathsf F_{yx}(f) : \A’_{F_yF_x}(\mathsf F_y(t), \mathsf F_x(s))|</li>
</ul>
<p>satisfying <br /><span class="math display">$$\begin{gather}
s : \O_x; \vdash \mathsf F_{xx}(\mathsf{id}_x) = \mathsf{id}'_{F_x}: \A'_{F_xF_x}(F_x(s), F_x(s))\qquad\text{and} \\
s : \O_x, u : \O_y, t : \O_z; g : \A_{zy}(t, u), f : \A_{yx}(u, s)
    \vdash \mathsf F_{zx}(g \circ_{xyz} f) = F_{zy}(g) \circ'_{F_xF_yF_z} F_{yx}(f) : \A'_{F_zF_x}(F_z(t), F_x(s))
\end{gather}$$</span><br /></p>
<p>The assignment of |\O’_{F_x}| for |\O_x| is, again, purely metatheoretical. From within the theory, all we know is that we happen to have some index types named |\O_x| and |\O’_{F_x}| and some data relating them. The fact that there is some kind of mapping of one to the other is not part of the data.</p>
<p>Next, I’ll define <strong>|\V|-natural transformations</strong> . As before, what we’re really doing is defining |\V|-natural transformations as a model of a theory of “a pair of (large) |\V|-categories with a pair of |\V|-functors between them and a |\V|-natural transformation between those”. As before, I’ll use primes to indicate the types and terms of the second of each pair of subtheories. Unlike before, I’ll only mention what is added which is:</p>
<ul>
<li>for each index type |\O_x|, a linear term |s : \O_x; \vdash \tau_x : \A’_{F’_xF_x}(\mathsf F’_x(s), \mathsf F_x(s))|</li>
</ul>
<p>satisfying <br /><span class="math display">$$\begin{gather}
s : \O_x, t : \O_y; f : \A_{yx}(t, s)
    \vdash \mathsf F'_{yx}(f) \circ'_{F_xF'_xF'_y} \tau_x = \tau_y \circ'_{F_xF_yF'_y} \mathsf F_{yx}(f) : \A'_{F'_yF_x}(\mathsf F'_y(t), \mathsf F_x(s))
\end{gather}$$</span><br /></p>
<p>In practice, I’ll suppress the subscripts on all but index types as the rest are inferrable. This makes the above equation the much more readable <br /><span class="math display">$$\begin{gather}
s : \O_x, t : \O_y; f : \A(t, s)
    \vdash \mathsf F'(f) \circ' \tau = \tau \circ' \mathsf F(f) : \A'(\mathsf F'(t), \mathsf F(s))
\end{gather}$$</span><br /></p>
<h3 id="v-profunctors">|\V|-profunctors</h3>
<p>Here’s where we need to depart from the usual story. In the usual story, a |\mathbf V|-enriched profunctor |\newcommand{\proarrow}{\mathrel{-\!\!\!\mapsto}} P : \mathcal C \proarrow \mathcal D| is a |\mathbf V|-enriched functor |P : \mathcal C\otimes\mathcal D^{op}\to\mathbf V| (or, often, the opposite convention is used |P : \mathcal C^{op}\otimes\mathcal D \to \mathbf V|). There are many problems with this definition in our context.</p>
<ol type="1">
<li>Without symmetry, we have no definition of opposite category.</li>
<li>Without symmetry, the tensor product of |\mathbf V|-enriched categories doesn’t make sense.</li>
<li>|\mathbf V| is not itself a |\mathbf V|-enriched category, so it doesn’t make sense to talk about |\mathbf V|-enriched functors into it.</li>
<li>Even if it was, we’d need some way of converting between arrows of |\mathbf V| as a category and arrows of |\mathbf V| as a |\mathbf V|-enriched category.</li>
<li>The equation |P(g \circ f, h \circ k) = P(g, k) \circ P(f, h)| requires symmetry. (This is arguably 2 again.)</li>
</ol>
<p>All of these problems are solved when |\mathbf V| is a symmetric monoidally closed category.</p>
<p>Alternatively, we can reformulate the notion of a |\V|-profunctor so that it works in our context and is equivalent to the usual one when it makes sense.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> To this end, at a low level a |\mathbf V|-enriched profunctor is a family of arrows <br /><span class="math display">$$\begin{gather}P : \mathcal C(t, s)\otimes\mathcal D(s', t') \to [P(s, s'), P(t, t')]\end{gather}$$</span><br /> which satisfies <br /><span class="math display">$$\begin{gather}P(g \circ f, h \circ k)(p) = P(g, k)(P(f, h)(p))\end{gather}$$</span><br /> in the internal language of a symmetric monoidally closed category among other laws. We can uncurry |P| to eliminate the need for closure, getting <br /><span class="math display">$$\begin{gather}P : \mathcal C(t, s)\otimes \mathcal D(s', t')\otimes P(s, s') \to P(t, t')\end{gather}$$</span><br /> satisfying <br /><span class="math display">$$\begin{gather}P(g \circ f, h \circ k, p) = P(g, k, P(f, h, p))\end{gather}$$</span><br /> We see that we’re always going to need to permute |f| and |h| past |k| unless we move the third argument to the second producing the nice <br /><span class="math display">$$\begin{gather}P : \mathcal C(t, s)\otimes P(s, s') \otimes \mathcal D(s', t') \to P(t, t')\end{gather}$$</span><br /> and the law <br /><span class="math display">$$\begin{gather}P(g \circ f, p, h \circ k) = P(g, P(f, p, h), k)\end{gather}$$</span><br /> which no longer requires symmetry. This is also where the order of the arguments of |\circ| drives the order of the arguments of |\V|-profunctors.</p>
<p>A <strong>|\V|-profunctor</strong>, |P : \mathcal C \proarrow \mathcal D|, is a model of the theory (containing subtheories for |\mathcal C| and |\mathcal D| etc. as in the |\V|-functor case) having:</p>
<ul>
<li>for each pair of index types |\O_x| and |\O’_{x’}|, a linear type |s : \O_x, t : \O’_{x’} \vdash \mathsf P_{x’x}(t, s)|, and</li>
<li>for each quadruple of index types |\O_x|, |\O_y|, |\O’_{x’}|, and |\O’_{y’}|, a linear term |s : \O_x, s’ : \O’_{x’}, t : \O_y, t’ : \O’_{y’}; f : \A_{yx}(t, s), p : \mathsf P_{xx’}(s, s’), h : \A’_{x’y’}(s’, t’) \vdash \mathsf P_{yxx’y’}(f, p, h) : \mathsf P_{yy’}(t, t’)|</li>
</ul>
<p>satisfying <br /><span class="math display">$$\begin{align}
 &amp; s : \O_x, s' : \O'_{x'}; p : \mathsf P(s, s')
    \vdash \mathsf P(\mathsf{id}, p, \mathsf{id}') = p : \mathsf P(s, s') \\ \\
 &amp; s : \O_x, s' : \O'_{x'}, u : \O_y, u' : \O'_{y'}, t : \O_z, t' : \O'_{z'}; \\
 &amp;  g : \A(t, u), f : \A(u, s), p : \mathsf P(s, s'), h : \A'(s', u'), k : \A'(u', t') \\
    \vdash\  &amp; \mathsf P(g \circ f, p, h \circ' k) = \mathsf P(g, \mathsf P(f, p, h), k) : \mathsf P(t, t')
\end{align}$$</span><br /></p>
<p>This can also be equivalently presented as a pair of a left and a right action satisfying <a href="https://en.wikipedia.org/wiki/Bimodule#Definition">bimodule laws</a>. We’ll make the following definitions |\mathsf P_l(f, p) = \mathsf P(f, p, \mathsf {id})| and |\mathsf P_r(p, h) = \mathsf P(\mathsf{id}, p ,h)|.</p>
<p>A <strong>|\V|-presheaf</strong> on |\mathcal C| is a |\V|-profunctor |P : \mathbb I \proarrow \mathcal C|. Similarly, a <strong>|\V|-copresheaf</strong> on |\mathcal C| is a |\V|-profunctor |P : \mathcal C \proarrow \mathbb I|.</p>
<p>Of course, we have the fact that the term <br /><span class="math display">$$\begin{gather}
s : \O_x, t : \O_y, s' : \O_z, t' : \O_w; h : \A(t, s), g : \A(s, s'), f : \A(s', t')
    \vdash h \circ g \circ f : \A(t, t')
\end{gather}$$</span><br /> witnesses the interpretation of |\A| as a |\V|-profunctor |\mathcal C \proarrow \mathcal C| for any |\V|-category, |\mathcal C|, which we’ll call the <strong>hom |\V|-profunctor</strong>. More generally, given a |\V|-profunctor |P : \mathcal C \proarrow \mathcal D|, and |\V|-functors |F : \mathcal C’ \to \mathcal C| and |F’ : \mathcal D’ \to \mathcal D|, we have the |\V|-profunctor |P(F, F’) : \mathcal C’ \proarrow \mathcal D’| defined as <br /><span class="math display">$$\begin{gather}
s : \O_x, s' : \O'_{x'}, t : \O_y, t' : \O'_{y'}; f : \A(t, s), p : \mathsf P(\mathsf F(s), \mathsf F'(s')), f' : \A'(s', t')
    \vdash \mathsf P(\mathsf F(f), p, \mathsf F'(f')) : \mathsf P(\mathsf F(t), \mathsf F'(t'))
\end{gather}$$</span><br /> In particular, we have the <strong>representable |\V|-profunctors</strong> when |P| is the hom |\V|-profunctor and either |F| or |F’| is the identity |\V|-functor, e.g. |\mathcal C(Id, F)| or |\mathcal C(F, Id)|.</p>
<h3 id="multimorphisms">Multimorphisms</h3>
<p>There’s a natural notion of morphism of |\V|-profunctors which we could derive either via passing the notion of natural transformation of the bifunctorial view through the same reformulations as above, or by generalizing the notion of a bimodule homomorphism. This would produce a notion like: a |\V|-natural transformation from |\alpha : P \to Q| is a |\alpha : P(t, s) \to Q(t, s)| satisfying |\alpha(P(f, p, h)) = Q(f, \alpha(p), h)|. While there’s nothing wrong with this definition, it doesn’t quite meet our needs. One way to see this is that it would be nice to have a bicategory whose |0|-cells were |\V|-categories, |1|-cells |\V|-profunctors, and |2|-cells |\V|-natural transformations as above. The problem there isn’t the |\V|-natural transformations but the |1|-cells. In particular, we don’t have composition of |\V|-profunctors. In the analogy with bimodules, we don’t have tensor products so we can’t reduce multilinear maps to linear maps; therefore, linear maps don’t suffice, and we really want a notion of multilinear maps.</p>
<p>So, instead of a bicategory what we’ll have is a virtual bicategory (or, more generally, a <a href="https://ncatlab.org/nlab/show/virtual+double+category">virtual double category</a>). A virtual bicategory is to a bicategory what a multicategory is to a monoidal category, i.e. multicategories are “virtual monoidal categories”. The only difference between a virtual bicategory and a multicategory is that instead of our multimorphisms having arbitrary lists of objects as their sources, our “objects” (|1|-cells) themselves have sources and targets (|0|-cells) and our multimorphisms (|2|-cells) have <em>composable sequences</em> of |1|-cells as their sources.</p>
<p>A <strong>|\V|-multimorphism</strong> from a composable sequence of |\V|-profunctors |P_1, \dots, P_n| to the |\V|-profunctor |Q| is a model of the theory consisting of the various necessary subtheories and:</p>
<ul>
<li>a linear term, |s_0 : \O_{x_0}^0, \dots, s_n : \O_{x_n}^n; p_1 : \mathsf P_{x_0x_1}^1(s_0, s_1), \dots, p_n : \mathsf P_{x_{n-1}x_n}^n(s_{n-1}, s_n) \vdash \tau_{x_0\cdots x_n}(p_1, \dots, p_n) : \mathsf Q_{x_0x_n}(s_0, s_n)|</li>
</ul>
<p>satisfying <br /><span class="math display">$$\begin{align}
&amp; t, s_0 : \O^0, \dots, s_n : \O^n;
    f : \A^0(t, s_0), p_1 : \mathsf P^1(s_0, s_1), \dots, p_n : \mathsf P^n(s_{n-1}, s_n) \\
    \vdash\ &amp; \tau(\mathsf P_l^0(f, p_1), \dots, p_n) = \mathsf Q_l(f, \tau(p_1, \dots, p_n)) : \mathsf Q(t, s_n) \\ \\
&amp; s_0 : \O^0, \dots, s_n, s : \O^n;
    p_1 : \mathsf P^1(s_0, s_1), \dots, p_n : \mathsf P^n(s_{n-1}, s_n), f : \A^n(s_n, s) \\
    \vdash\ &amp; \tau(p_1, \dots, \mathsf P_r^n(p_n, f)) = \mathsf Q_r(\tau(p_1, \dots, p_n), f) : \mathsf Q(s_0, s) \\ \\
&amp; s_0 : \O^0, \dots, s_n : \O^n; \\
    &amp; p_1 : \mathsf P^1(s_0, s_1), \dots, p_i : \mathsf P^i(s_{i-1}, s_i), f : \A^i(s_i, s_{i+1}),
      p_{i+1} : \mathsf P^{i+1}(s_i, s_{i+1}), \dots, p_n : \mathsf P^n(s_{n-1}, s_n) \\
    \vdash\ &amp; \tau(p_1, \dots, \mathsf P_r^i(p_i, f), p_{i+1}, \dots, p_n) = \tau(p_1, \dots, p_i, \mathsf P_l^{i+1}(f, p_{i+1}) \dots, p_n) : \mathsf Q(s_0, s_n)
\end{align}$$</span><br /> except for the |n=0| case in which case the only law is <br /><span class="math display">$$\begin{gather}
t, s : \O^0; f : \A^0(t, s) \vdash \mathsf Q_l(f, \tau()) = \mathsf Q_r(\tau(), f) : \mathsf Q(t, s)
\end{gather}$$</span><br /></p>
<p>The laws involving the action of |\mathsf Q| are called <strong>external equivariance</strong>, while the remaining law is called <strong>internal equivariance</strong>. We’ll write |\V\mathbf{Prof}(P_1, \dots, P_n; Q)| for the set of |\V|-multimorphisms from the composable sequence of |\V|-profunctors |P_1, \dots, P_n| to the |\V|-profunctor |Q|.</p>
<p>As with multilinear maps, we can characterize composition via a universal property. Write |Q_1\diamond\cdots\diamond Q_n| for the <strong>composite |\V|-profunctor</strong> (when it exists) of the composable sequence |Q_1, \dots, Q_n|. We then have for any pair of composable sequences |R_1, \dots, R_m| and |S_1, \dots, S_k| which compose with |Q_1, \dots, Q_n|, <br /><span class="math display">$$\begin{gather}
\V\mathbf{Prof}(R_1,\dots, R_m, Q_1 \diamond \cdots \diamond Q_n, S_1, \dots, S_k; -)
    \cong \V\mathbf{Prof}(R_1,\dots, R_m, Q_1, \dots, Q_n, S_1, \dots, S_k; -)
\end{gather}$$</span><br /> where the forward direction is induced by precomposition with a |\V|-multimorphism |Q_1, \dots, Q_n \to Q_1 \diamond \cdots \diamond Q_n|. A |\V|-multimorphism with this property is called <strong>opcartesian</strong>. The |n=0| case is particularly important and, for a |\V|-category |\mathcal C|, produces the <strong>unit |\V|-profunctor</strong>, |U_\mathcal C : \mathcal C \proarrow \mathcal C| as the composite of the empty sequence. When we have all composites, |\V\mathbf{Prof}| becomes an actual bicategory rather than a virtual bicategory. |\V\mathbf{Prof}| always has all units, namely the hom |\V|-profunctors. Much like we can define the tensor product of modules by quotienting the tensor product of their underlying abelian groups by internal equivariance, we will find that we can make composites when we have enough (well-behaved) colimits<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>Related to composites, we can talk about left/right closure of |\V\mathbf{Prof}|. In this case we have the natural isomorphisms: <br /><span class="math display">$$\begin{gather}
\V\mathbf{Prof}(Q_1,\dots, Q_n, R; S) \cong \V\mathbf{Prof}(Q_1, \dots, Q_n; R \triangleright S) \\
\V\mathbf{Prof}(R, Q_1, \dots, Q_n; S) \cong \V\mathbf{Prof}(Q_1, \dots, Q_n;S \triangleleft R)
\end{gather}$$</span><br /> Like composites, this merely characterizes these constructs; they need not exist in general. These will be important when we talk about Yoneda and (co)limits in |\V|-categories.</p>
<p>A |\V|-natural transformation |\alpha : F \to G : \mathcal C \to \mathcal D| is the same as |\alpha\in\V\mathbf{Prof}(;\mathcal D(G, F))|.</p>
<h3 id="example-proof">Example Proof</h3>
<p>Just as an example, let’s prove a basic fact about categories for arbitrary |\V|-categories. This will use an informal style.</p>
<p>The fact will be that full and faithful functors reflect isomorphisms. Let’s go through the typical proof for the ordinary category case.</p>
<p>Suppose we have an natural transformation |\varphi : \mathcal D(FA, FB) \to \mathcal C(A, B)| natural in |A| and |B| such that |\varphi| is an inverse to |F|, i.e. the action of the functor |F| on arrows. If |Ff \circ Fg = id| and |Fg \circ Ff = id|, then by the naturality of |\varphi|, |\varphi(id) = \varphi(Ff \circ id \circ Fg) = f \circ \varphi(id) \circ g| and similarly with |f| and |g| switched. We now just need to show that |\varphi(id) = id| but |id = F(id)|, so |\varphi(id) = \varphi(F(id)) = id|. |\square|</p>
<p>Now in the internal language. We’ll start with the theory of a |\V|-functor, so we have |\O|, |\O’|, |\A|, |\A’|, and |\mathsf F|. While the previous paragraph talks about a natural transformation, we can readily see that it’s really a multimorphism. In our case, it is a |\V|-multimorphism |\varphi| from |\A’(\mathsf F, \mathsf F)| to |\A|. Before we do that though, we need to show that |\mathsf F| itself is a |\V|-multimorphism. This corresponds to the naturality of the action on arrows of |F| which we took for granted in the previous paragraph. This is quickly verified: the external equivariance equations are just the functor law for composites. The additional data we have is two linear terms |\mathsf f| and |\mathsf g| such that |\mathsf F(\mathsf f) \circ \mathsf F(\mathsf g) = \mathsf{id}| and |\mathsf F(\mathsf g) \circ \mathsf F(\mathsf f) = \mathsf{id}|. Also, |\varphi(\mathsf F(h)) = h|. The result follows through almost identically to the previous paragraph. |\varphi(\mathsf{id}) = \varphi(\mathsf F(\mathsf f) \circ \mathsf F(\mathsf g)) = \varphi(\mathsf F(\mathsf f) \circ \mathsf{id} \circ \mathsf F(\mathsf g))|, we apply external equivariance twice to get |\mathsf f \circ \varphi(\mathsf{id}) \circ \mathsf g|. The functor law for |\mathsf{id}| gives |\varphi(\mathsf{id}) = \varphi(\mathsf F(\mathsf{id})) = \mathsf{id}|. A quick glance verifies that all these equations use their free variables linearly as required. |\square|</p>
<p>As a warning, in the above |\mathsf f| and |\mathsf g| are not free variables but constants, i.e. primitive linear terms. Thus there is no issue with an equation like |\mathsf F(\mathsf f) \circ \mathsf F(\mathsf g) = \mathsf{id}| as both sides have no free variables.</p>
<p>This is a very basic result but, again, the payoff here is how boring and similar to the usual case this is. For contrast, the definition of an internal profunctor is given <a href="https://ncatlab.org/nlab/show/internal+profunctor">here</a>. This definition is easier to connect to our notion of |\V|-presheaf, specifically a |\mathcal Self(\mathbf S)|-presheaf, than it is to the usual |\mathbf{Set}|-valued functor definition. While not hard, it would take me a bit of time to even formulate the above proposition, and a proof in terms of the explicit definitions would be hard to recognize as just the ordinary proof.</p>
<p>For fun, let’s figure out what the |\mathcal Const(\mathbf{Ab})| case of this result says explicitly. A |\mathcal Const(\mathbf{Ab})|-category is a ring, a |\mathcal Const(\mathbf{Ab})|-functor is a ring homomorphism, and a |\mathcal Const(\mathbf{Ab})|-profunctor is a bimodule. Let |R| and |S| be rings and |f : R \to S| be a ring homomorphism. An isomorphism in |R| viewed as a |\mathcal Const(\mathbf{Ab})|-category is just an invertible element. Every ring, |R|, is an |R|-|R|-bimodule. Given any |S|-|S|-bimodule |P|, we have an |R|-|R|-bimodule |f^*(P)| via restriction of scalars, i.e. |f^*(P)| has the same elements as |P| and for |p \in f^*(P)|, |rpr’ = f(r)pf(r’)|. In particular, |f| gives rise to a bimodule homomorphism, i.e. a linear function, |f : R \to f^*(S)| which corresponds to its action on arrows from the perspective of |f| as a |\mathcal Const(\mathbf{Ab})|-functor. If this linear transformation has an inverse, then the above result states that when |f(r)| is invertible so is |r|. So to restate this all in purely ring theoretic terms, given a ring homomorphism |f : R \to S| and an abelian group homomorphism |\varphi : S \to R| satisfying |\varphi(f(rst)) = r\varphi(f(s))t| and |\varphi(f(r)) = r|, then if |f(r)| is invertible so is |r|.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Indexed categories are equivalent to <em>cloven</em> fibrations and, if you have the Axiom of Choice, all fibrations can be cloven. Indexed categories can be viewed as <em>presentations</em> of fibrations.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>With a good understanding of what a class is, it’s clear that it doesn’t even make sense to have a proper class be an object. In frameworks with an explicit notion of "class", this is often manifested by saying that a class that is an element of another class is a set (and thus not a proper class).<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>This suggests that it might be interesting to consider categories that are (co)complete with respect to this monoid notion of “small”. I don’t think I’ve ever seen a study of such categories. (Co)Limits of monoids are <a href="https://doi.org/10.1007/BFb0084220">not trivial</a>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>This is one of the main things I like about working in weak foundations. It forces you to come up with better definitions that make it clear what is and is not important and eliminates coincidences. Of course, it also produces definitions and theorems that are inherently more general too.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>This connection isn’t much of a surprise as the tensor product of modules is exactly the (small) |\mathcal Const(\mathbf{Ab})| case of this.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Internal Language of Indexed Monoidal Categories</title>
    <link href="https://derekelkins.github.io/posts/internal-language-of-indexed-monoidal-categories.html" />
    <id>https://derekelkins.github.io/posts/internal-language-of-indexed-monoidal-categories.html</id>
    <published>2020-07-05 19:00:56-07:00</published>
    <updated>2020-07-06T02:00:56Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>This is part 2 in a series. See <a href="/posts/internal-language-of-a-monoidal-category.html">the previous part about internal languages for (non-indexed) monoidal categories</a>. The main application I have in mind – enriching in indexed monoidal categories – is covered in the <a href="/posts/enriched-indexed-categories-syntactically.html">next post</a>.</p>
<p>As Jean Bénabou pointed out in <a href="http://dx.doi.org/10.2307/2273784">Fibered Categories and the Foundations of Naive Category Theory</a> (<a href="https://pdfs.semanticscholar.org/a7ba/3c5b0431adf514595d53ea393910c9230745.pdf">PDF</a>) notions of “families of objects/arrows” are ubiquitous and fundamental in category theory. One of the more noticeable places early on is in the definition of a natural transformation as a family of arrows. However, even in the definition of category, identities and compositions are families of functions, or, in the enriched case, arrows of |\mathbf V|. From a foundational perspective, one place where this gets really in-your-face is when trying to formalize the notion of (co)completeness. It is straightforward to make a first-order theory of a finitely complete category, e.g. <a href="https://ncatlab.org/nlab/show/fully+formal+ETCS#the_theory_of_finitely_complete_categories">this one</a>. For arbitrary products and thus limits, we need to talk about families of objects. To formalize the usual meaning of this in a first-order theory would require attaching an entire first-order theory of sets, e.g. <strong>ZFC</strong>, to our notion of complete category. If your goals are of a foundational nature like Bénabou’s were, then this is unsatisfactory. Instead, we can abstract out what we need of the notion of “family”. The result turns out to be equivalent to the notion of a <a href="https://ncatlab.org/nlab/show/Grothendieck+fibration">fibration</a>.</p>
<p>My motivations here are not foundational but leaving the notion of “family” entirely meta-theoretical means not being able to talk about it except in the semantics. Bénabou’s comment suggests that at the semantic level we want not just a monoidal category, but a fibration of monoidal categories<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. At the syntactic level, it suggests that there should be a built-in notion of “family” in our language. We accomplish both of these goals by formulating the internal language of an <a href="https://ncatlab.org/nlab/show/indexed+monoidal+category">indexed monoidal category</a>.</p>
<p>As a benefit, we can generalize to other notions of “family” than set-indexed families. We’ll clearly be able to formulate the notion of an <a href="https://ncatlab.org/nlab/show/enriched+category">enriched category</a>. It’s also clear that we’ll be able to formulate the notion of an <a href="https://ncatlab.org/nlab/show/indexed+category">indexed category</a>. Of course, we’ll also be able to formulate the notion of a category that is both enriched and indexed which includes the important special case of an <a href="https://ncatlab.org/nlab/show/internal+category">internal category</a>. We can also consider cases with trivial indexing which, in the unenriched case, will give us monoids, and in the |\mathbf{Ab}|-enriched case will give us rings.</p>
<!--more-->
<h2 id="indexed-monoidal-categories">Indexed Monoidal Categories</h2>
<p>Following Shulman’s <a href="https://arxiv.org/abs/1212.3914">Enriched indexed categories</a>, let |\mathbf{S}| be a category with a cartesian monoidal structure, i.e. finite products. Then an |\mathbf{S}|-indexed monoidal category is simply a pseudofunctor |\newcommand{\V}{\mathcal V}\V : \mathbf{S}^{op} \to \mathbf{MonCat}|. A <a href="https://ncatlab.org/nlab/show/pseudofunctor">pseudofunctor</a> is like a functor except that the functor laws only hold up to isomorphism, e.g. |\V(id)\cong id|. |\mathbf{MonCat}| is the |2|-category of monoidal categories, <a href="https://ncatlab.org/nlab/show/monoidal+functor">strong monoidal functors</a><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, and <a href="https://ncatlab.org/nlab/show/monoidal+natural+transformation">monoidal natural transformations</a>. We’ll write |\V(X)| as |\V^X| and |\V(f)| as |f^*|. We’ll never have multiple relevant indexed monoidal categories so this notation will never be ambiguous. We’ll call the categories |\V^X| <strong>fiber categories</strong> and the functors |f^*| <strong>reindexing functors</strong>. The cartesian monoidal structure on |\mathbf S| becomes relevant when we want to equip the total category, |\int\V|, (computed via the <a href="https://ncatlab.org/nlab/show/Grothendieck+construction">Grothendieck construction</a> in the usual way) with a monoidal structure. In particular, the tensor product of |A \in \V^X| and |B \in \V^Y| is an object |A\otimes B \in \V^{X\times Y}| calculated as |\pi_1^*(A) \otimes_{X\times Y} \pi_2^*(B)| where |\otimes_{X\times Y}| is the monoidal tensor in |\V^{X\times Y}|. The unit, |I|, is the unit |I_1 \in \V^1|.</p>
<p>The two main examples are: |\mathcal Fam(\mathbf V)| where |\mathbf V| is a (non-indexed) monoidal category and |\mathcal Self(\mathbf S)| where |\mathbf S| is a category with finite limits. |\mathcal Fam(\mathbf V)| is a |\mathbf{Set}|-indexed monoidal category with |\mathcal Fam(\mathbf V)^X| defined as the set of |X|-indexed families of objects of |\mathbf V|, families of arrows between them, and an index-wise monoidal product. We can identify |\mathcal Fam(\mathbf V)^X| with the functor category |[DX, \mathbf V]| where |D : \mathbf{Set} \to \mathbf{cat}| takes a set |X| to a small discrete category. Enriching in indexed monoidal category |\mathcal Fam(\mathbf V)| will be equivalent to enriching in the non-indexed monoidal category |\mathbf V|, i.e. the usual notion of enrichment in a monoidal category. |\mathcal Self(\mathbf S)| is an |\mathbf S|-indexed monoidal category and |\mathcal Self(\mathbf S)^X| is the slice category |\mathbf S/X| with its cartesian monoidal structure. |f^*| is the pullback functor. |\mathcal Self(\mathbf S)|-enriched categories are categories internal to |\mathbf S|. A third example we’ll find interesting is |\mathcal Const(\mathbf V)| for a (non-indexed) monoidal category, |\mathbf V|, which is a |\mathbf 1|-indexed monoidal category, which corresponds to an object of |\mathbf{MonCat}|, namely |\mathbf V|.</p>
<h2 id="the-internal-language-of-indexed-monoidal-categories">The Internal Language of Indexed Monoidal Categories</h2>
<p>This builds on the internal language of a monoidal category described in the previous post. We’ll again have <strong>linear types</strong> and <strong>linear terms</strong> which will be interpreted into objects and arrows in the fiber categories. To indicate the dependence on the indexing, we’ll use two contexts: |\Gamma| will be an <strong>index context</strong> containing <strong>index types</strong> and <strong>index variables</strong>, which will be interpreted into objects and arrows of |\mathbf S|, while |\Delta|, the <strong>linear context</strong>, will contain linear types and linear variables as before except now linear types will be able to depend on <strong>index terms</strong>. So we’ll have judgements that look like: <br /><span class="math display">$$\begin{gather}
\Gamma \vdash A \quad \text{and} \quad \Gamma; \Delta \vdash E : B
\end{gather}$$</span><br /> The former indicates that |A| is a linear type indexed by the index variables of |\Gamma|. The latter states that |E| is a linear term of linear type |B| in the linear context |\Delta| indexed by the index variables of |\Gamma|. We’ll also have judgements for index types and index terms: <br /><span class="math display">$$\begin{gather}
\vdash X : \square \quad \text{and} \quad \Gamma \vdash E : Y
\end{gather}$$</span><br /> The former asserts that |X| is an index type. The latter asserts that |E| is an index term of index type |Y| in the index context |\Gamma|.</p>
<p>Since each fiber category is monoidal, we’ll have all the rules from before just with an extra |\Gamma| hanging around. Since our indexing category, |\mathbf S|, is also monoidal, we’ll also have copies of these rules at the level of indexes. However, since |\mathbf S| is <em>cartesian</em> monoidal, we’ll also have the structural rules of weakening, exchange, and contraction for index terms and types. To emphasize the cartesian monoidal structure of indexes, I’ll use the more traditional Cartesian product and tuple notation: |\times| and |(E_1, \dots, E_n)|. This notation allows a bit more uniformity as the |n=0| case can be notated by |()|.</p>
<p>The only really new rule is the rule that allows us to move linear types and terms from one index context to another, i.e. the rule that would correspond to applying a reindexing functor. I call this rule Reindex and, like Cut, it will be witnessed by substitution. Like Cut, it will also be a rule which we can eliminate. At the semantic level, this elimination corresponds to the fact that to understand the intepreration of any particular (linear) term, we can first reindex <em>everything</em>, i.e. all the intepretations of all subterms, into the same fiber category and then we can work entirely within that one fiber category. The Reindex rule is: <br /><span class="math display">$$\begin{gather}
\dfrac{\Gamma \vdash E : X \quad \Gamma', x : X; a_1 : A_1, \dots, a_n : A_n \vdash E' : B}{\Gamma',\Gamma; a_1 : A_1[E/x], \dots, a_n : A_n[E/x] \vdash E'[E/x] : B[E/x]}\text{Reindex}
\end{gather}$$</span><br /></p>
<p>By representing reindexing by syntactic substitution, we’re requiring the semantics of (linear) type and term formation operations to be respected by reindexing functors. This is exactly the right thing to do as the appropriate notion of, say, indexed coproducts, which would correspond to sum types, is coproducts in each fiber category which are preserved by reindexing functors.</p>
<p><a href="#rules-for-an-indexed-monoidal-category">Below</a> I provide a listing of rules and equations.</p>
<h2 id="relation-to-parameterized-and-dependent-types">Relation to Parameterized and Dependent Types</h2>
<p><small>None of this section is necessary for anything else.</small></p>
<p>This notion of (linear) types and terms being indexed by other types and terms is reminiscent of parametric types or dependent types. The machinery of indexed/fibered categories is also commonly used in the categorical semantics of parameterized and dependent types. However, there are important differences between those cases and our case.</p>
<p>In the case of parameterized types, we have types and terms that depend on other types. In this case, we have kinds, which are “types of types”, which classify types which in turn classify terms. If we try to set up an analogy to our situation, index types would correspond to kinds and index terms would correspond to types. The most natural thing to continue would be to have linear terms correspond to terms, but we start to see the problem. Linear terms are classified by linear types, but linear types are <em>not</em> index terms. They don’t even induce index terms. In the categorical semantics of parameterized types, this identification of types with (certain) expressions classified by kinds is handled by the notion of a generic object. A generic object corresponds to the kind |\mathsf{Type}| (what Haskell calls <code>*</code>). The assumption of a generic object is a rather strong assumption and one that none of our example indexed monoidal categories support in general.</p>
<p>A similar issue occurs when we try to make an analogy to dependent types. The defining feature of a dependent type system is that types can depend on terms. The problem with such a potential analogy is that linear types and terms do not induce index types and terms. A nice way to model the semantics of dependent types is the notion of a <a href="https://ncatlab.org/nlab/show/categorical+model+of+dependent+types#comprehension_categories">comprehension category</a>. This, however, is additional structure beyond what we are given by an indexed monoidal category. However, comprehension categories will implicitly come up later when we talk about adding |\mathbf S|-indexed (co)products. These comprehension categories will share the same index category as our indexed monoidal categories, namely |\mathbf S|, but will have different total categories. Essentially, a comprehension category shows how objects (and arrows) of a total category can be represented in the index category. We can then talk about having (co)products in a different total category with same index category with respect to those objects picked out by the comprehension category. We get dependent types in the case where the total categories are the same. (More precisely, the fibrations are the same.) Sure enough, we will see that when |\mathcal Self(\mathbf S)| has |\mathbf S|-indexed products, then |\mathbf S| is, indeed, a model of a dependent type theory. In particular, it is <a href="https://ncatlab.org/nlab/show/locally+cartesian+closed+category">locally cartesian closed</a>.</p>
<h2 id="rules-for-an-indexed-monoidal-category">Rules for an Indexed Monoidal Category</h2>
<p><br /><span class="math display">$$\begin{gather}
\dfrac{\vdash X : \square}{x : X \vdash x : X}\text{IxAx} \qquad
\dfrac{\Gamma\vdash E : X \quad \Gamma', x : X \vdash E': Y}{\Gamma',\Gamma \vdash E'[E/x] : Y}\text{IxCut}
 \\ \\
\dfrac{\vdash Y : \square \quad \Gamma\vdash E : X}{\Gamma, y : Y \vdash E : X}\text{Weakening},\ y\text{ fresh} \qquad
\dfrac{\Gamma, x : X, y : Y, \Gamma' \vdash E : Z}{\Gamma, y : Y, x : X, \Gamma' \vdash E : Z}\text{Exchange} \qquad
\dfrac{\Gamma, x : X, y : Y \vdash E : Z}{\Gamma, x : X \vdash E[x/y] : Z}\text{Contraction}
 \\ \\
\dfrac{\mathsf X : \mathsf{IxType}}{\vdash \mathsf X : \square}\text{PrimIxType} \qquad
\dfrac{\vdash X_1 : \square \quad \cdots \quad \vdash X_n : \square}{\vdash (X_1, \dots, X_n) : \square}{\times_n}\text{F}
 \\ \\
\dfrac{\Gamma \vdash E_1 : X_1 \quad \cdots \quad \Gamma \vdash E_n : X_n \quad \mathsf F : (X_1, \dots, X_n) \to Y}{\Gamma \vdash \mathsf F(E_1, \dots, E_n) : Y}\text{PrimIxTerm}
 \\ \\
\dfrac{\Gamma_1 \vdash E_1 : X_1 \quad \cdots \quad \Gamma_n \vdash E_n : X_n}{\Gamma_1,\dots,\Gamma_n \vdash (E_1, \dots, E_n) : (X_1, \dots, X_n)}{\times_n}\text{I} \qquad
\dfrac{\Gamma \vdash E : (X_1, \dots, X_n) \quad x_1 : X_1, \dots, x_n : X_n, \Gamma' \vdash E' : Y}{\Gamma, \Gamma' \vdash \mathsf{match}\ E\ \mathsf{as}\ (x_1, \dots, x_n)\ \mathsf{in}\ E' : Y}{\times_n}\text{E}
 \\ \\
\dfrac{\Gamma \vdash E_1 : X_1 \quad \cdots \quad \Gamma \vdash E_n : X_n \quad \mathsf A : (X_1, \dots, X_n) \to \mathsf{Type}}{\Gamma \vdash \mathsf A(E_1, \dots, E_n)}\text{PrimType}
 \\ \\
\dfrac{\Gamma \vdash A}{\Gamma; a : A \vdash a : A}\text{Ax} \qquad
\dfrac{\Gamma; \Delta_1 \vdash E_1 : A_1 \quad \cdots \quad \Gamma; \Delta_n \vdash E_n : A_n \quad \Gamma; \Delta_l, a_1 : A_1, \dots, a_n : A_n, \Delta_r \vdash E: B}{\Gamma; \Delta_l, \Delta_1, \dots, \Delta_n, \Delta_r \vdash E[E_1/a_1, \dots, E_n/a_n] : B}\text{Cut}
 \\ \\
\dfrac{\Gamma \vdash E : X \quad \Gamma', x : X; a_1 : A_1, \dots, a_n : A_n \vdash E' : B}{\Gamma',\Gamma; a_1 : A_1[E/x], \dots, a_n : A_n[E/x] \vdash E'[E/x] : B[E/x]}\text{Reindex}
 \\ \\
\dfrac{}{\Gamma\vdash I}I\text{F} \qquad
\dfrac{\Gamma\vdash A_1 \quad \cdots \quad \Gamma \vdash A_n}{\Gamma \vdash A_1 \otimes \cdots \otimes A_n}{\otimes_n}\text{F}, n \geq 1
 \\ \\
\dfrac{\Gamma \vdash E_1 : X_1 \quad \cdots \quad \Gamma \vdash E_n : X_n \quad \Gamma; \Delta_1 \vdash E_1' : A_1 \quad \cdots \quad \Gamma; \Delta_m \vdash E_m' : A_m \quad \mathsf f : (x_1 : X_1, \dots, x_n : X_n; A_1, \dots, A_m) \to B}{\Gamma; \Delta_1, \dots, \Delta_m \vdash \mathsf f(E_1, \dots, E_n; E_1', \dots, E_m') : B}\text{PrimTerm}
 \\ \\
\dfrac{}{\Gamma; \vdash * : I}I\text{I} \qquad
\dfrac{\Gamma; \Delta \vdash E : I \quad \Gamma; \Delta_l, \Delta_r \vdash E' : B}{\Gamma; \Delta_l, \Delta, \Delta_r \vdash \mathsf{match}\ E\ \mathsf{as}\ *\ \mathsf{in}\ E' : B}I\text{E}
 \\ \\
\dfrac{\Gamma; \Delta_1 \vdash E_1 : A_1 \quad \cdots \quad \Gamma; \Delta_n \vdash E_n : A_n}{\Gamma; \Delta_1,\dots,\Delta_n \vdash E_1 \otimes \cdots \otimes E_n : A_1 \otimes \cdots \otimes A_n}{\otimes_n}\text{I}
 \\ \\
\dfrac{\Gamma; \Delta \vdash E : A_1 \otimes \cdots \otimes A_n \quad \Gamma; \Delta_l, a_1 : A_1, \dots, a_n : A_n, \Delta_r \vdash E' : B}{\Gamma; \Delta_l, \Delta, \Delta_r \vdash \mathsf{match}\ E\ \mathsf{as}\ (a_1 \otimes \cdots \otimes a_n)\ \mathsf{in}\ E' : B}{\otimes_n}\text{E},n \geq 1
\end{gather}$$</span><br /></p>
<h3 id="equations">Equations</h3>
<p><br /><span class="math display">$$\begin{gather}
\dfrac{\Gamma_1 \vdash E_1 : X_1 \quad \cdots \quad \Gamma_n \vdash E_n : X_n \qquad x_1 : X_1, \dots, x_n : X_n, \Gamma \vdash E : Y}{\Gamma_1, \dots, \Gamma_n, \Gamma \vdash (\mathsf{match}\ (E_1, \dots, E_n)\ \mathsf{as}\ (x_1, \dots, x_n)\ \mathsf{in}\ E) = E[E_1/x_1, \dots, E_n/x_n] : Y}{\times_n}\beta
 \\ \\
\dfrac{\Gamma \vdash E : (X_1, \dots, X_n) \qquad \Gamma, x : (X_1, \dots, X_n) \vdash E' : B}{\Gamma \vdash E'[E/x] = \mathsf{match}\ E\ \mathsf{as}\ (x_1, \dots, x_n)\ \mathsf{in}\ E'[(x_1, \dots, x_n)/x] : B}{\times_n}\eta
 \\ \\
\dfrac{\Gamma \vdash E_1 : (X_1, \dots, X_n) \qquad x_1 : X_1, \dots, x_n : X_n \vdash E_2 : Y \quad y : Y \vdash E_3 : Z}{\Gamma \vdash (\mathsf{match}\ E_1\ \mathsf{as}\ (x_1, \dots, x_n)\ \mathsf{in}\ E_3[E_2/y]) = E_3[(\mathsf{match}\ E_1\ \mathsf{as}\ (x_1, \dots, x_n)\ \mathsf{in}\ E_2)/y] : Z}{\times_n}\text{CC}
 \\ \\
\dfrac{\Gamma;\vdash E : B}{\Gamma;\vdash (\mathsf{match}\ *\ \mathsf{as}\ *\ \mathsf{in}\ E) = E : B}{*}\beta \qquad
\dfrac{\Gamma; \Delta \vdash E : I \qquad \Gamma; \Delta_l, a : I, \Delta_r \vdash E' : B}{\Gamma; \Delta_l, \Delta, \Delta_r \vdash E'[E/a] = (\mathsf{match}\ E\ \mathsf{as}\ *\ \mathsf{in}\ E'[{*}/a]) : B}{*}\eta
 \\ \\
\dfrac{\Gamma; \Delta_1 \vdash E_1 : A_1 \quad \cdots \quad \Gamma; \Delta_n \vdash E_n : A_n \qquad \Gamma; \Delta_l, a_1 : A_1, \dots, a_n, \Delta_r : A_n \vdash E : B}{\Gamma; \Delta_l, \Delta_1, \dots, \Delta_n, \Delta_r \vdash (\mathsf{match}\ E_1\otimes\cdots\otimes E_n\ \mathsf{as}\ a_1\otimes\cdots\otimes a_n\ \mathsf{in}\ E) = E[E_1/a_1, \dots, E_n/a_n] : B}{\otimes_n}\beta
 \\ \\
\dfrac{\Gamma; \Delta \vdash E : A_1 \otimes \cdots \otimes A_n \qquad \Gamma; \Delta_l, a : A_1 \otimes \cdots \otimes A_n, \Delta_r \vdash E' : B}{\Gamma; \Delta_l, \Delta, \Delta_r \vdash E'[E/a] = \mathsf{match}\ E\ \mathsf{as}\ a_1\otimes\cdots\otimes a_n\ \mathsf{in}\ E'[(a_1\otimes\cdots\otimes a_n)/a] : B}{\otimes_n}\eta
 \\ \\
\dfrac{\Gamma; \Delta \vdash E_1 : I \qquad \Gamma; \Delta_l, \Delta_r \vdash E_2 : B \qquad \Gamma; b : B \vdash E_3 : C}{\Gamma; \Delta_l, \Delta, \Delta_r \vdash (\mathsf{match}\ E_1\ \mathsf{as}\ *\ \mathsf{in}\ E_3[E_2/b]) = E_3[(\mathsf{match}\ E_1\ \mathsf{as}\ *\ \mathsf{in}\ E_2)/b] : C}{*}\text{CC}
 \\ \\
\dfrac{\Gamma; \Delta \vdash E_1 : A_1 \otimes \cdots \otimes A_n \qquad \Gamma; \Delta_l, a_1 : A_1, \dots, a_n : A_n, \Delta_r \vdash E_2 : B \qquad \Gamma; b : B \vdash E_3 : C}{\Gamma; \Delta_l, \Delta, \Delta_r \vdash (\mathsf{match}\ E_1\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_n\ \mathsf{in}\ E_3[E_2/b]) = E_3[(\mathsf{match}\ E_1\ \mathsf{as}\ a_1 \otimes \dots \otimes a_n\ \mathsf{in}\ E_2)/b] : C}{\otimes_n}\text{CC}
\end{gather}$$</span><br /></p>
<p>|\mathsf X : \mathsf{IxType}| means |\mathsf X| is a primitive index type in the signature. |\mathsf A : (X_1, \dots, X_n) \to \mathsf{Type}| means that |\mathsf A| is a primitive linear type in the signature. |\mathsf F : (X_1, \dots, X_n) \to Y| and |\mathsf f : (x_1 : X_1, \dots, x_n : X_n; A_1, \dots, A_m) \to B| mean that |\mathsf F| and |\mathsf f| are assigned these types in the signature. In the latter case, it is assumed that |x_1 : X_1, \dots, x_n : X_n \vdash A_i| for |i = 1, \dots, m| and |x_1 : X_1, \dots, x_n : X_n \vdash B|. Alternatively, these assumptions could be added as additional hypotheses to the PrimTerm rule. Generally, every |x_i| will be used in some |A_j| or in |B|, though this isn’t technically required.</p>
<p>As before, I did not write the usual laws for equality (reflexivity and indiscernability of identicals) but they also should be included.</p>
<p>See the discussion in <a href="/posts/internal-language-of-a-monoidal-category.html">the previous part</a> about the commuting conversion (|\text{CC}|) rules.</p>
<p>A theory in this language is free to introduce additional index types, operations on indexes, linear types, and linear operations.</p>
<h2 id="interpretation-into-an-mathbf-s-indexed-monoidal-category">Interpretation into an |\mathbf S|-indexed monoidal category</h2>
<p>Fix an |\mathbf S|-indexed monoidal category |\V|. Write |\newcommand{\den}[1]{[\![#1]\!]}\den{-}| for the (overloaded) interpretation function. Its value on primitive operations is left as a parameter.</p>
<p>Associators for the semantic |\times| and |\otimes| will be omitted below.</p>
<h3 id="interpretation-of-index-types">Interpretation of Index Types</h3>
<p><br /><span class="math display">$$\begin{align}
\vdash X : \square \implies &amp; \den{X} \in \mathsf{Ob}(\mathbf S) \\ \\
\den{\Gamma} = &amp; \prod_{i=1}^n \den{X_i}\text{ where } \Gamma = x_1 : X_1, \dots, x_n : X_n \\
\den{(X_1, \dots, X_n)} = &amp; \prod_{i=1}^n \den{X_i}
\end{align}$$</span><br /></p>
<h3 id="interpretation-of-index-terms">Interpretation of Index Terms</h3>
<p><br /><span class="math display">$$\begin{align}
\Gamma \vdash E : X \implies &amp; \den{E} \in \mathbf{S}(\den{\Gamma}, \den{X}) \\ \\
\den{x_i} =\, &amp; \pi_i \text{ where } x_1 : X_1, \dots, x_n : X_n \vdash x_i : X_i \\
\den{(E_1, \dots, E_n)} =\, &amp; \den{E_1} \times \cdots \times \den{E_n} \\
\den{\mathsf{match}\ E\ \mathsf{as}\ (x_1, \dots, x_n)\ \mathsf{in}\ E'} =\, &amp; \den{E'} \circ (\den{E} \times id_{\den{\Gamma'}}) \text{ where }
    \Gamma' \vdash E' : Y \\
\den{\mathsf F(E_1, \dots, E_n)} =\, &amp; \den{\mathsf F} \circ (\den{E_1} \times \cdots \times \den{E_n}) \\
    &amp; \quad \text{ where }\mathsf F\text{ is an appropriately typed index operation}
\end{align}$$</span><br /></p>
<h3 id="witnesses-of-index-derivations">Witnesses of Index Derivations</h3>
<p>IxAx is witnessed by identity, and IxCut by composition in |\mathbf S|. Weakening is witnessed by projection. Exchange and Contraction are witnessed by expressions that can be built from projections and tupling. This is very standard.</p>
<h3 id="interpretation-of-linear-types">Interpretation of Linear Types</h3>
<p><br /><span class="math display">$$\begin{align}
\Gamma \vdash A \implies &amp; \den{A} \in \mathsf{Ob}(\V^{\den{\Gamma}}) \\ \\
\den{\Delta} =\, &amp; \den{A_1}\otimes_{\den{\Gamma}}\cdots\otimes_{\den{\Gamma}}\den{A_n} \text{ where } \Delta = a_1 : A_1, \dots, a_n : A_n \\
\den{I} =\, &amp; I_{\den{\Gamma}}\text{ where } \Gamma \vdash I \\
\den{A_1 \otimes \cdots \otimes A_n} =\, &amp; \den{A_1}\otimes_{\den{\Gamma}} \cdots \otimes_{\den{\Gamma}} \den{A_n}\text{ where }
    \Gamma \vdash A_i \\
\den{\mathsf A(E_1, \dots, E_n)} =\, &amp; \langle \den{E_1}, \dots, \den{E_n}\rangle^*(\den{\mathsf A}) \\
    &amp; \quad \text{ where }\mathsf A\text{ is an appropriately typed linear type operation}
\end{align}$$</span><br /></p>
<h3 id="interpretation-of-linear-terms">Interpretation of Linear Terms</h3>
<p><br /><span class="math display">$$\begin{align}
\Gamma; \Delta \vdash E : A \implies &amp; \den{E} \in \V^{\den{\Gamma}}(\den{\Delta}, \den{A}) \\ \\
\den{a} =\, &amp; id_{\den{A}} \text{ where } a : A \\
\den{*} =\, &amp; id_{I_{\den{\Gamma}}} \text{ where } \Gamma;\vdash * : I \\
\den{E_1 \otimes \cdots \otimes E_n} =\, &amp; \den{E_1} \otimes_{\den{\Gamma}} \cdots \otimes_{\den{\Gamma}} \den{E_n} \text{ where }
    \Gamma; \Delta_i \vdash E_i : A_i \\
\den{\mathsf{match}\ E\ \mathsf{as}\ {*}\ \mathsf{in}\ E'} =\, &amp;
    \den{E'} \circ (id_{\den{\Delta_l}} \otimes_{\den{\Gamma}} (\lambda_{\den{\Delta_r}} \circ (\den{E} \otimes_{\den{\Gamma}} id_{\den{\Delta_r}}))) \\
\den{\mathsf{match}\ E\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_n\ \mathsf{in}\ E'} =\, &amp;
    \den{E'} \circ (id_{\den{\Delta_l}} \otimes_{\den{\Gamma}} \den{E} \otimes_{\den{\Gamma}} id_{\den{\Delta_r}}) \\
\den{\mathsf f(E_1, \dots, E_n; E_1', \dots, E_n')} =\, &amp; \langle \den{E_1}, \dots, \den{E_n}\rangle^*(\den{\mathsf f})
    \circ (\den{E_1'} \otimes_{\den{\Gamma}} \cdots \otimes_{\den{\Gamma}} \den{E_n'}) \\
        &amp; \quad \text{ where }\mathsf f\text{ is an appropriately typed linear operation}
\end{align}$$</span><br /></p>
<h3 id="witnesses-of-linear-derivations">Witnesses of Linear Derivations</h3>
<p>As with the index derivations, Ax is witnessed by the identity, in this case in |\V^{\den{\Gamma}}|.</p>
<p>|\den{E[E_1/a_1,,E_n/a_n]} = \den{E} \circ (\den{E_1}\otimes\cdots\otimes\den{E_n})| witnesses Cut.</p>
<p>Roughly speaking, Reindex is witnessed by |\den{E}^*(\den{E’})|. If we were content to restrict ourselves to semantics in |\mathbf S|-indexed monoidal categories witnessed by functors, as opposed to pseudofunctors, into <em>strict</em> monoidal categories, then this would suffice. For an arbitrary |\mathbf S|-indexed monoidal category, we can’t be sure that the naive interpretation of |A[E/x][E’/y]|, i.e. |\den{E’}^*(\den{E}^*(\den{A}))|, which we’d get from two applications of the Reindex rule, is the same as the interpretation of |A[E[E’/y]/x]|, i.e. |\den{E \circ E’}^*(\den{A})|, which we’d get from IxCut followed by Reindex. On the other hand, |A[E/x][E’/y] = A[E[E’/y]/x]| is simply true syntactically by the definition of substitution (which I have not provided but is the obvious, usual thing). There are similar issues for (meta-)equations like |I[E/x] = I| and |(A_1 \otimes A_2)[E/x] = A_1[E/x] \otimes A_2[E/x]|.</p>
<p>The solution is that we essentially use a normal form where we eliminate the uses of Reindex. These normal form derivations will be reached by rewrites such as: <br /><span class="math display">$$\begin{gather}
\dfrac{\dfrac{\mathcal D}{\Gamma' \vdash E : X} \qquad \dfrac{\dfrac{\mathcal D_1}{\Gamma, x : X; \Delta_1 \vdash E_1 : A_1} \quad
                                        \cdots \quad \dfrac{\mathcal D_n}{\Gamma, x : X; \Delta_n \vdash E_n : A_n}}
    {\Gamma, x : X; \Delta_1, \dots, \Delta_n \vdash E_1 \otimes \cdots \otimes E_n : A_1 \otimes \cdots \otimes A_n}}
    {\Gamma, \Gamma'; \Delta_1[E/x], \dots, \Delta_n[E/x] \vdash E_1[E/x] \otimes \cdots \otimes E_n[E/x] : A_1[E/x] \otimes \cdots \otimes A_n[E/x]} \\
\Downarrow \\
\dfrac{\dfrac{\dfrac{\mathcal D}{\Gamma' \vdash E : X} \quad \dfrac{\mathcal D_1}{\Gamma, x : X; \Delta_1 \vdash E_1 : A_1}}
                {\Gamma, \Gamma'; \Delta_1[E/x] \vdash E_1[E/x] : A_1[E/x]} \quad
        \cdots \quad
       \dfrac{\dfrac{\mathcal D}{\Gamma' \vdash E : X} \quad \dfrac{\mathcal D_n}{\Gamma, x : X; \Delta_n \vdash E_n : A_n}}
                {\Gamma, \Gamma'; \Delta_n[E/x] \vdash E_n[E/x] : A_n[E/x]}}
    {\Gamma, \Gamma'; \Delta_1[E/x], \dots, \Delta_n[E/x] \vdash E_1[E/x] \otimes \cdots \otimes E_n[E/x] : A_1[E/x] \otimes \cdots \otimes A_n[E/x]}
\end{gather}$$</span><br /></p>
<p>Semantically, this is witnessed by the strong monoidal structure, i.e. |\den{E}^*(\den{E_1} \otimes \cdots \otimes \den{E_n}) \cong \den{E}^*(\den{E_1}) \otimes \cdots \otimes \den{E}^*(\den{E_n})|. We need such rewrites for all (linear) rules that can immediately precede Reindex in a derivation. For |I\text{I}|, |I\text{E}|, |\otimes_n\text{E}|, and, as we’ve just seen, |\otimes_n\text{I}|, these rewrites are witnessed by |\den{E}^*| being a strong monoidal functor. The rewrites for |\text{Ax}| and |\text{Cut}| are witnessed by functorality of |\den{E}^*| and also strong monoidality for Cut. Finally, two adjacent uses of Reindex become an IxCut and a Reindex and are witnessed by the pseudofunctoriality of |(\_)^*|. (While we’re normalizing, we may as well eliminate Cut and IxCut as well.)</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>As the previous post alludes, monoidal structure is more than we need. If we pursue the generalizations described there in this indexed context, we eventually end up at <a href="https://ncatlab.org/nlab/show/augmented+virtual+double+category">augmented virtual double categories</a> or <a href="https://ncatlab.org/nlab/show/virtual+equipment">virtual equipment</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>The terminology here is a mess. Leinster calls strong monoidal functors “weak”. “Strong” also refers to <a href="https://ncatlab.org/nlab/show/tensorial+strength">tensorial strength</a>, and it’s quite possible to have a “strong lax monoidal functor”. (In fact, this is what applicative functors are usually described as, though a <a href="https://ncatlab.org/nlab/show/closed+functor">strong lax closed functor</a> would be a more direct connection.) Or the functors we’re talking about which are not-strong strong monoidal functors…<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Internal Language of a Monoidal Category</title>
    <link href="https://derekelkins.github.io/posts/internal-language-of-a-monoidal-category.html" />
    <id>https://derekelkins.github.io/posts/internal-language-of-a-monoidal-category.html</id>
    <published>2020-05-22 01:46:31-07:00</published>
    <updated>2020-05-22T08:46:31Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>This is the first post in a series of posts on doing enriched indexed category theory and using the notion of an internal language to make this look relatively mundane. The internal language aspects are useful for other purposes too, as will be illustrated in this post, for example. This is related to the post <a href="/posts/category-theory-syntactically.html">Category Theory, Syntactically</a>. In particular, it can be considered half-way between the unary theories and the finite product theories described there.</p>
<p>First in this series – this post – covers the internal language of a monoidal category. This is fairly straightforward, but it already provides some use. For example, the category of endofunctors on a category is a strict monoidal category, and so we can take a different perspective on natural transformations. This will also motivate the notions of a (virtual) bicategory and an actegory. Throughout this post, I’ll give a fairly worked example of turning some categorical content into <a href="/posts/understanding-typing-judgments.html">rules</a> of a type-/proof-theory.</p>
<p>The <a href="/posts/internal-language-of-indexed-monoidal-categories.html">second post</a> will add indexing to the notion of monoidal category and introduce the very powerful and useful notion of an <a href="https://ncatlab.org/nlab/show/indexed+monoidal+category">indexed monoidal category</a>.</p>
<p>The <a href="/posts/enriched-indexed-categories-syntactically.html">third post</a> will formulate the notion of <a href="https://arxiv.org/abs/1212.3914">categories enriched in an indexed monoidal category</a> and give the definitions which don’t require any additional assumptions.</p>
<p>The fourth post will introduce the notion and internal language for an indexed cosmos. Normally, when we do enriched category theory, we want the category into which we’re enriching to not be just a monoidal category but a <a href="https://ncatlab.org/nlab/show/cosmos">cosmos</a>. This provides many additional properties. An indexed cosmos is just the analogue of that for indexed monoidal categories.</p>
<p>The fifth post will then formulate categorical concepts for our enriched indexed categories that require some or all of these additional properties provided by an indexed cosmos.</p>
<p>At some point, there will be a post on <a href="https://ncatlab.org/nlab/show/virtual+double+category">virtual double categories</a> as they (or, even better, <a href="https://arxiv.org/abs/1910.11189">augmented virtual double categories</a>) are what will really be behind the notion of enriched indexed categories we’ll define. Basically, we’ll secretly be spelling out a specific instance of the |\mathsf{Mod}| construction.</p>
<!--more-->
<h2 id="the-internal-language-of-monoidal-categories">The Internal Language of Monoidal Categories</h2>
<p>Fix a <a href="https://ncatlab.org/nlab/show/monoidal+category">monoidal category</a> called |\newcommand{\V}{\mathbf V}\V|.</p>
<p>The internal language of a monoidal category is quite simple to describe<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. We’ll write terms in context. The term <br /><span class="math display">$$\begin{align}
a_1 : A_1, \dots, a_n : A_n \vdash E : B\end{align}$$</span><br /> will represent an arrow |A_1 \otimes \cdots \otimes A_n \to B| in |\V|. The |n = 0| case will be represented by omitting the context and will correspond to an arrow from the unit, |I|. However, there’s a catch. The term |E| must use all the variables |a_1, \dots, a_n| <em>exactly</em> once and in the order that they are listed in the context. (The |\mathsf{match}| construct will make this more complicated. Ultimately, a one-dimensional syntax isn’t that well suited to this situation.) For example, <br /><span class="math display">$$\begin{align}a_1 : A_1, a_2: A_2 \vdash \mathsf f(a_2, a_1) : B, \quad a : A_1 \vdash \mathsf g(a, a) : B, \quad \text{and} \quad a : A \vdash \mathsf b : B\end{align}$$</span><br /> are all <em>in</em>valid. We’ll call the context the <strong>linear context</strong>, consisting of <strong>linear variables</strong> with <strong>linear types</strong>, which we’ll usually represent with the metavariable |\Delta|. The naming comes from the connections to ordered linear logic.</p>
<p>Substituting for the linear variables, written |E[E_1/a_1,\dots,E_n/a_n]|, corresponds to the composition |E \circ (E_1 \otimes \cdots \otimes E_n)|. You can work out what associativity and unit laws of composition would look like. It should be noted, though, that this is a meta-theorem. Substitution is defined in the typical, syntactic way, and we’d need to prove that for every term the interpretation of the result of substituting into that term is equal to the composition of the interpretations.</p>
<p>If we replaced arrows |A_1 \otimes \cdots \otimes A_n \to B| in a monoidal category with multiarrows |(A_1, \dots, A_n) \to B| in a multicategory<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, then we’d be done. However, monoidal categories correspond to <a href="https://ncatlab.org/nlab/show/representable+multicategory"><em>representable</em> multicategories</a>. If we write, |\mathcal C(A_1, \dots, A_n; B)| for the set of multiarrows |(A_1, \dots, A_n) \to B| in the multicatgory |\mathcal C|, then |\mathcal C| being representable means we have <br /><span class="math display">$$\begin{align}
\mathcal C(A_1, \dots, A_m, B_1, \dots, B_n, C_1, \dots, C_p; D)
    \cong \mathcal C(A_1, \dots, A_m, B_1 \otimes \cdots \otimes B_n, C_1, \dots, C_p; D)
\end{align}$$</span><br /> natural in |D| and multinatural in the |A_i| and |C_i|. This implies that every |n|-ary arrow is equivalent to a unary arrow.</p>
<p>Since it’s useful to know, I’ll go into some detail on how we derive natural-deduction-style rules from this categorical data. First, we note that the above natural isomorphism (at least when |m=0| and |p=0|) has the form of a universal property using representability and, specifically, is a mapping-out property. That is, we are saying that the functor |\mathcal C(B_1, \dots, B_n; \_)| is represented by |B_1 \otimes \cdots \otimes B_n|. Let’s call the left to right direction of the above natural isomorphism |\varphi|. While this universal property can, of course, be represented by the natural isomorphism as above, it can also be equivalently represented by a universal element. Namely, one particularly notable choice for |D| is |B_1 \otimes \cdots \otimes B_n| itself, at which point we can consider |\eta = \varphi^{-1}(id_{B_1 \otimes \cdots \otimes B_n}) : (B_1, \dots, B_n) \to B_1 \otimes \cdots \otimes B_n|. By using naturality of |\varphi^{-1}|, we can easily show that |\varphi^{-1}(f) = f \circ \eta|. To witness the fact that |\varphi^{-1}(\varphi(f)) = f|, we need |\varphi(f) \circ \eta = f|. We’ve now shown that a natural transformation |\varphi| as above and an element (in this case a multiarrow) |\eta| which satisfy |\varphi(\eta) = id| and |\varphi(f) \circ \eta = f| is equivalent to the natural isomorphism above.</p>
<p>To start translating this to rules, we look at |\eta| first. A direct translation would be to say we have the rule: <br /><span class="math display">$$\begin{align}
\dfrac{}{a_1 : A_1, \dots, a_n : A_n \vdash \eta : A_1 \otimes \cdots \otimes A_n}
\end{align}$$</span><br /> This is unnatural because it treats || like a primitive open term. This also means that to use |\eta|, we’d need to use the Cut rule (which corresponds to substitution/composition) which would stymie Cut elimination. The solution is to mix a use of Cut into the rule itself producing the term |\eta[E_1/a_1, \dots, E_n/a_n]| which I’ll write more perspicuously as |E_1 \otimes \cdots \otimes E_n|. This gives rise to the rule: <br /><span class="math display">$$\begin{align}
\dfrac{\Delta_1 \vdash E_1 : A_1 \quad \cdots \quad \Delta_n \vdash E_n : A_n}{\Delta_1,\dots,\Delta_n \vdash E_1 \otimes \cdots \otimes E_n : A_1 \otimes \cdots \otimes A_n}{\otimes_n}\text{I}
\end{align}$$</span><br /> Of course, we could restrict to just the |n=0| and |n=2| cases if we wanted. I’ll write the |n=0| case of |a_1\otimes\cdots\otimes a_n| as |*| in both the term and pattern cases. As the label for the rule, |\otimes_n I|, suggests, this is an introduction rule for |\otimes|.</p>
<p>The rule corresponding to |\varphi| takes less massaging. We’ll do the same trick of incorporating a Cut (Where?), but this makes a fairly minor difference in this case. The rule we get is: <br /><span class="math display">$$\begin{align}
\dfrac{\Delta \vdash E : A_1 \otimes \cdots \otimes A_n \quad \Delta_l, a_1 : A_1, \dots, a_n : A_n, \Delta_r \vdash E' : B}{\Delta_l, \Delta, \Delta_r \vdash \mathsf{match}\ E\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_n\ \mathsf{in}\ E' : B}{\otimes_n}\text{E},n \geq 1
\end{align}$$</span><br /> Again, as the label suggests, this is an elimination rule for |\otimes|.</p>
<p>We then need equalities for the two equations and a third equality for naturality of |\varphi|. <br /><span class="math display">$$\begin{gather}
\dfrac{\Delta_1 \vdash E_1 : A_1 \quad \cdots \quad \Delta_n \vdash E_n : A_n \quad \Delta_l, a_1 : A_1, \dots, a_n : A_n, \Delta_r \vdash E : B}{\Delta_l, \Delta_1, \dots, \Delta_n, \Delta_r \vdash (\mathsf{match}\ E_1\otimes\cdots\otimes E_n\ \mathsf{as}\ a_1\otimes\cdots\otimes a_n\ \mathsf{in}\ E) = E[E_1/a_1, \dots, E_n/a_n] : B}{\otimes}\beta
\\ \\
\dfrac{\Delta \vdash E : A_1 \otimes \cdots \otimes A_n \quad \Delta_l, a : A_1 \otimes \cdots \otimes A_n, \Delta_r \vdash E' : B}{\Delta_l, \Delta, \Delta_r \vdash E'[E/a] = (\mathsf{match}\ E\ \mathsf{as}\ a_1\otimes\cdots\otimes a_n\ \mathsf{in}\ E'[(a_1\otimes\cdots\otimes a_n)/a]) : B}{\otimes}\eta
\end{gather}$$</span><br /> The first equation corresponds to an introduction immediately followed by an elimination which is the form of a |\beta|-rule. The second is an elimination followed by an introduction and gives rise to an |\eta|-rule.</p>
<p>Since we are considering a mapping-out property, the naturality equations gives rise to what is called a commuting conversion: <br /><span class="math display">$$\begin{align}
\dfrac{\Delta \vdash E_1 : A_1 \otimes \cdots \otimes A_n \quad \Delta_l, a_1 : A_1, \dots, a_n : A_n, \Delta_r \vdash E_2 : B \quad b : B \vdash E_3 : C}{\Delta_l, \Delta, \Delta_r \vdash E_3[(\mathsf{match}\ E_1\ \mathsf{as}\ a_1 \otimes \dots \otimes a_n\ \mathsf{in}\ E_2)/b] = (\mathsf{match}\ E_1\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_n\ \mathsf{in}\ E_3[E_2/b] : C}{\otimes}\text{CC}
\end{align}$$</span><br /></p>
<p>This rule is pretty bad from the perspective of structural proof theory. If I give you terms of the forms of the left and right hand sides of the equation, it can be quite difficult and potentially ambiguous to figure out what terms |E_2| and |E_3| should be. While this particular example can’t happen in our case, imagine if |E_3| did not use the variable |b|. This technically isn’t a problem for building a derivation or verifying it as you can just require that when someone invokes this rule they must <em>specify</em> what all the meta-variables are. Still, this causes difficulties for proof search and normalization and proofs of meta-theorems.</p>
<p>The solution is straightforward enough. We simply instantiate |E_3| with a concrete term. The not-so-straightforward part is knowing which terms we need. In this case, the main rule we need (and only one if we interpret the above as covering the |n = 0| case) is the following: <br /><span class="math display">$$\begin{align}
\dfrac{\Delta \vdash E_1 : A_1 \otimes \cdots \otimes A_m \quad \Delta_l, a_1 : A_1, \dots, a_m : A_m, \Delta_r \vdash E_2 : B_1 \otimes \cdots \otimes B_n \quad \Delta_l', b_1 : B_1, \dots, b_n : B_n, \Delta_r' \vdash E_3 : C}{\Delta_l', \Delta_l, \Delta, \Delta_r, \Delta_r' \vdash (\mathsf{match}\ (\mathsf{match}\ E_1\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_m\ \mathsf{in}\ E_2)\ \mathsf{as}\ b_1 \otimes \cdots \otimes b_n\ \mathsf{in}\ E_3) \\ \qquad \qquad \qquad \quad \, = (\mathsf{match}\ E_1\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_n\ \mathsf{in}\ \mathsf{match}\ E_2\ \mathsf{as}\ b_1 \otimes \cdots \otimes b_m\ \mathsf{in}\ E_3) : C}{\otimes}{\otimes}\text{CC}
\end{align}$$</span><br /></p>
<p>You can see that this rule presents no difficulty in picking out the subterms that should correspond to the meta-variables. Fortunately, we don’t need a rule for every possible top-level term the |E_3| from the first rule could be, let alone the infinite number of possible instantiations of |E_3|. Unfortunately, we <em>do</em> need one for each elimination rule, and so the number of commuting conversions grows roughly quadratically with the number of connectives.</p>
<p>The motivation for all these rules – the |\beta| and |\eta| rules as well as the commuting conversions – is ideally to have a well-defined normal form for terms that we can systematically reach. In particular, we want two terms to have the same normal form if and only if they are semantically equivalent. If we fail to have normal forms, we’d still at least want two terms to be in the same equivalence class induced by the equations if and only if they are semantically equivalent. Often, we only consider normal forms modulo the equivalence induced by commuting conversions and then endeavor to ensure that this equivalence is easily decidable.</p>
<p>If we were to consider a mapping-in property, e.g. |\mathcal C(\_, B) \times \mathcal C(\_, C) \cong \mathcal C(\_, B \times C)| for the categorical product, the story would be very similar except with introduction and elimination switched. We’d also find that we don’t need a commuting conversion rule, though we would need to expand any existing commuting conversions with the new eliminator. You can work through it and try to find where the naturality equation went.</p>
<p>See the <a href="#appendix">appendix</a> for a full and compact listing of the rules and an explicit formulation of what it means to interpret this language into a monoidal category.</p>
<h2 id="examples">Examples</h2>
<p>Before we go on to examples of monoidal categories, let’s consider an example of a theory that we can formulate in our internal language. The main example is the theory of monoids. We assume a type |\mathsf M| and operations |\mathsf e : () \to \mathsf M| and |\mathsf m : (\mathsf M, \mathsf M) \to \mathsf M|. To this we add the equations, |\mathsf m(\mathsf e(), a) = a = \mathsf m(a, \mathsf e())| and |\mathsf m(\mathsf m(a_1, a_2), a_3) = \mathsf m(a_1, \mathsf m(a_2, a_3))|. As we can readily verify, all these equations use the free variables in order and exactly once on each side of the equations. You can contrast this to <a href="https://en.wikipedia.org/wiki/Group_(mathematics)#Definition">the axioms of a group</a> and see that those axioms aren’t valid in our internal language. A model of the theory of a monoid in a monoidal category is known as a <a href="https://ncatlab.org/nlab/show/monoid+in+a+monoidal+category">monoid object</a>. Another theory we could formulate in our internal language is <a href="https://en.wikipedia.org/wiki/Semigroup_action#Formal_definitions">the theory of a monoid action</a>.</p>
<p>For mathematicians, the archetypal example of a monoidal category is the category of vector spaces over a field |k|, e.g. the real numbers. The monoidal product is the tensor product of vector spaces with unit |k|. The multiarrows of the associated multicategory are multilinear maps. The tensor product is symmetric which corresponds to adding the <a href="https://en.wikipedia.org/wiki/Structural_rule">structural rule</a> of Exchange to our list of rules. In practice, this means while we’re still required to use each variable in the context of a term exactly once, we are no longer required to use them in order. A model of the theory of monoids in this monoidal category, i.e. a monoid object in this monoidal category, is exactly an <a href="https://en.wikipedia.org/wiki/Associative_algebra">associative, unital |k|-algebra</a>. Closely related, a ring is exactly a monoid object in the monoidal category of abelian groups.</p>
<p>Another major class of monoidal categories is cartesian monoidal categories where the monoidal product is the categorical product. The category of vector spaces has categorical products so it forms a monoidal category with that as well. Therefore part of the data of a monoidal category is a specific choice of monoidal product as there can easily be many inequivalent monoidal products. In terms of our internal language, a cartesian monoidal product corresponds to adding all the structural rules: Exchange, Weakening, and Contraction. This means that we are free to use variables however we like, i.e. we can use them in any order, ignore them, or use them multiple times. Usually, categorical products are presented in terms of a mapping-in property as I mentioned in the previous section. For a type theory, this leads to having tupling and projections. A good exercise would be to look up (or formulate) the rules for product types and show how the rules we’ve provided, in addition to the structural rules, allows us to define projections and show that the relevant equations are satisfied. A monoid object in |\mathbf{Set}| with respect to its cartesian monoidal product is exactly what we typically mean by a monoid.</p>
<p>A final example is the category of endofunctors on a given category which becomes a strict, non-symmetric monoidal category with composition as the monoidal product and the identity functor as the unit. Our internal language then provides a rather different perspective on natural transformations. A natural transformation |\tau : F \circ G \to H| is now viewed as a binary operation |\tau : (F, G) \to H|. To be clear, this is still <em>interpreted</em> as a family of (<em>unary</em>) arrows |\tau_A : F(G(A)) \to H(A)|. The action on arrows (which are natural transformations in this case) of the monoidal product is horizontal composition of natural transformations. The famous example is, of course, the natural transformations |\mu : T \circ T \to T| and |\eta : Id \to T| become the operations |\mu : (T, T) \to T| and |\eta : () \to T| and the monad laws are exactly the monoid laws. That is, a monad is a monoid object in the category of endofunctors equipped with this monoidal product.</p>
<h2 id="generalizations---virtual-bicategories-and-actegories">Generalizations - (Virtual) Bicategories and Actegories</h2>
<p>The category of endofunctors example is pretty nice, but it is weird to limit to just endofunctors. We can consider natural transformations from an arbitrary composable sequence of functors whose composite has the same source and target objects as the target functor. The source of this restriction is that our objects are (endo-)functors and the monoidal product needs to work on any pair of objects in any order. The solution to this is to use the fact that a monoidal category is exactly a one-object <a href="https://ncatlab.org/nlab/show/bicategory">bicategory</a>.</p>
<p>We can thus readily generalize to the internal language of a bicategory. As before, it was helpful to use the notion of a multicategory, at least in passing. The analogue of a multicategory in this context is a virtual bicategory. That is, a multicategory is to a monoidal category as a virtual bicategory is to a bicategory. Basically, a virtual bicategory is like a multicategory except that each object now has a specified source and target and instead of allowing arbitrary sequences as sources for multiarrows, we only allow <em>composable</em> sequences. Here, a composable sequence is a sequence of objects such that the target of one object in the sequence is the source of the next. The analogue of a representable multicategory is the existence of composites in our virtual bicategory. We can say a bicategory is a virtual bicategory that has all composites.</p>
<p>For our internal language, the only real change we need to make is to keep track of and enforce the composability constraint. One way of doing this is to modify our type formation judgement to |\vdash_S^T A| asserting that |A| is a linear type with source |S| and target |T|. Our typing judgement is similarly decorated producing |\Delta \vdash_S^T E : B|. |\Delta| is again of the form |a_1 : A_1, \dots, a_n : A_n|, but now there is the constraint that the source of |A_n| is |S|, the target of |A_1| is |T|, and the target of |A_i| is the source of |A_{i+1}| for |i &lt; n|. This leads to rules like <br /><span class="math display">$$\begin{align}
\dfrac{\Delta_1 \vdash_{T_1}^{T_0} E_1 : A_1 \quad \cdots \quad \Delta_n \vdash_{T_n}^{T_{n-1}} E_n : A_n}{\Delta_1,\dots,\Delta_n \vdash_{T_n}^{T_0} E_1 * \cdots * E_n : A_1 * \cdots * A_n}
\end{align}$$</span><br /> but nothing needs to change at the term level (though I did rename |\otimes| to |*| as that’s less misleading). The main (strict) bicategory would be |\mathbf{Cat}| where we’d interpret the linear types as functors and the linear terms as natural transformations.</p>
<p>Another direction for generalization is motivated by <a href="https://ncatlab.org/nlab/show/algebra+over+a+monad#definition">T-algebras</a> where |T| is a monad. A |T|-algebra is an arrow |\alpha : TA \to A|. If we think of |\alpha| as a binary operation, |\alpha : (T, A) \to A|, similarly to how we viewed |\mu|, the |T|-algebra laws would look like |\alpha(\eta(), a) = a| and |\alpha(\mu(x, y), a) = \alpha(x, \alpha(y, a))|. These look exactly like the laws of a <a href="https://en.wikipedia.org/wiki/Semigroup_action">monoid action</a>. The problem with this idea is that |T| and |A| are different kinds of objects; they live in different categories. One solution to this is to consider the internal language of an <a href="https://ncatlab.org/nlab/show/actegory">actegory</a>.</p>
<p>An actegory is a monoidal category, |\mathcal C|, that acts on another category, |\mathcal D|. The quickest way of describing this is to say it is a strong monoidal functor from |\mathcal C \to [\mathcal D, \mathcal D]| where the (endo-)functor category |[\mathcal D, \mathcal D]| is equipped with composition as its monoidal product. We can uncurry this functor into a bifunctor |({-})\cdot({=}) : \mathcal C \times \mathcal D \to \mathcal D| satisfying |I\cdot D \cong D| and |(C \otimes C’)\cdot D \cong C \cdot (C’ \cdot D)|. For our |T|-algebras, |\mathcal C| would be |[\mathcal D, \mathcal D]|, and the monoidal functor would just be the identity.</p>
<p>To make the internal language, we’d start by including all the rules for a monoidal category to handle the structure on |\mathcal C|. Next, we’d add a judgement |\Delta / d : D \vdash E : D’| where |\Delta = c_1 : C_1, \dots, c_n : C_n|. This would have the same restriction that all variables, |c_1, \dots, c_n| and |d|, would need to be used exactly once and in the order they were written. The idea is that this would be interpreted as an arrow in |\mathcal D| from |(C_1 \otimes \cdots \otimes C_n)\cdot D \to D’|.</p>
<p>We’d have the rules (among others): <br /><span class="math display">$$\begin{gather}
\dfrac{}{/ d : D \vdash d : D} \\ \\
\dfrac{\Delta \vdash E : C \quad \Delta' / d : D \vdash E' : D'}{\Delta, \Delta' / d : D \vdash E \cdot E' : C \cdot D'} \\ \\
\dfrac{\Delta / d : D \vdash E : C \cdot D' \quad c : C / d' : D' \vdash E' : D''}{\Delta / d : D \vdash \mathsf{match}\ E\ \mathsf{as}\ c \cdot d'\ \mathsf{in}\ E' : D''}
\end{gather}$$</span><br /></p>
<p>Presumably, you could formulate a notion of “virtual actegory” where the arrows consist of a list of objects from a multicategory |\mathcal C| and a final object from a category |\mathcal D| as their source and an object of |\mathcal D| as their target. You could imagine going further (or alternately) for an analogue of a (virtual) bicategory which would, again, amount to using composable sequences. (The name “biactegory” is already taken.)</p>
<p>Regardless, the above framework allows us to have |t : T / a : A \vdash \alpha(t, a) : A|, and we can then express our desired equations for a |T|-algebra in the form of the laws of a monoid action. One place where this notation comes in handy is in the connections between |T|-algebras and absolute colimits.</p>
<h2 id="appendix">Appendix</h2>
<h3 id="rules-for-a-monoidal-category">Rules for a Monoidal Category</h3>
<p><br /><span class="math display">$$\begin{gather}
\dfrac{\vdash A}{a : A \vdash a : A}\text{Ax} \qquad
\dfrac{\Delta_1 \vdash E_1 : A_1 \quad \cdots \quad \Delta_n \vdash E_n : A_n \quad \Delta_l, a_1 : A_1, \dots, a_n : A_n, \Delta_r \vdash E: B}{\Delta_l, \Delta_1, \dots, \Delta_n, \Delta_r \vdash E[E_1/a_1,\dots,E_n/a_n] : B}\text{Cut}
 \\ \\
\dfrac{}{\vdash I}I\text{F} \qquad
\dfrac{\vdash A_1 \quad \cdots \quad \vdash A_n}{\vdash A_1 \otimes \cdots \otimes A_n}{\otimes_n}\text{F}, n \geq 1 \qquad
\dfrac{\mathsf A : \mathsf{Type}}{\vdash \mathsf A}\text{PrimType}
 \\ \\
\dfrac{\Delta_1 \vdash E_1 : A_1 \quad \cdots \quad \Delta_n \vdash E_n : A_n \quad \mathsf f : (A_1, \dots, A_n) \to B}{\Delta_1, \dots, \Delta_n \vdash \mathsf f(E_1, \dots, E_n) : B}\text{PrimTerm}
 \\ \\
\dfrac{}{\vdash * : I}I\text{I} \qquad
\dfrac{\Delta_1 \vdash E_1 : A_1 \quad \cdots \quad \Delta_n \vdash E_n : A_n}{\Delta_1,\dots,\Delta_n \vdash E_1 \otimes \cdots \otimes E_n : A_1 \otimes \cdots \otimes A_n}{\otimes_n}\text{I}
 \\ \\
\dfrac{\Delta \vdash E : I \quad \Delta_l, \Delta_r \vdash E' : B}{\Delta_l, \Delta, \Delta_r \vdash \mathsf{match}\ E\ \mathsf{as}\ {*}\ \mathsf{in}\ E' : B}I\text{E} \qquad
\dfrac{\Delta \vdash E : A_1 \otimes \cdots \otimes A_n \quad \Delta_l, a_1 : A_1, \dots, a_n : A_n, \Delta_r \vdash E' : B}{\Delta_l, \Delta, \Delta_r \vdash \mathsf{match}\ E\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_n\ \mathsf{in}\ E' : B}{\otimes_n}\text{E},n \geq 1
\end{gather}$$</span><br /></p>
<h3 id="equations">Equations</h3>
<p><br /><span class="math display">$$\begin{gather}
\dfrac{\Delta \vdash E : B}{\Delta \vdash (\mathsf{match}\ {*}\ \mathsf{as}\ {*}\ \mathsf{in}\ E) = E : B}{*}\beta \qquad
\dfrac{\Delta \vdash E : I \quad \Delta_l, a : I, \Delta_r \vdash E' : B}{\Delta_l, \Delta, \Delta_r \vdash E'[E/a] = (\mathsf{match}\ E\ \mathsf{as}\ {*}\ \mathsf{in}\ E'[*/a]) : B}{*}\eta
 \\ \\
\dfrac{\Delta_1 \vdash E_1 : A_1 \quad \cdots \quad \Delta_n \vdash E_n : A_n \quad \Delta_l, a_1 : A_1, \dots, a_n : A_n, \Delta_r \vdash E : B}{\Delta_l, \Delta_1, \dots, \Delta_n, \Delta_r \vdash (\mathsf{match}\ E_1\otimes\cdots\otimes E_n\ \mathsf{as}\ a_1\otimes\cdots\otimes a_n\ \mathsf{in}\ E) = E[E_1/a_1, \dots, E_n/a_n] : B}{\otimes_n}\beta
\\ \\
\dfrac{\Delta \vdash E : A_1 \otimes \cdots \otimes A_n \quad \Delta_l, a : A_1 \otimes \cdots \otimes A_n, \Delta_r \vdash E' : B}{\Delta_l, \Delta, \Delta_r \vdash E'[E/a] = (\mathsf{match}\ E\ \mathsf{as}\ a_1\otimes\cdots\otimes a_n\ \mathsf{in}\ E'[(a_1\otimes\cdots\otimes a_n)/a]) : B}{\otimes_n}\eta
 \\ \\
\dfrac{\Delta \vdash E_1 : I \quad \Delta_l, \Delta_r \vdash E_2 : I \quad \Delta_l', \Delta_r' \vdash E_3 : C}{\Delta_l', \Delta_l, \Delta, \Delta_r, \Delta_r' \vdash (\mathsf{match}\ (\mathsf{match}\ E_1\ \mathsf{as}\ {*}\ \mathsf{in}\ E_2)\ \mathsf{as}\ {*}\ \mathsf{in}\ E_3) = (\mathsf{match}\ E_1\ \mathsf{as}\ {*}\ \mathsf{in}\ \mathsf{match}\ E_2\ \mathsf{as}\ {*}\ \mathsf{in}\ E_3) : C}{*}{*}\text{CC}
 \\ \\
\dfrac{\Delta \vdash E_1 : A_1 \otimes \cdots \otimes A_n \quad \Delta_l, a_1 : A_1, \dots, a_n : A_n, \Delta_r \vdash E_2 : I\quad \Delta_l', \Delta_r' \vdash E_3 : C}{\Delta_l', \Delta_l, \Delta, \Delta_r, \Delta_r' \vdash (\mathsf{match}\ (\mathsf{match}\ E_1\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_n\ \mathsf{in}\ E_2)\ \mathsf{as}\ {*}\ \mathsf{in}\ E_3) \\ \qquad \qquad \qquad \quad \, = (\mathsf{match}\ E_1\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_n\ \mathsf{in}\ \mathsf{match}\ E_2\ \mathsf{as}\ {*}\ \mathsf{in}\ E_3) : C}{\otimes_n}{*}\text{CC}
 \\ \\
\dfrac{\Delta \vdash E_1 : I \quad \Delta_l, \Delta_r \vdash E_2 : B_1 \otimes \cdots \otimes B_m \quad \Delta_l', b_1 : B_1, \dots, b_m : B_m, \Delta_r' \vdash E_3 : C}{\Delta_l', \Delta_l, \Delta, \Delta_r, \Delta_r' \vdash (\mathsf{match}\ (\mathsf{match}\ E_1\ \mathsf{as}\ {*}\ \mathsf{in}\ E_2)\ \mathsf{as}\ b_1 \otimes \cdots \otimes b_m\ \mathsf{in}\ E_3) \\ \qquad \qquad \qquad \quad \, = (\mathsf{match}\ E_1\ \mathsf{as}\ {*}\ \mathsf{in}\ \mathsf{match}\ E_2\ \mathsf{as}\ b_1 \otimes \cdots \otimes b_m\ \mathsf{in}\ E_3) : C}{*}{\otimes_m}\text{CC}
 \\ \\
\dfrac{\Delta \vdash E_1 : A_1 \otimes \cdots \otimes A_n \quad \Delta_l, a_1 : A_1, \dots, a_n : A_n, \Delta_r \vdash E_2 : B_1 \otimes \cdots \otimes B_m \quad \Delta_l', b_1 : B_1, \dots, b_m : B_m, \Delta_r' \vdash E_3 : C}{\Delta_l', \Delta_l, \Delta, \Delta_r, \Delta_r' \vdash (\mathsf{match}\ (\mathsf{match}\ E_1\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_n\ \mathsf{in}\ E_2)\ \mathsf{as}\ b_1 \otimes \cdots \otimes b_m\ \mathsf{in}\ E_3) \\ \qquad \qquad \qquad \quad \, = (\mathsf{match}\ E_1\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_n\ \mathsf{in}\ \mathsf{match}\ E_2\ \mathsf{as}\ b_1 \otimes \cdots \otimes b_m\ \mathsf{in}\ E_3) : C}{\otimes_n}{\otimes_m}\text{CC}
\end{gather}$$</span><br /></p>
<p>|\mathsf A : \mathsf{Type}| means that |\mathsf A| is a primitive type in the signature. |\mathsf f : (A_1, \dots, A_n) \to B| means that |\mathsf f| is assigned this type in the signature.</p>
<p>I did not write them, but the usual laws for equality (reflexivity and indiscernability of identicals) should be included.</p>
<p>A theory in this language is free to introduce additional linear types and linear operations.</p>
<h3 id="interpretation-into-a-monoidal-category">Interpretation into a monoidal category</h3>
<p>Write |\newcommand{\den}[1]{[\![#1]\!]}\den{-}| for the (overloaded) interpretation function. Its value on primitive operations is left as a parameter.</p>
<p>Associators for the semantic |\otimes| will be omitted below. We can arbitrarily assume a particular association of monoidal products and then the relevant associators are completely determined by the input and output types. There are multiple possible expressions for those associators, but the coherence conditions of monoidal categories guarantee that they are equal.</p>
<h4 id="interpretation-of-linear-types">Interpretation of Linear Types</h4>
<p><br /><span class="math display">$$\begin{align}
\vdash A \implies &amp; \den{A} \in \mathsf{Ob}(\V) \\ \\
\den{\Delta} =\, &amp; \den{A_1}\otimes \cdots \otimes \den{A_n} \text{ where } \Delta = a_1 : A_1, \dots, a_n : A_n \\
\den{I} =\, &amp; I \\
\den{A_1 \otimes \cdots \otimes A_n} =\, &amp; \den{A_1}\otimes \cdots \otimes \den{A_n} \\
\end{align}$$</span><br /></p>
<h4 id="interpretation-of-linear-terms">Interpretation of Linear Terms</h4>
<p><br /><span class="math display">$$\begin{align}
\Delta \vdash E : A \implies &amp; \den{E} \in \V(\den{\Delta}, \den{A}) \\ \\
\den{a} =\, &amp; id_{\den{A}} \text{ where } a : A \\
\den{*} =\, &amp; id_I \\
\den{E_1 \otimes \cdots \otimes E_n} =\, &amp; \den{E_1} \otimes \cdots \otimes \den{E_n} \text{ where }\Delta_i \vdash E_i : A_i \\
\den{\mathsf{match}\ E\ \mathsf{as}\ {*}\ \mathsf{in}\ E'} =\, &amp;
    \den{E'} \circ (id_{\den{\Delta_l}} \otimes (\lambda_{\den{\Delta_r}} \circ (\den{E} \otimes id_{\den{\Delta_r}}))) \\
\den{\mathsf{match}\ E\ \mathsf{as}\ a_1 \otimes \cdots \otimes a_n\ \mathsf{in}\ E'} =\, &amp;
    \den{E'} \circ (id_{\den{\Delta_l}} \otimes \den{E} \otimes id_{\den{\Delta_r}}) \\
\den{\mathsf f(E_1, \dots, E_n)} =\, &amp; \den{\mathsf f} \circ (\den{E_1} \otimes \cdots \otimes \den{E_n})
    \text{ where }\mathsf f\text{ is an appropriately typed linear operation}
\end{align}$$</span><br /></p>
<p>where |\lambda_B : I \otimes B \cong B| is the left unitor.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>It would be even simpler if we talked about the internal language of a multicategory…<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Every monoidal category gives rise to a multicategory in this way.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Example Representability Argument</title>
    <link href="https://derekelkins.github.io/posts/example-representability-argument.html" />
    <id>https://derekelkins.github.io/posts/example-representability-argument.html</id>
    <published>2020-05-04 10:21:49-07:00</published>
    <updated>2020-05-04T17:21:49Z</updated>
    <summary type="html"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>When I was young and dumb and first learning category theory, I got it into my head that arguments involving sets were not “categorical”. This is not completely crazy as the idea of category theory being an alternate “foundation” and categorical critiques of set theoretic reasoning are easy to find. As such, I tended to neglect techniques that significantly leveraged |\mathbf{Set}|, and, in particular, representability. Instead, I’d prefer arguments using universal arrows as those translate naturally and directly to 2-categories.</p>
<p>This was a mistake. I have long since completely reversed my position on this for both practical and theoretical reasons. Practically, representability and related techniques provide very concise definitions which lead to concise proofs which I find relatively easy to formulate and easy to verify. This is especially true when combined with the (co)end calculus. It’s also the case that for a lot of math you simply don’t need any potential generality you might gain by, e.g. being able to use an arbitrary 2-category. Theoretically, I’ve gained a better understanding of where and how category theory is (or is not) “foundational”, and a better understanding of what about set theory categorists were critiquing. Category theory as a whole does <em>not</em> provide an alternate foundation for mathematics as that term is usually understood by mathematicians. A branch of category theory, topos theory, does, but a topos is fairly intentionally designed to give a somewhat |\mathbf{Set}|-like experience. Similarly, many approaches to higher category theory still include a |\mathbf{Set}|-like level.</p>
<p>This is, of course, not to suggest ideas like universal arrows <em>aren’t</em> important or can’t lead to elegant proofs.</p>
<p>Below is a particular example of attacking a problem from the perspective of representability. I use this example more because it is a neat proof that I hadn’t seen before. There are plenty of simpler compelling examples, such as proving that right(/left) adjoints are (co)continuous, and I regularly use representability in proofs I presented on, e.g. the Math StackExchange.</p>
<h3 id="the-problem">The Problem</h3>
<p>An <a href="https://ncatlab.org/nlab/show/topos#ElementaryTopos">elementary topos</a>, |\mathcal E|, can be described as a category with finite limits and power objects. Having <a href="https://ncatlab.org/nlab/show/power+object">power objects</a> means having a functor |\mathsf P : \mathcal E^{op} \to \mathcal E| such that |\mathcal E(A,\mathsf PB) \cong \mathsf{Sub}(A \times B)| natural in |A| and |B| where |\mathsf{Sub}| is the (contravariant) functor that takes an object to its set of <a href="https://ncatlab.org/nlab/show/subobject">subobjects</a>. The action of |\mathsf{Sub}(f)| for an arrow |f : A \to B| is a function |m \mapsto f^*(m)| where |m| is a (representative) monomorphism and |f^*(m)| is the pullback of |f| along |m| which is a monomorphism by basic facts about pullbacks. In diagrammatic form: <br /><span class="math display">$$\require{AMScd}
\begin{CD}
f^{-1}(B') @&gt;f^\ast(m)&gt;&gt; A \\
@VVV @VVfV \\
B' @&gt;&gt;m&gt; B
\end{CD}$$</span><br /></p>
<p>This is a characterization of |\mathsf P| via representability. We are saying that |\mathsf PB| represents the functor |\mathsf{Sub}(- \times B)| parameterized in |B|.</p>
<p>A well-known and basic fact about elementary toposes is that they are <a href="https://ncatlab.org/nlab/show/cartesian+closed+category">cartesian closed</a>. (Indeed, finite limits + cartesian closure + a <a href="https://ncatlab.org/nlab/show/subobject+classifier">subobject classifier</a> is a common alternative definition.) Cartesian closure can be characterized as |\mathcal E(- \times A, B) \cong \mathcal E(-,B^A)| which characterizes the exponent, |B^A|, via representability. Namely, that |B^A| represents the functor |\mathcal E(- \times A, B)| parameterized in |A|. Proving that elementary toposes are cartesian closed is not too difficult, but it is a bit fiddly. This is the example that I’m going to use.</p>
<h3 id="common-setup">Common Setup</h3>
<p>All the proofs I reference rely on the following basic facts about an elementary topos.</p>
<p>We have the monomorphism |\top : 1 \to \mathsf P1| induced by the identity arrow |\mathsf P1 \to \mathsf P1|.</p>
<p>We need the lemma that |\mathcal E(A \times B,PC) \cong \mathcal E(A,\mathsf P(B \times C))|. <strong>Proof</strong>: <br /><span class="math display">$$\begin{align}
\mathcal E(A \times B,\mathsf PC) \cong \mathsf{Sub}((A \times B) \times C) \cong \mathsf{Sub}(A \times (B \times C)) \cong \mathcal E(A,\mathsf P(B \times C))\ \square
\end{align}$$</span><br /></p>
<p>Since the arrow |\langle id_A, f\rangle : A \to A \times B| is a monomorphism for any arrow |f : A \to B|, the map |f \mapsto \langle id, f \rangle| is a map from |\mathcal E(-, B)| to |\mathsf{Sub}(- \times B)|. Using |\mathsf{Sub}(- \times B) \cong \mathcal E(-,\mathsf PB)|, we get a map |\mathcal E(-, B) \to \mathsf{Sub}(- \times B) \cong \mathcal E(-,\mathsf PB)|. By Yoneda, i.e. by evaluating it at |id|, we get the singleton map: |\{\}_B : B \to \mathsf PB|. If we can show that |\{\}| is a monomorphism, then, since |\mathsf PA \cong \mathsf P(A \times 1)|, we’ll get an arrow |\sigma : \mathsf PA \to \mathsf P1| such that</p>
<p><br /><span class="math display">$$\begin{CD}
A @&gt;\{\}_A&gt;&gt; \mathsf PA \\
@VVV @VV\sigma_AV \\
1 @&gt;&gt;\top&gt; \mathsf P1
\end{CD}$$</span><br /></p>
<p>is a pullback.</p>
<p>|\{\}_A| is a monomorphism because any |f, g : X \to A| gets mapped by the above to |\langle id_X, f\rangle| and |\langle id_X, g\rangle| which represent the same subobject when |\{\} \circ f = \{\} \circ g|. Therefore, there’s an isomorphism |j : X \cong X| such that |\langle id_X, f\rangle \circ j = \langle j, f \circ j\rangle = \langle id_X, g\rangle| but this means |j = id_X| and thus |f = g|.</p>
<h3 id="other-proofs">Other Proofs</h3>
<p>To restate the problem: given the above setup, we want to show that the elementary topos |\mathcal E| is cartesian closed.</p>
<p><a href="http://www.tac.mta.ca/tac/reprints/articles/12/tr12.pdf#page=170">Toposes, Triples, and Theories</a> by Barr and Wells actually provides <em>two</em> proofs of this statement: Theorem 4.1 of Chapter 5. The first proof is in exactly the representability-style approach I’m advocating, but it relies on earlier results about how a topos relates to its slice categories. The second proof is more concrete and direct, but it also involves |\mathsf P\mathsf P\mathsf P\mathsf P B|…</p>
<p><a href="https://doi.org/10.1007/978-1-4612-0927-0">Sheaves in Geometry and Logic</a> by Mac Lane and Moerdijk also has this result as Theorem 1 of section IV.2 “The Construction of Exponentials”. The proof starts on page 167 and finishes on 169. The idea is to take the set theoretic construction of functions via their graphs and interpret that into topos concepts. This proof involves a decent amount of equational reasoning (either via diagrams or via generalized elements).</p>
<h3 id="dans-proof-via-representability">Dan’s Proof via Representability</h3>
<p>Contrast these to Dan Doel’s proof<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> using representability, which proceeds as follows. (Any mistakes are mine.)</p>
<p>Start with the pullback induced by the singleton map.</p>
<p><br /><span class="math display">$$\begin{CD}
B @&gt;\{\}_B&gt;&gt; \mathsf PB \\
@VVV @VV\sigma_BV \\
1 @&gt;&gt;\top&gt; \mathsf P1
\end{CD}$$</span><br /></p>
<p>Apply the functor |\mathcal E(= \times A,-)| which preserves the fact that it is a pullback via continuity. <br /><span class="math display">$$\begin{CD}
\mathcal{E}(-\times A,B) @&gt;&gt;&gt; \mathcal{E}(- \times A,\mathsf PB) \\
@VVV @VVV \\
\mathcal{E}(- \times A,1) @&gt;&gt;&gt; \mathcal{E}(- \times A,\mathsf P1)
\end{CD}$$</span><br /></p>
<p>Note:</p>
<ul>
<li>|\mathcal E(- \times A,1) \cong 1 \cong \mathcal E(-,1)| (by continuity)</li>
<li>|\mathcal E(- \times A,\mathsf PB) \cong \mathcal E(-,\mathsf P(A \times B))| (by definition of power objects)</li>
<li>|\mathcal E(- \times A,\mathsf P1) \cong \mathcal E(-,\mathsf PA)| (because |A \times 1 \cong A|)</li>
</ul>
<p>This means that the above pullback is also the pullback of <br /><span class="math display">$$\begin{CD}
\mathcal E(-\times A,B) @&gt;&gt;&gt; \mathcal E(-,\mathsf P(A\times B)) \\
@VVV @VVV \\
\mathcal E(-,1) @&gt;&gt;&gt; \mathcal E(-,\mathsf PA)
\end{CD}$$</span><br /></p>
<p>Since |\mathcal E| has all finite limits, it has the following pullback</p>
<p><br /><span class="math display">$$\begin{CD}
X @&gt;&gt;&gt; \mathsf P(A \times B) \\
@VVV @VVV \\
1 @&gt;&gt;&gt; \mathsf PA
\end{CD}$$</span><br /></p>
<p>where the bottom and right arrows are induced by the corresponding arrows of the previous diagram by Yoneda. Applying |\mathcal E(=,-)| to this diagram gives another pullback diagram by continuity</p>
<p><br /><span class="math display">$$\begin{CD}
\mathcal E(-,X) @&gt;&gt;&gt; \mathcal E(-,\mathsf P(A\times B)) \\
@VVV @VVV \\
\mathcal E(-,1) @&gt;&gt;&gt; \mathcal E(-,\mathsf PA)
\end{CD}$$</span><br /></p>
<p>which is to say |\mathcal E(- \times A, B) \cong \mathcal E(-, X)| because pullbacks are unique up to isomorphism, so |X| satisfies the universal property of |B^A|, namely |\mathcal E(- \times A, B) \cong \mathcal E(-,B^A)|.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Sent to me almost exactly three years ago.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Recursive Helping</title>
    <link href="https://derekelkins.github.io/posts/recursive-helping.html" />
    <id>https://derekelkins.github.io/posts/recursive-helping.html</id>
    <published>2020-04-29 23:06:52-07:00</published>
    <updated>2020-04-30T06:06:52Z</updated>
    <summary type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Recursive helping is a technique for implementing lock-free concurrent data structures and algorithms. I’m going to illustrate this in the case of implementing a multi-variable compare-and-swap (MCAS) in terms of a single variable compare-and-swap. Basically everything I’m going to talk about comes from Keir Fraser’s PhD Thesis, <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-579.html">Practical Lock-Freedom</a> (2004) which I <strong>strongly</strong> recommend. Fraser’s thesis goes much further than this, e.g. fine-grained lock-free implementations of software transactional memory (STM)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Fraser went on to contribute to the initial implementations of STM in Haskell, though his thesis uses C++.</p>
<!--more-->
<p>First, some prerequisites.</p>
<h2 id="terms">Terms</h2>
<p>I imagine most developers when they hear the term “lock-free” take it to mean a concurrent algorithm implemented without using locks. It, however, has a technical definition. Assuming a concurrent application is functionally correct, e.g. it does the right thing if it terminates no matter how things are scheduled, we still have three liveness problems in decreasing order of severity:</p>
<ul>
<li><strong>Deadlock</strong> - the application getting stuck in state where no subprocesses can be scheduled</li>
<li><strong>Livelock</strong> - the application fails to make progress despite subprocesses being scheduled, e.g. endlessly retrying</li>
<li><strong>Starvation</strong> - some subprocesses never make progress even though the application as a whole makes progress</li>
</ul>
<p>In parallel, we have three properties corresponding to programs that cannot exhibit the above behaviors:</p>
<ul>
<li><strong>Obstruction-free</strong> - no deadlock, you’ll get this if you don’t use locks</li>
<li><strong>Lock-free</strong> - obstruction-free and no livelock</li>
<li><strong>Wait-free</strong> - lock-free and no starvation</li>
</ul>
<p>Wait-freedom is the most desirable property but was difficult to achieve with reasonable efficiency. However, <a href="https://dl.acm.org/doi/10.1145/2692916.2555261">relatively recent techniques</a> (2014) may do for wait-free algorithms what Fraser’s thesis did for lock-free algorithms, namely reduce a research problem to an exercise. These techniques, however, still start from a lock-free algorithm. Obstruction-freedom is usually what you get from concurrency control mechanisms that abort and retry in the case of conflicts. To achieve lock-freedom, we need to avoid losing and redoing work, or at least doing so repeatedly indefinitely.</p>
<p>See Fraser’s thesis for more formal definitions.</p>
<p>I’ll use “<strong>lockless</strong>” to mean an algorithm implemented without using locks.</p>
<h2 id="compare-and-swap">Compare-and-Swap</h2>
<p>The usual primitive used to implement and describe lockless algorithms is <a href="https://en.wikipedia.org/wiki/Compare-and-swap">compare-and-swap</a>, often just called <code>cas</code>. There are other possibilities, but <code>cas</code> is <a href="https://dl.acm.org/doi/10.1145/114005.102808">universal</a>, relatively simple, and widely implemented, e.g. as the <code>cmpxchg</code> operation for the x86 architecture. The following is a specification of a <code>cas</code> operation in Haskell with the additional note that this intended to be performed atomically (which it would not be in Haskell).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1"></a><span class="ot">cas ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> <span class="dt">IORef</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> a</span>
<span id="cb1-2"><a href="#cb1-2"></a>cas ref old new <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>    curr <span class="ot">&lt;-</span> readIORef ref</span>
<span id="cb1-4"><a href="#cb1-4"></a>    <span class="kw">if</span> curr <span class="op">==</span> old <span class="kw">then</span> <span class="kw">do</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>        writeIORef ref new</span>
<span id="cb1-6"><a href="#cb1-6"></a>        <span class="fu">return</span> curr</span>
<span id="cb1-7"><a href="#cb1-7"></a>    <span class="kw">else</span> <span class="kw">do</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>        <span class="fu">return</span> curr</span></code></pre></div>
<h2 id="specification-of-multiple-compare-and-swap">Specification of Multiple Compare-and-Swap</h2>
<p>The specification of multiple compare-and-swap is the straightforward extension to the above to several variables.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1"></a><span class="ot">mcasSpec ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> [(<span class="dt">IORef</span> a, a, a)] <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Bool</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>mcasSpec entries <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>    eqs <span class="ot">&lt;-</span> forM entries <span class="op">$</span> \(ref, old, _) <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>        curr <span class="ot">&lt;-</span> readIORef ref</span>
<span id="cb2-5"><a href="#cb2-5"></a>        <span class="fu">return</span> (curr <span class="op">==</span> old)</span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="kw">if</span> <span class="fu">and</span> eqs <span class="kw">then</span> <span class="kw">do</span> <span class="co">-- if all equal</span></span>
<span id="cb2-7"><a href="#cb2-7"></a>        forM_ entries <span class="op">$</span> \(ref, _, new) <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb2-8"><a href="#cb2-8"></a>            writeIORef ref new</span>
<span id="cb2-9"><a href="#cb2-9"></a>        <span class="fu">return</span> <span class="dt">True</span></span>
<span id="cb2-10"><a href="#cb2-10"></a>    <span class="kw">else</span> <span class="kw">do</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>        <span class="fu">return</span> <span class="dt">False</span></span></code></pre></div>
<p>The above is, again, intended to be executed atomically. It will be convenient to allow a bit more flexibility in the type producing the type:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1"></a><span class="ot">mcas ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> [(<span class="dt">MCASRef</span> a, a, a)] <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Bool</span></span></code></pre></div>
<p>where we have</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1"></a><span class="co">-- Abstract.</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="kw">newtype</span> <span class="dt">MCASRef</span> a <span class="ot">=</span> <span class="dt">MCASRef</span> {<span class="ot"> unMCASRef ::</span> <span class="dt">IORef</span> (<span class="dt">Either</span> (<span class="dt">T</span> a) a) } <span class="kw">deriving</span> ( <span class="dt">Eq</span> )</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="ot">newMCASRef ::</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">MCASRef</span> a)</span>
<span id="cb4-5"><a href="#cb4-5"></a>newMCASRef v <span class="ot">=</span> <span class="dt">MCASRef</span> <span class="op">&lt;$&gt;</span> newIORef (<span class="dt">Right</span> v)</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="ot">readMCASRef ::</span> <span class="dt">MCASRef</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> a</span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="co">-- Will be implemented below.</span></span></code></pre></div>
<p>The idea here is that, in addition to values of type <code>a</code>, we can also store values of type <code>T a</code> into the pointers for internal use, and we can unambiguously distinguish them from values of type <code>a</code>. <code>T a</code> can be any type constructor you like.</p>
<p>In the code below, I will assume <code>IORef</code>s have an <code>Ord</code> instance, i.e. that they can be sorted. This is <em>not</em> true, but an approach as in <a href="https://hackage.haskell.org/package/ioref-stable-0.1.1.0">ioref-stable</a> could be used to accomplish this. Alternatively, <code>Ptr</code>s to <code>StablePtr</code>s could be used.</p>
<p>We won’t worry about memory consistency concerns here. That is, we’ll assume sequential consistency where all CPU cores see all updates immediately.</p>
<p>I recommend stopping here and thinking about how you would implement <code>mcas</code> in terms of <code>cas</code> while, of course, achieving the desired atomicity. The solution I’ll present – the one from Fraser’s thesis – is moderately involved, so if the approach you come up with is very simple, then you’ve probably made a mistake. Unsurprisingly, the solution I’ll present makes use of the additional flexibility in the type and the ability to sort <code>IORef</code>s. I’m not claiming it is impossible to accomplish this without these though. For example, you could apply a universal construction which witnesses the universality of <code>cas</code>.</p>
<h2 id="recursive-helping">Recursive Helping</h2>
<p>Lockless algorithms typically proceed by attempting an operation and detecting conflicts, e.g. as in multi-version concurrency control. This requires storing enough information to tell that a conflicting operation has occurred/is occurring. Once a conflict is detected, the simplest solution is typically to abort and retry hoping that there isn’t a conflict the next time. This clearly leads to the possibility of livelock.</p>
<p>Instead of aborting, the later invocation of the operation could instead help the earlier one to complete, thereby getting it out of its way. This ensures that the first invocation will always, eventually complete giving us lock-freedom. However, this doesn’t guarantee that once the second invocation finishes helping the first invocation that a third invocation won’t jump in before the second invocation gets a chance to start on its own work. In this case, the second invocation will help the third invocation to complete before attempting to start itself. A process can end up spending all its time helping other processes while never getting its own work done, leading to starvation.</p>
<p>To perform recursive helping, we need an invocation to store enough information so that subsequent, overlapping invocations are able to assist. To accomplish this, we’ll store a (pointer to a) “descriptor” containing the parameters of the invocation being helped and potentially additional information<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. This is what we’ll use for the <code>T</code> type constructor.</p>
<p>The general approach will be: at the beginning of the operation we will attempt to “install” a descriptor in the first field we touch utilizing <code>cas</code>. There are then three possible outcomes. If we fail and find a value, then the operation has failed. If we fail and find an existing descriptor, then we (potentially recursively) help that descriptor. If we succeed, then we have successfully “acquired” the field and we “help” ourselves. We can have many processes all trying to help the same invocation at the same time, so it is still important that multiple identical help calls don’t interfere with each other. Just because we’re helping an invocation of an operation doesn’t mean that that the original process isn’t still executing.</p>
<p>Since we’ll be replacing pointers to values with pointers to descriptors, reading the value becomes non-trivial. In particular, if when we read a pointer we get a descriptor, we’ll need to help the invocation described to completion. We will need to keep doing this until we successfully read a value.</p>
<h2 id="conditional-compare-and-swap">Conditional Compare-and-Swap</h2>
<p>An operation we’ll use in the implementations of <code>mcas</code> is a conditional compare-and-swap (CCAS) where we only perform the swap if, additionally, an additional variable is set to a given value. It has the following specification.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">-- Specification. Implementations should perform this atomically.</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="ot">ccasSpec ::</span> (<span class="dt">Eq</span> a, <span class="dt">Eq</span> c) <span class="ot">=&gt;</span> <span class="dt">IORef</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IORef</span> c <span class="ot">-&gt;</span> c <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb5-3"><a href="#cb5-3"></a>ccasSpec ref old new condRef check <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>    curr <span class="ot">&lt;-</span> readIORef ref</span>
<span id="cb5-5"><a href="#cb5-5"></a>    cond <span class="ot">&lt;-</span> readIORef condRef</span>
<span id="cb5-6"><a href="#cb5-6"></a>    <span class="kw">if</span> cond <span class="op">==</span> check <span class="op">&amp;&amp;</span> curr <span class="op">==</span> old <span class="kw">then</span> <span class="kw">do</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>        writeIORef ref new</span>
<span id="cb5-8"><a href="#cb5-8"></a>    <span class="kw">else</span> <span class="kw">do</span></span>
<span id="cb5-9"><a href="#cb5-9"></a>        <span class="fu">return</span> ()</span></code></pre></div>
<p>We’ll need to show that this can be implemented in terms of <code>cas</code>, or rather a version with modifications similar to those mentioned for <code>mcas</code>. This will be a simple instance of the recursive helping approach that will be applied in the implementation of <code>mcas</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">type</span> <span class="dt">CCASDescriptor</span> a c <span class="ot">=</span> <span class="dt">IORef</span> (<span class="dt">CCASRef</span> a c, a, a, <span class="dt">IORef</span> c, c)</span>
<span id="cb6-2"><a href="#cb6-2"></a></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="kw">newtype</span> <span class="dt">CCASRef</span> a c <span class="ot">=</span> <span class="dt">CCASRef</span> {<span class="ot"> unCCASRef ::</span> <span class="dt">IORef</span> (<span class="dt">Either</span> (<span class="dt">CCASDescriptor</span> a c) a) } <span class="kw">deriving</span> ( <span class="dt">Eq</span>, <span class="dt">Ord</span> )</span></code></pre></div>
<p>We begin with the types. As described above, a <code>CCASRef</code> is just an <code>IORef</code> that holds either a value or a descriptor, and the descriptor is just an <code>IORef</code> pointing at a tuple holding the arguments to <code>ccas</code>. We won’t actually modify this latter <code>IORef</code> and instead are just using it for its object identity. It could be replaced with a read-only <code>IVar</code> or a <code>Unique</code> could be allocated and used as an identifier instead. In a lower-level language, this <code>IORef</code> corresponds to having a pointer to the descriptor.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1"></a><span class="ot">newCCASRef ::</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">CCASRef</span> a c)</span>
<span id="cb7-2"><a href="#cb7-2"></a>newCCASRef v <span class="ot">=</span> <span class="dt">CCASRef</span> <span class="op">&lt;$&gt;</span> newIORef (<span class="dt">Right</span> v)</span>
<span id="cb7-3"><a href="#cb7-3"></a></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="ot">readCCASRef ::</span> (<span class="dt">Eq</span> a, <span class="dt">Eq</span> c) <span class="ot">=&gt;</span> <span class="dt">CCASRef</span> a c <span class="ot">-&gt;</span> <span class="dt">IO</span> a</span>
<span id="cb7-5"><a href="#cb7-5"></a>readCCASRef ref <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>    x <span class="ot">&lt;-</span> readIORef (unCCASRef ref)</span>
<span id="cb7-7"><a href="#cb7-7"></a>    <span class="kw">case</span> x <span class="kw">of</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>        <span class="dt">Right</span> v <span class="ot">-&gt;</span> <span class="fu">return</span> v</span>
<span id="cb7-9"><a href="#cb7-9"></a>        <span class="dt">Left</span> d <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb7-10"><a href="#cb7-10"></a>            ccasHelp d</span>
<span id="cb7-11"><a href="#cb7-11"></a>            readCCASRef ref</span>
<span id="cb7-12"><a href="#cb7-12"></a></span>
<span id="cb7-13"><a href="#cb7-13"></a><span class="co">-- Not atomic. This CAS can fail even when it would be impossible if `ccas` was truly atomic.</span></span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="co">-- Example: ccas a reference to the same value but where the condRef is False. The ccas fails</span></span>
<span id="cb7-15"><a href="#cb7-15"></a><span class="co">-- and thus should behave as a no-op, but if a `casCCASRef` occurs during the course of the ccas,</span></span>
<span id="cb7-16"><a href="#cb7-16"></a><span class="co">-- the `casCCASRef` can fail even though it should succeed in all cases.</span></span>
<span id="cb7-17"><a href="#cb7-17"></a><span class="ot">casCCASRef ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> <span class="dt">CCASRef</span> a c <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Bool</span></span>
<span id="cb7-18"><a href="#cb7-18"></a>casCCASRef (<span class="dt">CCASRef</span> ref) old new <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb7-19"><a href="#cb7-19"></a>    curr <span class="ot">&lt;-</span> cas ref (<span class="dt">Right</span> old) (<span class="dt">Right</span> new)</span>
<span id="cb7-20"><a href="#cb7-20"></a>    <span class="fu">return</span> (curr <span class="op">==</span> <span class="dt">Right</span> old)</span>
<span id="cb7-21"><a href="#cb7-21"></a></span>
<span id="cb7-22"><a href="#cb7-22"></a><span class="ot">tryReadCCASRef ::</span> <span class="dt">CCASRef</span> a c <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">Maybe</span> a)</span>
<span id="cb7-23"><a href="#cb7-23"></a>tryReadCCASRef ref <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb7-24"><a href="#cb7-24"></a>    x <span class="ot">&lt;-</span> readIORef (unCCASRef ref)</span>
<span id="cb7-25"><a href="#cb7-25"></a>    <span class="fu">return</span> (<span class="kw">case</span> x <span class="kw">of</span> <span class="dt">Left</span> _ <span class="ot">-&gt;</span> <span class="dt">Nothing</span>; <span class="dt">Right</span> v <span class="ot">-&gt;</span> <span class="dt">Just</span> v)</span></code></pre></div>
<p>To get them out of the way, the following functions implement the reference-like aspects of a <code>CCASRef</code>. The descriptor is an internal implementation detail. The interface is meant to look like a normal reference to a value of type <code>a</code>. The main notes are:</p>
<ul>
<li>since the <code>CCASRef</code> may not contain a value when we read, we loop helping to complete the <code>ccas</code> until it does,</li>
<li><code>casCCASRef</code> is a slightly simplified <code>cas</code> used in<code>mcas</code> but should be not be considered part of the interface, and</li>
<li><code>tryReadCCASRef</code> is used in the implementation of <code>mcas</code>, but you quite possibly wouldn’t provide it otherwise.</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb8-1"><a href="#cb8-1"></a><span class="ot">ccas ::</span> (<span class="dt">Eq</span> a, <span class="dt">Eq</span> c) <span class="ot">=&gt;</span> <span class="dt">CCASRef</span> a c <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IORef</span> c <span class="ot">-&gt;</span> c <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb8-2"><a href="#cb8-2"></a>ccas ref old new condRef check <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>    d <span class="ot">&lt;-</span> newIORef (ref, old, new, condRef, check)</span>
<span id="cb8-4"><a href="#cb8-4"></a>    v <span class="ot">&lt;-</span> cas (unCCASRef ref) (<span class="dt">Right</span> old) (<span class="dt">Left</span> d)</span>
<span id="cb8-5"><a href="#cb8-5"></a>    go d v</span>
<span id="cb8-6"><a href="#cb8-6"></a>  <span class="kw">where</span> go d (<span class="dt">Left</span> d&#39;) <span class="ot">=</span> <span class="kw">do</span> <span class="co">-- descriptor already there</span></span>
<span id="cb8-7"><a href="#cb8-7"></a>            ccasHelp d&#39;</span>
<span id="cb8-8"><a href="#cb8-8"></a>            v <span class="ot">&lt;-</span> cas (unCCASRef ref) (<span class="dt">Right</span> old) (<span class="dt">Left</span> d)</span>
<span id="cb8-9"><a href="#cb8-9"></a>            go d v</span>
<span id="cb8-10"><a href="#cb8-10"></a>        go d (<span class="dt">Right</span> curr) <span class="op">|</span> curr <span class="op">==</span> old <span class="ot">=</span> ccasHelp d <span class="co">-- we succeeded</span></span>
<span id="cb8-11"><a href="#cb8-11"></a>                          <span class="op">|</span> <span class="fu">otherwise</span> <span class="ot">=</span> <span class="fu">return</span> ()   <span class="co">-- we failed</span></span>
<span id="cb8-12"><a href="#cb8-12"></a>    </span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="ot">ccasHelp ::</span> (<span class="dt">Eq</span> a, <span class="dt">Eq</span> c) <span class="ot">=&gt;</span> <span class="dt">CCASDescriptor</span> a c <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb8-14"><a href="#cb8-14"></a>ccasHelp d <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb8-15"><a href="#cb8-15"></a>    (<span class="dt">CCASRef</span> ref, old, new, condRef, check) <span class="ot">&lt;-</span> readIORef d</span>
<span id="cb8-16"><a href="#cb8-16"></a>    cond <span class="ot">&lt;-</span> readIORef condRef</span>
<span id="cb8-17"><a href="#cb8-17"></a>    _ <span class="ot">&lt;-</span> cas ref (<span class="dt">Left</span> d) (<span class="dt">Right</span> <span class="op">$!</span> <span class="kw">if</span> cond <span class="op">==</span> check <span class="kw">then</span> new <span class="kw">else</span> old)</span>
<span id="cb8-18"><a href="#cb8-18"></a>    <span class="fu">return</span> ()</span></code></pre></div>
<p>Here we illustrate the (not so recursive) helping pattern. <code>ccas</code> allocates a descriptor and then attempts to “acquire” the reference. There are three possibilities.</p>
<ol type="1">
<li>We find a descriptor already there, in which case we help it and then try to acquire the reference again.</li>
<li>The CAS succeeds and thus we successfully “acquire” the reference. We then “help ourselves”.</li>
<li>The CAS fails with an unexpected (non-descriptor) value. Thus, the CCAS fails and we do nothing.</li>
</ol>
<p>Helping, implemented by <code>ccasHelp</code>, just performs the logic of CCAS. If we’ve gotten to <code>ccasHelp</code>, we know the invocation described by the descriptor did, in fact, find the expected value there. By installing our descriptor, we’ve effectively “locked out” any other calls to <code>ccas</code> until we complete. We can thus check the <code>condRef</code> at our leisure. As long as our descriptor is still in the <code>CCASRef</code>, which we check via a <code>cas</code>, we know that there have been no intervening operations, including other processes completing this <code>ccas</code>. <code>ccasHelp</code> is idempotent in the sense that running it multiple times, even in parallel, with the same descriptor is the same as running it once. This is due to the fact that we only (successfully) CAS in the descriptor once, so we can only CAS it out at most once.</p>
<h2 id="multiple-compare-and-swap">Multiple Compare-and-Swap</h2>
<p>The setup for MCAS is much the same as CCAS. The main additional complexity comes from the fact that we need to simultaneously “acquire” multiple references. This is handled by a two-phase approach. In the first phase, we attempt to “acquire” each reference. We proceed to the second phase once we’ve either seen that the MCAS is going to fail, or we have successfully “acquired” each reference. In the second phase, we either reset all the “acquired” references to their old values if the MCAS failed or to their new values if it succeeded. The MCAS will be considered to have occurred atomically at the point we record this decision, via a CAS, i.e. between the two phases.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">data</span> <span class="dt">MCASStatus</span> <span class="ot">=</span> <span class="dt">UNDECIDED</span> <span class="op">|</span> <span class="dt">FAILED</span> <span class="op">|</span> <span class="dt">SUCCESSFUL</span> <span class="kw">deriving</span> ( <span class="dt">Eq</span> )</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="kw">data</span> <span class="dt">MCASDescriptor&#39;</span> a <span class="ot">=</span> <span class="dt">MCASDescriptor</span> [(<span class="dt">MCASRef</span> a, a, a)] (<span class="dt">IORef</span> <span class="dt">MCASStatus</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="kw">type</span> <span class="dt">MCASDescriptor</span> a <span class="ot">=</span> <span class="dt">IORef</span> (<span class="dt">MCASDescriptor&#39;</span> a)</span>
<span id="cb9-5"><a href="#cb9-5"></a></span>
<span id="cb9-6"><a href="#cb9-6"></a><span class="kw">newtype</span> <span class="dt">MCASRef</span> a <span class="ot">=</span> <span class="dt">MCASRef</span> {<span class="ot"> unMCASRef ::</span> <span class="dt">CCASRef</span> (<span class="dt">Either</span> (<span class="dt">MCASDescriptor</span> a) a) <span class="dt">MCASStatus</span> }</span>
<span id="cb9-7"><a href="#cb9-7"></a>    <span class="kw">deriving</span> ( <span class="dt">Eq</span>, <span class="dt">Ord</span> )</span></code></pre></div>
<p>As with CCAS, an <code>MCASRef</code> is a reference, in this case a <code>CCASRef</code>, that either holds a value or a descriptor. The descriptor holds the arguments of <code>mcas</code>, as with <code>ccas</code>, but it additionally holds a status reference. This status reference will be used as the condition reference of the CCAS. In particular, as we’ll see, we will only perform <code>ccas</code>’s when the status is <code>UNDECIDED</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb10-1"><a href="#cb10-1"></a><span class="ot">newMCASRef ::</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">MCASRef</span> a)</span>
<span id="cb10-2"><a href="#cb10-2"></a>newMCASRef v <span class="ot">=</span> <span class="dt">MCASRef</span> <span class="op">&lt;$&gt;</span> newCCASRef (<span class="dt">Right</span> v)</span>
<span id="cb10-3"><a href="#cb10-3"></a></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="ot">readMCASRef ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> <span class="dt">MCASRef</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> a</span>
<span id="cb10-5"><a href="#cb10-5"></a>readMCASRef ref <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb10-6"><a href="#cb10-6"></a>    x <span class="ot">&lt;-</span> readCCASRef (unMCASRef ref)</span>
<span id="cb10-7"><a href="#cb10-7"></a>    <span class="kw">case</span> x <span class="kw">of</span></span>
<span id="cb10-8"><a href="#cb10-8"></a>        <span class="dt">Right</span> v <span class="ot">-&gt;</span> <span class="fu">return</span> v</span>
<span id="cb10-9"><a href="#cb10-9"></a>        <span class="dt">Left</span> d <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb10-10"><a href="#cb10-10"></a>            mcasHelp d</span>
<span id="cb10-11"><a href="#cb10-11"></a>            readMCASRef ref</span></code></pre></div>
<p>There’s nothing to say about the reference interface functions. They are essentially identical to the CCAS ones for the same reasons only with <code>CCASRef</code>s instead of <code>IORef</code>s.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb11-1"><a href="#cb11-1"></a><span class="ot">mcas ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> [(<span class="dt">MCASRef</span> a, a, a)] <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Bool</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>mcas entries <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>    status <span class="ot">&lt;-</span> newIORef <span class="dt">UNDECIDED</span></span>
<span id="cb11-4"><a href="#cb11-4"></a>    d <span class="ot">&lt;-</span> newIORef (<span class="dt">MCASDescriptor</span> (sortOn (\(ref, _, _) <span class="ot">-&gt;</span> ref) entries) status)</span>
<span id="cb11-5"><a href="#cb11-5"></a>    mcasHelp d</span></code></pre></div>
<p>The <code>mcas</code> function is fairly straightforward. It allocates a status reference and a descriptor and delegates most of the work to <code>mcasHelp</code>. The main but critical subtlety is the sort. This is critical to ensuring termination.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb12-1"><a href="#cb12-1"></a><span class="ot">mcasHelp ::</span> (<span class="dt">Eq</span> a) <span class="ot">=&gt;</span> <span class="dt">MCASDescriptor</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Bool</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>mcasHelp d <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>    <span class="dt">MCASDescriptor</span> entries statusRef <span class="ot">&lt;-</span> readIORef d</span>
<span id="cb12-4"><a href="#cb12-4"></a>    <span class="kw">let</span> phase1 [] <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb12-5"><a href="#cb12-5"></a>            _ <span class="ot">&lt;-</span> cas statusRef <span class="dt">UNDECIDED</span> <span class="dt">SUCCESSFUL</span></span>
<span id="cb12-6"><a href="#cb12-6"></a>            phase2</span>
<span id="cb12-7"><a href="#cb12-7"></a>        phase1 ((<span class="dt">MCASRef</span> ref, old, new)<span class="op">:</span>es) <span class="ot">=</span> tryAcquire ref old new es</span>
<span id="cb12-8"><a href="#cb12-8"></a></span>
<span id="cb12-9"><a href="#cb12-9"></a>        tryAcquire ref old new es <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb12-10"><a href="#cb12-10"></a>            _ <span class="ot">&lt;-</span> ccas ref (<span class="dt">Right</span> old) (<span class="dt">Left</span> d) statusRef <span class="dt">UNDECIDED</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>            v <span class="ot">&lt;-</span> tryReadCCASRef ref</span>
<span id="cb12-12"><a href="#cb12-12"></a>            <span class="kw">case</span> v <span class="kw">of</span></span>
<span id="cb12-13"><a href="#cb12-13"></a>                <span class="dt">Just</span> (<span class="dt">Left</span> d&#39;) <span class="op">|</span> d <span class="op">==</span> d&#39; <span class="ot">-&gt;</span> phase1 es <span class="co">-- successful acquisition</span></span>
<span id="cb12-14"><a href="#cb12-14"></a>                               <span class="op">|</span> <span class="fu">otherwise</span> <span class="ot">-&gt;</span> <span class="kw">do</span> <span class="co">-- help someone else</span></span>
<span id="cb12-15"><a href="#cb12-15"></a>                                    mcasHelp d&#39;</span>
<span id="cb12-16"><a href="#cb12-16"></a>                                    tryAcquire ref old new es</span>
<span id="cb12-17"><a href="#cb12-17"></a>                <span class="dt">Just</span> (<span class="dt">Right</span> curr) <span class="op">|</span> curr <span class="op">==</span> old <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb12-18"><a href="#cb12-18"></a>                    status <span class="ot">&lt;-</span> readIORef statusRef</span>
<span id="cb12-19"><a href="#cb12-19"></a>                    <span class="kw">if</span> status <span class="op">==</span> <span class="dt">UNDECIDED</span> <span class="kw">then</span> <span class="kw">do</span></span>
<span id="cb12-20"><a href="#cb12-20"></a>                        tryAcquire ref old new es <span class="co">-- failed to acquire but could still succeed</span></span>
<span id="cb12-21"><a href="#cb12-21"></a>                    <span class="kw">else</span> <span class="kw">do</span></span>
<span id="cb12-22"><a href="#cb12-22"></a>                        phase2</span>
<span id="cb12-23"><a href="#cb12-23"></a>                _ <span class="ot">-&gt;</span> <span class="kw">do</span> <span class="co">-- failed MCAS</span></span>
<span id="cb12-24"><a href="#cb12-24"></a>                    _ <span class="ot">&lt;-</span> cas statusRef <span class="dt">UNDECIDED</span> <span class="dt">FAILED</span></span>
<span id="cb12-25"><a href="#cb12-25"></a>                    phase2</span>
<span id="cb12-26"><a href="#cb12-26"></a></span>
<span id="cb12-27"><a href="#cb12-27"></a>        phase2 <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb12-28"><a href="#cb12-28"></a>            status <span class="ot">&lt;-</span> readIORef statusRef</span>
<span id="cb12-29"><a href="#cb12-29"></a>            <span class="kw">let</span> succeeded <span class="ot">=</span> status <span class="op">==</span> <span class="dt">SUCCESSFUL</span></span>
<span id="cb12-30"><a href="#cb12-30"></a>            forM_ entries <span class="op">$</span> \(<span class="dt">MCASRef</span> ref, old, new) <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb12-31"><a href="#cb12-31"></a>                casCCASRef ref (<span class="dt">Left</span> d) (<span class="dt">Right</span> (<span class="kw">if</span> succeeded <span class="kw">then</span> new <span class="kw">else</span> old))</span>
<span id="cb12-32"><a href="#cb12-32"></a>            <span class="fu">return</span> succeeded</span>
<span id="cb12-33"><a href="#cb12-33"></a></span>
<span id="cb12-34"><a href="#cb12-34"></a>    phase1 entries </span></code></pre></div>
<p><code>phase1</code> attempts to “acquire” each <code>MCASRef</code> by using <code>tryAcquire</code> which will move <code>phase1</code> to the next entry each time it succeeds. Therefore, if <code>phase1</code> reaches the end of the list and there was no interference, we will have successfully “acquired” all references. We record this with a CAS against <code>statusRef</code>. If this CAS succeeds, then the MCAS will be considered successful and conceptually to have occurred at this point. If the CAS fails, then some other process has already completed this MCAS, possibly in success or failure. We then move to <code>phase2</code>.</p>
<p><code>tryAcquire</code> can also detect that the MCAS should fail. In this case, we immediately attempt to record this fact via a CAS into <code>statusRef</code>. As with the successful case, this CAS succeeding marks the conceptual instant that the MCAS completes. As before, we then move on to <code>phase2</code>.</p>
<p>We never enter <code>phase2</code> without <code>statusRef</code> being set to either <code>SUCCESSFUL</code> or <code>FAILED</code>. <code>phase2</code> is completely straightforward. We simply set each “acquired” reference to either the new or old value depending on whether the MCAS succeeded or not. The <code>casCCASRef</code> will fail if either we never got around to “acquiring” a particular reference (in the case of MCAS failure), or if a reference was written to since it was “acquired”. Since such writes conceptually occurred after the MCAS completed, we do not want to overwrite them.</p>
<p>During <code>tryAcquire</code>, there are a few cases that lead to retrying. First, if we find that a reference has already been “acquired” by some other MCAS operation, we recursively help it. Here, the sorting of the references is important to ensure that any MCAS operation we help will never try to help us back. It’s easy to see that without the sorting, if two MCAS operations on the same two references each acquired one of the references, the (concurrent) recursive calls to <code>mcasHelp</code> would become infinite loops. With a total ordering on references, each recursive call to <code>mcasHelp</code> will be at a greater reference and thus must eventually terminate. The other case for <code>tryAcquire</code>, is that the expected value is written after the <code>ccas</code> but before the <code>tryReadCCASRef</code>. In this case, we try again unless the status has already been decided. It might seem like this is just an optimization, and that we could instead treat this as the MCAS failing. However, the intervening write may have written the value that was there before the <code>ccas</code>, meaning that there was never a point at which the MCAS could have failed.</p>
<p>References are only “acquired” at the <code>ccas</code> in <code>phase1</code>. Once the status has been decided, no references may be “acquired” any longer. Since it’s impossible to enter <code>phase2</code> without deciding the status, once one process enters <code>phase2</code>, no processes are capable of “acquiring” references. This makes <code>phase2</code> idempotent and, indeed, each CAS in <code>phase2</code> is independently idempotent. Overlapping executions of <code>phase1</code> are fine essentially because each <code>ccas</code> is idempotent and the <code>statusRef</code> can change at most once.</p>
<p>Let’s see how an <code>mcas</code> interacts with other operations from the perspective of atomicity. If we attempt to read a reference via <code>readMCASRef</code> which is included in the list of references of an ongoing <code>mcas</code>, there are two possibilities. Either that reference has not yet been “acquired” by the <code>mcas</code>, in which case the read will occur conceptually before the MCAS, or it has been “acquired” in which case the read will help the MCAS to completion and then try again after the MCAS. The story is similar for overlapping <code>mcas</code>’s. The <code>mcas</code> which “acquires” the least reference in their intersection will conceptually complete first, either because it literally finishes before the second <code>mcas</code> notices or because the second <code>mcas</code> will help it to completion. Writes are only slightly different.</p>
<p>It is important to note that these operations are NOT atomic with respect to some other reasonable operations. Most notably, they are not atomic with respect to blind writes. It is easy to construct a scenario where two blind writes happen in sequence but the first appears to happen after the <code>mcas</code> and the second before. Except for initialization, I don’t believe there are any blind writes to references involved in a <code>mcas</code> in Fraser’s thesis. Fraser’s thesis does, however, contain <code>cas</code> operations directly against these references. These are also not atomic for exactly the same reason <code>casCCASRef</code> isn’t. That said, Fraser’s uses of <code>cas</code> against <code>MCASRef</code>s are safe, because in each case they just retry until success.</p>
<h2 id="conclusion">Conclusion</h2>
<p>While I’ve gone into a good amount of detail here, I’ve mainly wanted to illustrate the concept of recursive helping. It’s a key concept for lock-free and wait-free algorithm designs, but it also may be a useful idea to have in mind when designing concurrent code even if you aren’t explicitly trying to achieve a lock-free guarantee.</p>
<!--

Scenario:

```               
               |--mcas((a, 1, 11), (b, 2, 22))---|
w(a,1)--w(b,1)----acq(a)------------------acq(b)--
--------------------------w(a,2)--w(b,2)----------
```

`w(a,2)` seems to happen after the `mcas` but `w(b,2)` seems to occur before.
-->
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>MCAS allows you to perform transactions involving multiple updates atomically. STM additionally allows you to perform transactions involving multiple reads and updates atomically.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>While I haven’t attempted it to see if it works out, it seems like you could make a generic “recursive helping” framework by storing an <code>IO</code> action instead. The “descriptors” have the flavor of defunctionalized continuations.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Beck-Chevalley</title>
    <link href="https://derekelkins.github.io/posts/beck-chevalley.html" />
    <id>https://derekelkins.github.io/posts/beck-chevalley.html</id>
    <published>2020-02-22 23:59:28-08:00</published>
    <updated>2020-02-23T07:59:28Z</updated>
    <summary type="html"><![CDATA[<p>This is a fairly technical article. This article will most likely not have any significance for you if you haven’t heard of the Beck-Chevalley condition before.</p>
<h2 id="introduction">Introduction</h2>
<p>When one talks about “indexed (co)products” in an indexed category, it is often described as follows:</p>
<p>Let |\mathcal C| be an <a href="https://ncatlab.org/nlab/show/indexed+category"><strong>|\mathbf S|-indexed category</strong></a>, i.e. a <a href="https://ncatlab.org/nlab/show/pseudofunctor">pseudofunctor</a> |\mathbf S^{op} \to \mathbf{Cat}| where |\mathbf S| is an ordinary category. Write |\mathcal C^I| for |\mathcal C(I)| and |f^* : \mathcal C^J \to \mathcal C^I| for |\mathcal C(f)| where |f : I \to J|. The functors |f^*| will be called <strong>reindexing functors</strong>. |\mathcal C| has <strong>|\mathbf S|-indexed coproducts</strong> whenever</p>
<ol type="1">
<li>each reindexing functor |f^*| has a left adjoint |\Sigma_f|, and</li>
<li>the Beck-Chevalley condition holds, i.e. whenever <br /><span class="math display">$$\require{AMScd}\begin{CD}
   I @&gt;h&gt;&gt; J \\
   @VkVV @VVfV \\
   K @&gt;&gt;g&gt; L
\end{CD}$$</span><br /> is a pullback square in |\mathbf S|, then the canonical morphism |\Sigma_k \circ h^* \to g^* \circ \Sigma_f| is an isomorphism.</li>
</ol>
<p>The first condition is reasonable, especially motivated with some examples, but the second condition is more mysterious. It’s clear that you’d need <em>something</em> more than simply a family of adjunctions, but it’s not clear how you could calculate the particular condition quoted. That’s the goal of this article. I will not cover what the Beck-Chevalley condition is intuitively saying. I cover that in <a href="https://math.stackexchange.com/a/2203383">this Stack Exchange answer</a> from a logical perspective, though there are definitely other possible perspectives as well.</p>
<p>Some questions are:</p>
<ol type="1">
<li>Where does the Beck-Chevalley condition come from?</li>
<li>What is this “canonical morphism”?</li>
<li>Why do we care about pullback squares in particular?</li>
</ol>
<!--more-->
<h2 id="indexed-functors-and-indexed-natural-transformations">Indexed Functors and Indexed Natural Transformations</h2>
<p>The concepts we’re interested in will typically be characterized by universal properties, so we’ll want an indexed notion of adjunction. We can get that by instantiating the general definition of an adjunction in any bicategory if we can make a bicategory of indexed categories. This is pretty easy since indexed categories are already described as pseudofunctors which immediately suggests a natural notion of indexed functor would be a pseudonatural transformation.</p>
<p>Explicitly, given indexed categories |\mathcal C, \mathcal D : \mathbf S^{op} \to \mathbf{Cat}|, an <strong>indexed functor</strong> |F : \mathcal C \to \mathcal D| consists of a functor |F^I : \mathcal C^I \to \mathcal D^I| for each object |I| of |\mathbf S| and a natural isomorphism |F^f : \mathcal D(f) \circ F^J \cong F^I \circ \mathcal C(f)| for each |f : I \to J| in |\mathbf S|.</p>
<p>An indexed natural transformation corresponds to a <a href="https://ncatlab.org/nlab/show/modification">modification</a> which is the name for the 3-cells between the 2-cells in the 3-category of 2-categories. For us, this works out to be the following: for each object |I| of |\mathbf S|, we have a natural transformation |\alpha^I : F^I \to G^I| such that for each |f : I \to J| the following diagram commutes <br /><span class="math display">$$\begin{CD}
\mathcal D(f) \circ F^J @&gt;id_{\mathcal D(f)}*\alpha^J&gt;&gt; \mathcal D(f) \circ G^J \\
@V\cong VV @VV\cong V \\
F^I \circ \mathcal C(f) @&gt;&gt;\alpha^I*id_{\mathcal C(f)}&gt; G^I \circ \mathcal C(f)
\end{CD}$$</span><br /> where the isomorphisms are the isomorphisms from the pseudonaturality of |F| and |G|.</p>
<h2 id="indexed-adjunctions-and-beck-chevalley">Indexed Adjunctions and Beck-Chevalley</h2>
<p>Indexed adjunctions can now be defined via the <a href="https://ncatlab.org/nlab/show/adjunction#direct_definition">unit and counit definition</a> which works in any bicategory. In particular, since indexed functors consist of families of functors and indexed natural transformations consist of families of natural transformations, both indexed by the objects of |\mathbf S|, part of the data of an indexed adjunction is a family of adjunctions.</p>
<p>Let’s work out what the additional data is. First, to establish notation, we have indexed functor |F : \mathcal D\to \mathcal C| and |U : \mathcal C \to \mathcal D| such that |F \dashv U| in an indexed sense. That means we have |\eta : Id \to U \circ F| and |\varepsilon : F \circ U \to Id| as indexed natural transformations. The first pieces of additional data, then, are the fact that |F| and |U| are indexed functors, so we have natural isomorphisms |F^f : \mathcal C(f)\circ F^J \to F^I\circ \mathcal D(f)| and |U^f : \mathcal C(f) \circ U^J \to U^I \circ \mathcal D(f)| for each |f : I \to J| in |\mathbf S|. The next pieces of additional data, or rather constraints, are the coherence conditions on |\eta| and |\varepsilon|. These work out to <br /><span class="math display">$$\begin{gather}
U^I(F^f)^{-1} \circ \eta_{\mathcal D(f)}^I = U_{F^J}^f \circ \mathcal D(f)\eta^J \qquad\text{and}\qquad
\varepsilon_{\mathcal C(f)}^I \circ F^I U^f = \mathcal C(f)\varepsilon^J \circ (F_{U^J}^f)^{-1}
\end{gather}$$</span><br /></p>
<p>This doesn’t look too much like the example in the introduction, but maybe some of this additional data is redundant. If we didn’t already know where we end up, one hint would be that |(F^f)^{-1} : F^I \circ \mathcal C(f) \to \mathcal D(f) \circ F^J| and |U^f : \mathcal D(f) \circ U^J \to U^I \circ \mathcal C(f)| look like <a href="https://ncatlab.org/nlab/show/mate">mates</a>. Indeed, it would be quite nice if they were as mates uniquely determine each other and this would make the reindexing give rise to a morphism of adjunctions. Unsurprisingly, this is the case.</p>
<p>To recall, generally, given adjunctions |F \dashv U : \mathcal C \to \mathcal D| and |F’ \dashv U’ : \mathcal C’ \to \mathcal D’|, a <strong>morphism of adjunctions</strong> from the former to the latter is a pair of functors |K : \mathcal C \to \mathcal C’| and |L : \mathcal D \to \mathcal D’|, and a natural transformation |\lambda : F’ \circ L \to K \circ F| or, equivalently, a natural transformation |\mu : L \circ U \to U’ \circ K|. You can show that there is a bijection |[\mathcal D,\mathcal C’](F’\circ L, K \circ F) \cong [\mathcal C, \mathcal D’](L \circ U, U’ \circ K)|. Concretely, |\mu = U’K\varepsilon \circ U’\lambda_U \circ \eta’_{LU}| provides the mapping in one direction. The mapping in the other direction is similar, and we can prove it is a bijection using the triangle equalities. |\lambda| and |\mu| are referred to as <strong>mates</strong> of each other.</p>
<p>In our case, |K| and |L| will be reindexing functors |\mathcal C(f)| and |\mathcal D(f)| respectively for some |f : I \to J|. We need to show that the family of adjunctions and the coherence conditions on |\eta| and |\varepsilon| force |(F^f)^{-1}| and |U^f| to be mates. The proof is as follows: <br /><span class="math display">$$\begin{align}
&amp; U^I \mathcal C(f) \varepsilon^J \circ U^I(F_{U^J}^f)^{-1} \circ \eta_{\mathcal D(f)U^J}^I &amp; \qquad \{\text{coherence of }\eta \} \\
= \quad &amp; U^I \mathcal C(f) \varepsilon^J \circ U_{F^JU^J}^f \circ \mathcal D(f)\eta_{U^J}^J &amp; \qquad \{\text{naturality of }U^f \} \\
= \quad &amp; U^f \circ \mathcal D(f)U^J\varepsilon^J \circ \mathcal D(f)\eta_{U^J}^J &amp; \qquad \{\text{functoriality of }\mathcal D(f) \} \\
= \quad &amp; U^f \circ \mathcal D(f)(U^J\varepsilon^J \circ \eta_{U^J}^J) &amp; \qquad \{\text{triangle equality} \} \\
= \quad &amp; U^f &amp;
\end{align}$$</span><br /></p>
<p>The next natural question is: if we know |(F^f)^{-1}| and |U^f| are mates, do we still need the coherence conditions on |\eta| and |\varepsilon|? The answer is “no”. <br /><span class="math display">$$\begin{align}
&amp; U_{F^J}^f \circ \mathcal D(f)\eta^J &amp; \qquad \{\text{mate of }U^f \} \\
= \quad &amp; U^I \mathcal C(f) \varepsilon_{F^J}^J \circ U^I(F_{F^J}^f)^{-1} \circ \eta^I_{\mathcal D(f)U^I} \circ \mathcal D(f)\eta^J &amp; \{\text{naturality of }\eta^I \} \\
= \quad &amp; U^I \mathcal C(f) \varepsilon_{F^J}^J \circ U^I(F_{F^J}^f)^{-1} \circ U^I F^I D(f)\eta^J \circ \eta_{\mathcal D(f)}^I &amp; \{\text{naturality of }U^I(F^f)^{-1} \} \\
= \quad &amp; U^I \mathcal C(f) \varepsilon_{F^J}^J \circ U^I\mathcal C(f)F^J\eta^J \circ U^I (F^f)^{-1} \circ \eta_{\mathcal D(f)}^I &amp; \{\text{functoriality of }U^I\mathcal C(f) \} \\
= \quad &amp; U^I \mathcal C(f)(\varepsilon_{F^J}^J \circ F^J\eta^J) \circ U^I(F^f)^{-1} \circ \eta_{\mathcal D(f)}^I &amp; \{\text{triangle equality} \} \\
= \quad &amp; U^I (F^f)^{-1} \circ \eta_{\mathcal D(f)}^I &amp;
\end{align}$$</span><br /> Similarly for the other coherence condition.</p>
<p>We’ve shown that if |U| is an indexed functor it has a left adjoint exactly when each |U^I| has a left adjoint, |F^I|, <strong><em>and</em></strong> for each |f : I \to J|, the mate of |U^f| with respect to those adjoints, which will be |(F^f)^{-1}|, is invertible. This latter condition is the Beck-Chevalley condition. As you can quickly verify, an invertible natural transformation doesn’t imply that it’s mate is invertible. Indeed, if |F| and |F’| are left adjoints and |\lambda : F’\circ L \to K \circ F| is invertible, then |\lambda^{-1} : K \circ F \to F’ \circ L| is not of the right form to have a mate (unless |F| and |F’| are also right adjoints and, particularly, an adjoint equivalence if we want to get an inverse to the mate of |\lambda|).</p>
<h2 id="comprehension-categories">Comprehension Categories</h2>
<p>We’ve answered questions 1 and 2 from above, but 3 is still open, and we’ve generated a new question: what is the <em>indexed</em> functor whose left adjoint we’re finding? The family of reindexing functors isn’t indexed by objects of |\mathbf S| but, most obviously, by <em>arrows</em> of |\mathbf S|. To answer these questions, we’ll consider a more general notion of indexed (co)products.</p>
<p>A <a href="https://ncatlab.org/nlab/show/categorical+model+of+dependent+types#comprehension_categories"><strong>comprehension category</strong></a> is a functor |\mathcal P : \mathcal E \to \mathbf S^{\to}| (where |\mathbf S^{\to}| is the <a href="https://ncatlab.org/nlab/show/arrow+category">arrow category</a>) such that |p = \mathsf{cod} \circ \mathcal P| is a <a href="https://ncatlab.org/nlab/show/Grothendieck+fibration">(Grothendieck) fibration</a> and |\mathcal P| takes (|p|-)cartesian arrows of |\mathcal E| to pullback squares in |\mathbf S^{\to}|. It won’t be necessary to know what a fibration is, as we’ll need only a few simple examples, but fibrations provide a different, and in many ways better, perspective<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> on indexed categories and being able to move between the perspectives is valuable.</p>
<p>A comprehension category can also be presented as a natural transformation |\mathcal P : \{{-}\} \to p| where |\{{-}\}| is just another name for |\mathsf{dom} \circ \mathcal P|. This natural transformation induces an indexed functor |\langle\mathcal P\rangle : \mathcal C \circ p \to \mathcal C \circ \{{-}\}| where |\mathcal C| is an |\mathbf S|-indexed category. We have <strong>|\mathcal P|-(co)products</strong> when there is an indexed (left) right adjoint to this indexed functor.</p>
<p>One of the most important fibrations is the codomain fibration |\mathsf{cod} : \mathbf S^{\to} \to \mathbf S| which corresponds to |Id| as a comprehension category. However, |\mathsf{cod}| is only a fibration when |\mathbf S| has all pullbacks. In particular, the cartesian morphisms of |\mathbf S^{\to}| are the pullback squares. However, we can define the notion of <a href="https://ncatlab.org/nlab/show/Cartesian+morphism">cartesian morphism</a> with respect to any functor; we only need |\mathbf S| to have pullbacks for |\mathsf{cod}| to be a fibration because a fibration requires you to have <em>enough</em> cartesian morphisms. However, given <em>any</em> functor |p : \mathcal E \to \mathbf S|, we have a subcategory |\mathsf{Cart}(p) \hookrightarrow \mathcal E| which consists of just the cartesian morphisms of |\mathcal E|. The composition |\mathsf{Cart}(p)\hookrightarrow \mathcal E \to \mathbf S| is always a fibration.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Thus, if we consider the category |\mathsf{Cart}(\mathsf{cod})|, this will consist of whatever pullback squares exist in |\mathbf S|. The inclusion |\mathsf{Cart}(\mathsf{cod}) \hookrightarrow \mathbf S^{\to}| gives us a comprehension category. Write |\vert\mathsf{cod}\vert| for that comprehension category. The definition in the introduction is now seen to be equivalent to having |\vert\mathsf{cod}\vert|-coproducts. That is, the indexed functor |\langle\vert\mathsf{cod}\vert\rangle| having an indexed left adjoint. The Beck-Chevalley condition is what is necessary to show that a family of left (or right) adjoints to (the components of) an indexed functor combine together into an <em>indexed</em> functor.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Indexed categories are, in some sense, a <em>presentation</em> of fibrations which are the more intrinsic notion. This means it is better to work out concepts with respect to fibrations and then see what this means for indexed categories rather than the other way around or even using the “natural” suggestions. This is why indexed categories are pseudofunctors rather than either strict or lax functors. For our purposes, we have an equivalence of 2-categories between the 2-category of |\mathbf S|-indexed categories and the 2-category of fibrations over |\mathbf S|.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Absolute Colimits</title>
    <link href="https://derekelkins.github.io/posts/absolute-colimits.html" />
    <id>https://derekelkins.github.io/posts/absolute-colimits.html</id>
    <published>2019-05-18 16:59:33-07:00</published>
    <updated>2019-05-18T23:59:33Z</updated>
    <summary type="html"><![CDATA[<p>In category theory a concept is called <strong>absolute</strong> if it is preserved by <em>all</em> functors. Identity arrows and composition are absolute by the definition of functor. Less trivially, isomorphisms are absolute. In general, anything that is described by a diagram commuting is absolute as that diagram will be preserved by any functor. This is generally the case, but if I tell you something is an absolute epimorphism, it’s not clear what diagram is causing that; the notion of epimorphism itself doesn’t reduce to the commutativity of a particular diagram.</p>
<p>Below I’ll be focused primarily on <a href="https://ncatlab.org/nlab/show/absolute+colimit">absolute colimits</a> as those are the most commonly used examples. They are an important part of the theory of <a href="https://ncatlab.org/nlab/show/monadicity+theorem">monadicity</a>. The trick to many questions about absolute colimits and related concepts is to see what it means for |\newcommand{\Set}{\mathbf{Set}}\newcommand{\Hom}{\mathsf{Hom}} \newcommand{onto}{\twoheadrightarrow}\Hom| functors to preserve them.</p>
<!--more-->
<h2 id="non-examples">Non-Examples</h2>
<p>To start, we can show that certain colimits <em>cannot</em> be absolute, at least for |\Set|-enriched category theory. In particular, initial objects and coproducts are never absolute. Using the trick above, this is easily proven.</p>
<p>\[\Hom(0,0)\cong 1 \not\cong 0\]</p>
<p>\[\Set(\Hom(X+Y,Z),1)\cong 1 \not\cong 2\cong\Set(\Hom(X,Z),1)+\Set(\Hom(Y,Z),1)\]</p>
<h2 id="absolute-epimorphisms">Absolute Epimorphisms</h2>
<p>What do absolute epimorphisms look like? We know that there <em>are</em> absolute epimorphisms because a split epimorphism is defined by a certain diagram commuting. Are there other absolute epimorphisms? To find out, we apply our trick.</p>
<p>Let |r:X\onto Y| be our epimorphism. Then we have the surjection \[\Hom(Y,r):\Hom(Y,X)\onto\Hom(Y,Y)\] but this means that for every arrow |f:Y\to Y|, there’s an arrow |s:Y\to X| such that |f = r \circ s|. As you can no doubt guess, we want to choose |f=id_Y|, and we then have that |r| is a split epimorphism. Therefore split epimorphisms are the only examples of absolute epimorphisms.</p>
<h2 id="split-coequalizers">Split Coequalizers</h2>
<p>Now let’s consider the coequalizer case. Let |f,g:X\to Y| and |e:Y\onto C| be their coequalizer which we’ll assume is absolute. Before we pull out our trick, we can immediately use the previous result to show that |e| has a section, i.e. an arrow |s : C\rightarrowtail Y| such that |id_C=e\circ s|. Moving on, we use the trick to get the diagram: \[\Hom(Y,X)\rightrightarrows\Hom(Y,Y)\onto\Hom(Y,C)\]</p>
<p>Next, we use the explicit construction of the coequalizer in |\Set| which |\Hom(Y,C)| is supposed to be canonically isomorphic to. That is, the coequalizer of |\Hom(Y,f)| and |\Hom(Y,g)| is |\Hom(Y,Y)| quotiented by the equivalence relation generated by the relation which identifies |h,k:Y\to Y| when |\exists j:Y\to X.h = f\circ j \land k = g\circ j|. Let |[h]| represent the equivalence class of |h|. The claim that |\Hom(Y,C)| is (with |\Hom(Y,e)|) a coequalizer of the above arrows implies that |e\circ h = \bar e([h])| and |[h]=\bar e^{-1}(e\circ h)| with |\bar e| and |\bar e^{-1}| forming an isomorphism. Of course, our next move is to choose |h=id_Y| giving |e=\bar e([id_Y])|. However, |e=e\circ s\circ e = \bar e([s\circ e])| so we get |[id_Y]=[s\circ e]| because |\bar e| is invertible.</p>
<p>If we call the earlier relation |\sim| and write |\sim^*| for its reflexive, symmetric, transitive closure, then |[id_Y] = \{h:Y\to Y\mid id_Y\sim^* h\}|. Therefore |id_Y \sim^* s\circ e|. Now let’s make a simplifying assumption and assume further that |id_Y \sim s\circ e|, i.e. that |id_Y| is <em>directly</em> related to |s\circ e| by |\sim|. By definition of |\sim| this means there is a |t : Y\to X| such that |id_Y = f\circ t| and |s\circ e = g\circ t|. A given triple of |f|, |g|, and |e| such that |e\circ f = e\circ g| and equipped with a |s : C\to Y| and |t : Y\to X| satisfying the previous two equations along with |q\circ s = id_C| is called a <a href="https://ncatlab.org/nlab/show/split+coequalizer"><strong>split coequalizer</strong></a>. This data is specified diagrammatically and so is preserved by all functors, thus split coequalizers are absolute. All that we need to show is that this data is enough, on its own, to show that |e| is a coequalizer.</p>
<p>Given any |q : Y\to Z| such that |q\circ f = q\circ g|, we need to show that there exists a unique arrow |C\to Z| which |q| factors through. The obvious candidate is |q\circ s| leading to us needing to verify that |q=q\circ s\circ e|. We calculate as follows: \[ \begin{align} q \circ s \circ e &amp; = q \circ g \circ t \\ &amp; = q \circ f \circ t \\ &amp; = q \end{align}\] Uniqueness then quickly follows since if |q = k\circ e| then |q\circ s = k\circ e \circ s = k|. |\square|</p>
<p>There’s actually an even simpler example where |s\circ e = id_Y| which corresponds to the trivial case where |f=g|.</p>
<h2 id="absolute-coequalizers">Absolute Coequalizers</h2>
<p>Split coequalizers show that (non-trivial) absolute coequalizers can exist, but they don’t exhaust all the possibilities. The obvious cause of this is the simplifying assumption we made where we said |id_Y\sim s\circ e| rather than |id_Y\sim^* s\circ e|. In the general case, the equivalence will be witnessed by a sequence of arrows |t_i : Y\to X| such that we have either |s\circ e = g\circ t_0| or |s \circ e = f\circ t_0|, then |f\circ t_0 = g\circ t_1| or |g\circ t_0 = f\circ t_1| respectively, and so on until we get to |f\circ t_n = id_Y| or |g\circ t_n = id_Y|. As a diagram, this is a fan of diamonds of the form |f\circ t_i = g\circ t_{i+1}| or |g\circ t_i = f\circ t_{i+1}| with a diamond with side |s\circ e| on one end of the fan and a triangle with |id_Y| on the other. All this data is diagrammatic so it is preserved by all functors making the coequalizer absolute. That it <em>is</em> a coequalizer uses the same proof as for split coequalizers except that we have a series of equalities to show that |q\circ s \circ e = q|, namely all those pasted together diamonds. There is no conceptual difficulty here; it’s just awkward to notate.</p>
<h2 id="absolute-colimits">Absolute Colimits</h2>
<p>The absolute coequalizer case captures the spirit of the general case, but you can see a description <a href="https://ncatlab.org/nlab/show/absolute+colimit#particular_absolute_colimits_2">here</a>. I’m not going to work through it, but you could as an exercise. Less tediously, you could work through absolute pushouts. If |P| is the pushout of |Y \leftarrow X \to Z|, then the functors to consider are |\Hom(P,-)| and |\Hom(Y,-)+\Hom(Z,-)|. For each, the pushout in |\Set| can be turned into a coequalizer of a coproduct. For the first functor, as before, this gives us an inverse image of |id_P| which will either be an arrow |P\to Y| or an arrow |P\to Z| which will play the role of |s|. The other functor produces a coequalizer of |\Hom(Y,Y)+\Hom(Z,Y)+\Hom(Y,Z)+\Hom(Z,Z)|. The generating relation of the equivalence relation states that there exists either an arrow |Y\to X| or an arrow |Z\to X| such that the appropriate equation holds. The story plays out much as before except that we have a sequence of arrows from two different hom-sets.</p>]]></summary>
</entry>
<entry>
    <title>Finite</title>
    <link href="https://derekelkins.github.io/posts/finite.html" />
    <id>https://derekelkins.github.io/posts/finite.html</id>
    <published>2018-03-26 00:57:41-07:00</published>
    <updated>2018-03-26T07:57:41Z</updated>
    <summary type="html"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>When sets are first introduced to students, usually examples are used with finite, explicitly presented sets. For example, #{1,2,3} uu {2,3,4} = {1,2,3,4}#. This is the beginning of the idea that a set is a “collection” of things. Later, when infinite sets are introduced, the idea that sets are “collections” of things is still commonly used as the intuitive basis for the definitions. While I personally find this a bit of a philosophical bait-and-switch, my main issue with it is that I don’t think it does a good job reflecting how we work with sets day-to-day nor for more in-depth set-theoretic investigations. Instead, I recommend thinking about infinite sets as defined by properties and #x in X# for some infinite set #X# means checking whether it satisfies the property defining #X#, <em>not</em> rummaging through the infinite elements that make up #X# and seeing if #x# is amongst them. This perspective closely reflects how we prove things about infinite sets. It makes it much clearer that the job is to find logical relationships between the properties that define sets.</p>
<p>Of course, this view can also be applied to finite sets and <em>should</em> be applied to them. For a constructivist, the notion of “finite set” splits into multiple inequivalent notions, and it is quite easy to show that there is a subset of #{0,1}# which is not “finite” with respect to strong notions of “finite” that are commonly used by constructivists. Today, though, I’ll stick with classical logic and classical set theory. In particular, I’m going to talk about the <a href="https://en.wikipedia.org/wiki/Internal_set_theory">Internal Set Theory</a> of Edward Nelson, or mostly the small fragment he used in <a href="https://web.math.princeton.edu/~nelson/books/rept.pdf">Radically Elementary Probability Theory</a>. In the <a href="https://web.math.princeton.edu/~nelson/books/1.pdf">first chapter of an unfinished book on Internal Set Theory</a>, he states the following:</p>
<blockquote>
<p>Perhaps it is fair to say that “finite” does not mean what we have always thought it to mean. What have we always thought it to mean? I used to think that I knew what I had always thought it to mean, but I no longer think so.</p>
</blockquote>
<p>While it may be a bit strong to say that Internal Set Theory leads to some question about what “finite” means, I think it makes a good case for questioning what “finite set” means. These concerns are similar to the relativity of the notion of “(un)countable set”.</p>
<!--more-->
<h3 id="minimal-internal-set-theory">Minimal Internal Set Theory</h3>
<p>Minimal Internal Set Theory (<strong>minIST</strong>) starts by taking all the axiom( schema)s of <a href="https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory"><strong>ZFC</strong></a> as axioms. So right from the get-go we can do “all of math” in exactly the same way we used to. <strong>minIST</strong> is a conservative extension of <strong>ZFC</strong>, so a formula of <strong>minIST</strong> can be translated to a formula of <strong>ZFC</strong> that is provable if and only if the original formula was provable in <strong>minIST</strong>.</p>
<p>Before going into what makes <strong>minIST</strong> different from <strong>ZFC</strong>, let’s talk about what “finite set” means in <strong>ZFC</strong> (and thus <strong>minIST</strong>). There are <a href="https://en.wikipedia.org/wiki/Finite_set">multiple equivalent characterizations of a set being “finite”</a>. One common definition is a set that has no bijection with a proper subset of itself. A more positive and more useful equivalent definition is a set is <strong>finite</strong> if and only if it has a bijection with #{k in bbbN | k &lt; n}# for some #n in bbbN#. This latter definition will be handier for our purposes here.</p>
<p>The only new primitive concept <strong>minIST</strong> adds is the concept of a “standard” natural number. That is, a new predicate symbol is added to the language of <strong>ZFC</strong>, <code class="asciimath">#sf "St"#</code>, and #n in bbbN# is <strong>standard</strong> if and only if <code class="asciimath">#sf "St"(n)#</code> holds. For convenience, we’ll define the following shorthand: <code class="asciimath">#forall^{:sf "St":}n.P(n)#</code> for <code class="asciimath">#forall n.sf "St"(n) =&gt; P(n)#</code>. This is very similar to the shorthand: #forall x in X.P(x)# which stands for #forall x.x in X =&gt; P(x)#. In fact, if we had a set of standard natural numbers then we could use this more common shorthand, but it will turn out that the existence of a set of standard natural numbers is contradictory.</p>
<p>The four additional axiom( schema)s of <strong>minIST</strong> are the following:</p>
<ol type="1">
<li><code class="asciimath">#sf "St"(0)#</code>, i.e. #0# is standard.</li>
<li><code class="asciimath">#forall n.sf "St"(n) =&gt; sf "St"(n+1)#</code>, i.e. if #n# is standard, then #n+1# is standard.</li>
<li><code class="asciimath">#P(0) ^^ (forall^{:sf "St":}n.P(n) =&gt; P(n+1)) =&gt; forall^{:sf "St":} n.P(n)#</code> for every formula #P#, i.e. we can do induction over standard natural numbers to prove things about all standard natural numbers.</li>
<li><code class="asciimath">#exists n in bbbN.not sf "St"(n)#</code>, i.e. there exists a natural number that is not standard.</li>
</ol>
<p>That’s it. There is a bit of a subtlety though. The <a href="https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory#3._Axiom_schema_of_specification_(also_called_the_axiom_schema_of_separation_or_of_restricted_comprehension)">Axiom Schema of Separation (aka the Axiom Schema of Specification or Comprehension)</a> is an axiom schema of <strong>ZFC</strong>. It is the axiom schema that justifies the notion of set comprehension, i.e. defining a subset of #X# via #{x in X | P(x)}#. This axiom schema is indexed by the formulas <em>of <strong>ZFC</strong></em>. When we incorporated <strong>ZFC</strong> into <strong>minIST</strong>, we only got the instances of this axiom schema for <strong>ZFC</strong> formulas, <em>not</em> for <strong>minIST</strong> formulas. That is, we are only allowed to form #{x in X | P(x)}# in <strong>minIST</strong> when #P# is a formula of <strong>ZFC</strong>. Formulas of <strong>minIST</strong> that are also formulas of <strong>ZFC</strong>, i.e. that don’t involve (directly or indirectly) <code class="asciimath">#sf "St"#</code> are called <strong>internal</strong> formulas. Other formulas are called <strong>external</strong>. So we can’t use the Axiom Schema of Separation to form the set of standard natural numbers via <code class="asciimath">#{n in bbbN | sf "St"(n)}#</code>. This doesn’t mean there isn’t some other way of making the set of standard natural numbers. That possibility is excluded by the following proof.</p>
<p>Suppose there is some set #S# such that <code class="asciimath">#n in S &lt;=&gt; sf "St"(n)#</code>. Using axioms 1 and 2 we can prove that #0 in S# and if #n in S# then #n+1 in S#. The <em>internal</em> form of induction, like all internal theorems, still holds. Internal induction is just axiom 3 but with #forall# instead of <code class="asciimath">#forall^{:sf "St":}#</code>. Using internal induction, we can immediately prove that #forall n in bbbN.n in S#, but this means <code class="asciimath">#forall n in bbbN.sf "St"(n)#</code> which directly contradicts axiom 4, thus there is no such #S#. #square#</p>
<p>This negative result is actually quite productive. For example, we can’t have a set #N# of nonstandard natural numbers otherwise we could define #S# as #bbbN \\ N#. This means that if we can prove some <em>internal</em> property holds for all nonstandard naturals, then there must exist a standard natural number for which it holds as well. Properties like this are called <strong>overspill</strong> because an internal property that holds for the standard natural numbers spills over into the nonstandard natural numbers. Here’s a small example. First, a real number #x# is <strong>infinitesimal</strong> if and only if there exists a nonstandard natural #nu# such that <code class="asciimath">#|x| &lt;= 1/nu#</code>. (Note, we can prove that every nonstandard natural is larger than every standard natural.) Two real numbers #x# and #y# are <strong>nearly equal</strong>, written |x \simeq y|, if and only if #x - y# is infinitesimal. A function #f : bbbR -&gt; bbbR# is <strong>(nearly) continuous at #x#</strong> if and only if for all #y in bbbR#, if |x \simeq y| then |f(x) \simeq f(y)|. Now if #f# is continuous at #x#, then for every standard natural #n# and #y in bbbR#, <code class="asciimath">#|x - y| &lt;= delta =&gt; |f(x) - f(y)| &lt;= 1/n#</code> holds for all infinitesimal #delta# by continuity. Thus by overspill it holds for some non-infinitesimal #delta#, i.e. for some #delta &gt; 1/m# for some standard #m#. #square#</p>
<p>In Radically Elementary Probability Theory, Nelson reformulates much of probability theory and stochastic process theory by restricting to the finite case. Traditionally, probability theory for finite sample spaces is (relatively) trivial. But remember, “finite” means bijective with #{k in bbbN | k &lt; n}# for some #n in bbbN# and that #n# can be nonstandard in <strong>minIST</strong>. The notion of “continuous” defined before translates to the usual notion when the <strong>minIST</strong> formula is translated to <strong>ZFC</strong>. Similarly, subsets of infinitesimal probability become subsets of measure zero. Sums of infinitesimals over finite sets of nonstandard cardinality become integrals. Little to nothing is lost, instead it is just presented in a different language, a language that often lets you say complex things simply.</p>
<p>Again, I want to reiterate that <em>everything true in</em> <strong>ZFC</strong> <em>is true in</em> <strong>minIST</strong>. One of the issues with Robinson-style nonstandard analysis is that the hyperreals are not an Archimedean field. An ordered field #F# is Archimedean if for all #x in F# such that #x &gt; 0# there is an #n in bbbN# such that #nx &gt; 1#. The hyperreals serve as a model of the (<strong>min</strong>)<strong>IST</strong> reals. <em>Within</em> <strong>minIST</strong>, the Archimedean property holds. Even if #x in bbbR# is infinitesimal, there is still a nonstandard #n in bbbN# such that #nx &gt; 1#. But this statement translates to the statement in the meta-language that there is a hypernatural #n# such that for every positive hyperreal #x#, #nx &gt; 1# which does hold. However, if we use the <em>meta-language’s</em> notion of “natural number”, this statement is false.</p>
<h3 id="internal-set-theory">Internal Set Theory</h3>
<p>Full Internal Set Theory (<strong>IST</strong>) only has three axiom schemas in addition to the axioms of <strong>ZFC</strong>. Like <strong>minIST</strong>, there is <code class="asciimath">#sf "St"#</code> but now we talk about standard sets in general, not just standard natural numbers. The three axiom schemas are as follows:</p>
<ol type="1">
<li><code class="asciimath">#forall^{:sf "St":}t_1 cdots forall^{:sf "St":}t_n.(forall^{:sf "St":} x.P(x, t_1, ..., t_n)) &lt;=&gt; forall x.P(x, t_1, ..., t_n)#</code> for internal #P# with no (other) free variables.</li>
<li><code class="asciimath">#(forall^{:sf "StFin":}X.exists y.forall x in X.P(x, y)) &lt;=&gt; exists y.forall^{:sf "St"} x.P(x, y)#</code> for internal #P# which may have other free variables.</li>
<li><code class="asciimath">#forall^{:sf "St":}X.exists^{:sf "St"} Y.forall^{:sf "St":}z.(z in Y &lt;=&gt; z in X ^^ P(z))#</code> for <em>any</em> formula #P# which may have other free variables.</li>
</ol>
<p>These are called the Transfer Principle (T), the Idealization Principle (I), and the Standardization Principle (S) respectively. <code class="asciimath">#forall^{sf "StFin"}X.P(X)#</code> means for all #X# which are both standard and finite. The Transfer Principle essentially states that an internal property is true for all sets if and only if it is true for all standard sets. The Transfer Principle allows us to prove that any internal property that is satisfied uniquely is satisfied by a standard set. This means everything we can define in ZFC is standard, e.g. the set of reals, the set of naturals, the set of functions from reals to naturals, any particular natural, #pi#, the Riemann sphere.</p>
<p>The Idealization Principle is what allows nonstandard sets to exist. For example, let #P(x, y) -= y in bbbN ^^ (x in bbbN =&gt; x &lt; y)#. The Idealization Principle then states that if for any finite set of natural numbers, we can find a natural number greater than all of them, then there is a natural number greater than any standard natural number. Since we clearly can find a natural number greater than any natural number in some given finite set of natural numbers, the premise holds and thus we have a natural number greater than any standard one and thus necessarily nonstandard. In fact, a similar argument can be used to show that <em>all</em> infinite sets have nonstandard elements. A related argument shows that a standard, finite set is exactly a set all of whose elements are standard.</p>
<p>The Standardization Principle isn’t needed to derive <strong>minIST</strong> nor is it needed for the following result so I won’t discuss it further. Another result derivable from Idealization is: there is a finite set that contains every standard set. To prove it, simply instantiate the Idealization Principle with <code class="asciimath">#P(x,y) -= x in y ^^ y\ "is finite"#</code> where “is finite” stands for the formalization of any of the definitions of “finite” given before. It is in the discussion after proving this that Nelson makes the statement quoted in the introduction. We can prove some results about this set. First, it can’t be standard itself. Here are three different reasons why: 1) if it were standard, then it would include itself violating the Axiom of Foundation/Regularity; 2) if it were standard, then by Transfer we could prove that it contains <em>all</em> sets again violating the Axiom of Foundation; 3) if it were standard, then by the result mentioned in the previous paragraph, it would contain <em>only</em> standard elements, but then we could intersect it with #bbbN# to get the set of standard naturals which does not exist. Like the fact that there is no smallest nonstandard natural, there is no smallest finite set containing all standard sets.</p>
<p><strong>IST</strong> also illustrates another concept. In set theory we often talk about classes. My experience with this concept was a series of poor explanations: a class was “the extent of a predicate”, “the ‘collection’ of entities that satisfy a predicate” and a proper class was a “collection” that was “too big” to be a set like the class of all sets. I muddled on with vague descriptions like this for quite some time before I finally figured out what they meant. Harking back to my first paragraph, a <strong>class</strong> (in <strong>ZFC</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>) is just a (unary) predicate, i.e. a formula (of <strong>ZFC</strong>) with one free variable, or at least it can be represented by such<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. A formula #P(x)# is a <strong>proper class</strong> if there is no set #X# such that #forall x.P(x) &lt;=&gt; x in X#. The “class of all sets” is simply the constantly true predicate. This is a proper class because a set #U# such that #forall x.x in U# leads to a contradiction, e.g. Russell’s paradox by warranting #{x in U | x notin x}# via the Axiom of Separation. It may have already dawned on you that <code class="asciimath">#sf "St"(x)#</code> is a class in (<strong>min</strong>)<strong>IST</strong> and, in fact, a <em>proper</em> class. All of those times when I said “there is no set of standard naturals/nonstandard naturals/infinitesimals/standard sets”, I could equally well have said “there is a proper class of standard naturals/nonstandard naturals/infinitesimals/standard sets”. The result of the previous paragraph means the (proper) class of standard sets – far from being “too big” to be a set – is a subclass of (the class induced by) a finite set! Classes were never about “size”<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>To reiterate, <strong>IST</strong> is a <em>conservative extension</em> of <strong>ZFC</strong>. Focusing first on the “conservative” part, when we translate the theorem that there is a finite set containing all standard sets to <strong>ZFC</strong>, it becomes the tautologous statement that every finite set is a subset of some finite set. This starts to reveal what is happening in <strong>IST</strong>. However, focusing on the “extension” part, <em>nothing in</em> <strong>ZFC</strong> <em>refutes any of</em> <strong>IST</strong>. To take a Platonist perspective, these nonstandard sets could “always have been there” and <strong>IST</strong> just finally lets us talk about them. If you were a Platonist but didn’t want to accept these nonstandard sets, the conservativity result would still allow you to use <strong>IST</strong> as “just a shorthand” for formulas in <strong>ZFC</strong>. Either way, it is clear that the notion of “finite set” in <strong>ZFC</strong> has a lot of wiggle room.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Some set theories have an explicit notion of “class”.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>The point is that any logically equivalent formula would do, i.e. classes are equivalence classes of formulas under logical equivalence. One could arguably go further and say that the two formulas represent the same class even if we can’t <em>prove</em> that they are logically equivalent as long as we (somehow) know they are “true” for exactly the same things. This extensional view of predicates is what the “extent” stuff is about.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>On the other hand, the Axiom Schema of Specification states that any subclass of a set is a set in ZFC, so for ZFC no proper class can be contained in a set. In fact, many non-traditional systems can be understood as only allowing certain subclasses of “sets” to be “sets”. For example, in constructive recursive mathematics, we may require a “set” to be recursively enumerable, but we can certainly have subclasses of recursively enumerable sets that are not recursively enumerable. See also <a href="https://mathoverflow.net/questions/299120/are-classes-still-larger-than-sets-without-the-axiom-of-choice">this</a> discussion on the Axiom of Choice and “size”.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>The Pedagogy of Logic: A Rant</title>
    <link href="https://derekelkins.github.io/posts/the-pedagogy-of-logic-a-rant.html" />
    <id>https://derekelkins.github.io/posts/the-pedagogy-of-logic-a-rant.html</id>
    <published>2018-02-28 20:35:14-08:00</published>
    <updated>2018-03-01T04:35:14Z</updated>
    <summary type="html"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>Over the years I’ve seen a lot of confusion about formal logic online. Usually this is from students (but I’ve also seen it with experienced mathematicians) on platforms like the Math or CS StackExchange. Now there is clearly some selection bias where the people asking the questions are the people who are confused, but while the questions <em>about</em> the confusions are common, the confusions are often evident even in questions about other aspects, the confusions are <em>in the (upvoted!) answers</em>, and when you look at the study material they are using it is often completely unsurprising that they are confused. Again, these confusions are also not limited to just “students”, though I’ll use that word in a broad sense below. Finally, on at least one of the points below, the confusion seems to be entirely on the instructors’ side.</p>
<p>An indication that this is true more broadly is this quote from the <a href="http://ncatlab.org/nlab/revision/constructive%20mathematics/64">nLab</a>:</p>
<blockquote>
<p>The distinction between object language and metalanguage exists even in classical mathematics, but it seems that most classical mathematicians are not used to remembering it, although it is not entirely clear why this should be so. One possibly relevant observation is that even if <span class="math inline"><em>P</em></span> is a statement which is neither provable nor disprovable (like the continuum hypothesis), in classical mathematics it is still provable that “<span class="math inline"><em>P</em></span> is either true or false.” Moreover, classical model theory often restricts itself to two-valued models in which the only truth values are “true” and “false,” although classical logic still holds in Boolean-valued models, and in such a case the truth value of <span class="math inline"><em>P</em></span> may be neither “true” nor “false,” although the truth value of “<span class="math inline"><em>P</em></span> or not <span class="math inline"><em>P</em></span>” is still “true.” Certainly when talking about classical truths which fail constructively, such as excluded middle, it is important to remember that “fails to be provable” is different from “is provably false.”</p>
</blockquote>
<p>To give an indication of the problem, here is my strong impression of what would happen if I gave a student who had just passed a full year introduction to formal logic the following exercise: “Give me a formal proof of #(neg P =&gt; neg Q) =&gt; (Q =&gt; P)#”. I suspect most would draw up a truth table. When I responded, “that’s not a formal proof,” they would be confused. If you are confused by that response, then this is definitely for you. I’m sure they would completely agree with me that if I asked for the inverse matrix of some matrix #M#, showing that the determinant of #M# is non-zero does not constitute an answer to that question. The parallelism of these scenarios would likely be lost on them though. While I think courses that focus on a syntactic approach to logic will produce students that are much more likely to give an appropriate answer to my exercise, that doesn’t mean they lack the confusions, just that this exercise isn’t a good discriminator for them. For example, if they don’t see the parallelism of the two scenarios I described, or worse, have no idea what truth tables have to do with anything, then they have some gap.</p>
<p>As a disclaimer, I am not an educator nor even an academic in any professional capacity.</p>
<p>As a summary, the points I will touch on are:</p>
<ul>
<li><a href="#syntax-versus-semantics">Not <em>continually</em> emphasizing the distinction between syntax and semantics</a>.</li>
<li><a href="#unnecessary-philosophizing">Unnecessary philosophizing</a>.</li>
<li><a href="#the-complete-absence-of-non-classical-logics">The complete absence of non-classical logics</a>.</li>
<li><a href="#silly-linguistics">Silly linguistics exercises</a>.</li>
<li><a href="#the-complete-conflation-of-negation-introduction-with-double-negation-elimination">The complete conflation of negation introduction with double negation elimination</a>.</li>
<li><a href="#overuse-of-indirect-proof">Overuse of indirect proof</a>.</li>
<li><a href="#smaller-issues">Some smaller issues that are not so bad and some missed opportunities</a>.</li>
</ul>
<p>If anyone reading this is aware of <em>introductory</em> textbooks or other resources (ideally freely available, but I’ll take a reasonably priced textbook too) that avoid most or all of the major issues I list and otherwise do a good job, I would be very interested. My own education on these topics has been piecemeal and eclectic and also strongly dependent on my background in programming. This leaves me with no reasonable recommendations. Please leave a comment or email me, if you have such a recommendation.</p>
<!--more-->
<h3 id="syntax-versus-semantics">Syntax versus Semantics</h3>
<p>The distinction between syntax and semantics is fundamental in logic. Many, many students, on the other hand, seem completely unaware of the distinction. The most blatant form of this is that for many students it seems quite clear that “proving a formula true” <em>means</em> plugging in truth values and seeing that the result is always “true”. Often they (seemingly) think a truth table <em>is</em> a formal proof. They find it almost impossible to state what the difference between provable and “true” is. The impression I get from some is that they think plugging truth values into formulas <em>is</em> what logic is about.</p>
<p>If the students get to classical predicate logic, most of these misunderstandings will be challenged, but students with this misunderstanding will struggle unnecessarily, and you can see this on sites like <a href="https://math.stackexchange.com">math.stackexchange.com</a> where questioners <a href="https://math.stackexchange.com/q/2646421/305738">present attempts to prove statements of predicate logic by reducing them to propositional statements</a>. It’s not clear to me what typically happens to students with such misunderstandings as they gain proficiency with classical predicate logic. Certainly some clear up their misunderstanding, but since it’s clear that some still have a slippery grasp of the distinction between syntax and semantics, some misunderstanding remains for them. My guess would be that many just view classical propositional logic and classical predicate logic as more different than they are with unrelated approaches to proof. Any instructors reading this can probably describe much stranger rationalizations that students have come up with, and perhaps give some indications of what faulty rationalizations are common.</p>
<p>Speaking of instructors, the pedagogical problem I see here is not that the textbook authors and instructors don’t understand these distinctions, or even that they don’t <em>present</em> these distinctions, but that they don’t continually <em>emphasize</em> these distinctions. Maybe they do in the classroom, but going by the lecture notes or books, the notation and terminology that many use make it <em>very</em> easy to conflate syntax and semantics. For understandable reasons, proofs of the soundness and completeness theorems that are necessary to justify this conflation are pushed off until much later.</p>
<p>I googled <code>lecture notes "classical propositional logic"</code> and looked at the <a href="http://www.philosophy.ed.ac.uk/undergraduate/documents/Propositional_Logic_2008_09.pdf">first hit I got</a>. It clearly has a section on semantics and multiple sections on a (syntactic) deductive system “helpfully” named “semantic tableaux”. It bases all concepts on truth tables introduced in section 4. Later concepts are justified by corresponding to the truth table semantics. Here is how it describes the semantics for negation:</p>
<blockquote>
<p>A negation #neg A# is <strong>true</strong> (in any situation) if and only if #A# is <strong>not true</strong> (in that situation).</p>
</blockquote>
<p>This is followed by the sentence: “For the moment, we shall suppress the qualifier ‘in any situation’[…]”, and we get the rules for the other connectives starting with the one for conjunction:</p>
<blockquote>
<p>A conjunction #A ^^ B# is <strong>true</strong> if and only if #A# is true and #B# is true.</p>
</blockquote>
<p>It later explicates the “in any situation” by talking about truth-value assignments and giving “rules” like:</p>
<blockquote>
<p>#A ^^ B# is true in an assignment iff #A# and #B# are true in that assignment.</p>
</blockquote>
<p>Between these two renditions of the rule for conjunction, the notes tell us what a “truth-value” is and seemingly what “is true” means with the following text:</p>
<blockquote>
<p>In classical propositional logic, formulas may have only <strong>one</strong> of two possible “<strong>truth values</strong>”. Each formula must be <strong>either true</strong> or <strong>false</strong>; and no formula may be both true and false. These values are written <strong>T</strong> and <strong>F</strong>. (Some authors use #1# and #0# instead. Nothing hinges on this.)</p>
</blockquote>
<p>The description so far makes it sound like logical formula are “expressions” which “compute” to <strong>true</strong> or <strong>false</strong>, which will also be written <strong>T</strong> or <strong>F</strong>, like arithmetical expressions. This is made worse in section 11 where the connectives are identified with truth-functions (which is only mentioned in this early section). It would be better to have a notation to distinguish a formula from its truth value e.g. #v(A)# or, the notation I like, |[\! [A]\!]|. (As I’ll discuss in the next section, it would also be better to avoid using philosophically loaded terms like “true” and “false”. #1# and #0# are better choices, but if you <em>really</em> wanted to drive the point home, you could use arbitrary things like “dog” and “cat”. As the authors state, “nothing hinges on this”.) Rewording the above produces a result that I believe is much harder to misunderstand.</p>
<blockquote>
<p>#v(A ^^ B)=1# if and only if #v(A)=1# and #v(B)=1#.</p>
</blockquote>
<p>This is likely to be perceived as less “intuitive” to students, but I strongly suspect the seeming cognitive ease was masking actually grappling with what is being said in the earlier versions of the statement. If we actually want to introduce truth functions, I would highly recommend using different notation for them, for example:</p>
<blockquote>
<p><code class="asciimath">#v(A ^^ B)=sf "and"(v(A),v(B))#.</code></p>
</blockquote>
<p>Of course, we can interpret “#A# is true” as defining a unary predicate on formulas, “_ is true”, but I seriously doubt this is where most students’ minds jump to when being introduced to formal logic, and this view is undermined by the notion of “truth values”. Again, I’m not saying the authors are saying things that are <em>wrong</em>, I’m saying that they make it very easy to misunderstand what is going on.</p>
<p>Following this, most of the remainder of the notes discusses the method of semantic tableaux. It mentions soundness and completeness but also states: “In this introductory course, we do not expect you to study the proof of these two results[.]”) At times it suggests that the tableaux method can stand on its own: “However, we can show that it is inconsistent <strong>without using a truth table</strong>, by a form of <strong>deductive reasoning</strong>. That is, by following computer-programmable <strong>inference rules</strong>.” But the method is largely presented as summarizing semantic arguments: “This rule just summarizes the information in the truth table that…” In my ideal world, this more deductive approach would be clearly presented as an <em>alternative</em> approach to thinking about logic with the inference rules not explained in terms of semantic notions but related to them after the fact.</p>
<p>My point isn’t to pick on these notes which seem reasonable enough overall. They have some of the other issues I’ll cover but also include some of the things I’d like to see in such texts.</p>
<h3 id="unnecessary-philosophizing">Unnecessary Philosophizing</h3>
<p>As mentioned in the previous section, words like “true” and “false” have a huge philosophical weight attached to them. It is quite easy to define any particular logic either syntactically or semantically without even suggesting the identification of anything with the philosophical notion of “truth”. My point isn’t that philosophy should be banished from a mathematical logic course, though that <em>is</em> an option, but that we can <em>separate</em> the definition of a logical system with how it relates to the intuitive or philosophical notion of “truth”. This allows us to present logics as the mathematical constructions they are and not feel the need to justify why any given rules/semantics really are the “right” ones. Another way of saying it is that your philosophical understanding/stance doesn’t change the definition of classical propositional logic. It just changes whether that logic is a good reflection of your philosophical views.</p>
<p>(As an aside, while I did read introductions to logic early on, the beginnings of my in-depth understanding of logic arose via programming language theory and type systems [and then category theory]. These areas often cover the same concepts and systems but with little to no philosophizing. Papers discuss classical and constructive logics with no philosophical commitment for or against the law of excluded middle being remotely implied.)</p>
<p>Of course, we do want to leverage our intuitive understanding of logic to motivate either the inference rules or the chosen semantics, but we can phrase the definitions as being <em>inspired by</em> the intuition rather than codifying it. Things like the <a href="https://math.stackexchange.com/q/232309/305738">common misgivings</a> students (and <a href="https://en.wikipedia.org/wiki/Material_conditional#Philosophical_problems_with_material_conditional">others</a>) have about material implication are a lot easier to deal with when logic isn’t presented as “The One True Formulation of Correct Reasoning”. Of course another way to undermine this impression would be to present a variety of logics including ones where material implication doesn’t hold. This is also confounded by unnecessary philosphizing. It makes it seem like non-classical logics are only of interest to crazy contrarians who reject “self-evident” “truths” like every statement is either “true” or “false”. While there definitely is a philosophical discussion that could be had here, there are plenty of <em>mathematical</em> reasons to be interested in non-classical logics regardless of your philosophical stance on truth. Non-classical logics can also shed a lot of light on what’s going on in classical logics.</p>
<p>This assumes non-classical logics are even mentioned at all.</p>
<h3 id="the-complete-absence-of-non-classical-logics">The Complete Absence of Non-Classical Logics</h3>
<p>I’m highly confident you can go through many introductory courses on logic and never have any inkling that there are logics other than classical logics. I suspect you can easily go through an entire math major and also be completely unaware of non-classical logics. For an introductory course on logic, I completely understand that time constraints can make it difficult to include any substantial discussion of non-classical logics. I somewhat less forgiving when it comes to textbooks.</p>
<p>What bothers me isn’t that non-classical logics aren’t given equal time - I wouldn’t expect them to be - it’s that classical logic is presented as a comprehensive picture of logic. This is likely, to some degree, unintentional, but it is easily fixable. First, at the beginning of the course, simply mention that the course will cover classical logics and there are other logics like constructive or paraconsistent logics that have different rules and different notions of semantics. Use less encompassing language. Instead of saying “logic”, say “a logic” or “the logic we’re studying”. Similarly, talk about “<em>a</em> semantics” rather than “<em>the</em> semantics”, and “<em>a</em> set of rules” rather than “<em>the</em> rules”. Consistently say “<em>classical</em> propositional/first-order/predicate logic” rather than just “propositional/first-order/predicate logic” or worse just “logic”. Note points where non-classical logics make different choices when the topics come up. For example, that truth tables are a semantics for classical propositional logic while non-classical logics will use a different notion of semantics. The law of excluded middle, indirect proof, and material implication are other areas where a passing mention of how different logics make different decisions about these topics could be made. The principle of explosion, related to the issues with material implication, is an interesting point where classical and constructive logics typically agree while paraconsistent logics differ. Heck, I suspect some people find my use of the plural, “logics”, jarring, as if there could be more than one logic. Fancy that!</p>
<p>The goal with this advice is just to indicate to students that there is a wider world of logic out there. I don’t expect students to be quizzed on this. Some of this advice is relevant even purely for classical logics. It’s not uncommon for people to ask questions on the Math StackExchange asking for a proof of some formula. When asked what rules they are using, they seem confused: “The rules of predicate logic(?!)” The idea that the presentation of (classical) predicate logic that they were given is only one of many possibilities seems never to have occurred to them. Similarly for semantics. So generally when a choice is made, it would seem of long-term pedagogical value to at least mention that a choice has been made and that other choices are possible.</p>
<h3 id="silly-linguistics">Silly Linguistics</h3>
<p>Nearly any week on the Math StackExchange you can <a href="https://math.stackexchange.com/q/2270547/305738">find</a> <a href="https://math.stackexchange.com/q/2536711/305738">someone</a> <a href="https://math.stackexchange.com/q/2589373/305738">asking</a> <a href="https://math.stackexchange.com/q/2549016/305738">about</a> an <a href="https://math.stackexchange.com/q/2658713/305738">exercise like the following</a>:</p>
<blockquote>
<p>Translate “Jan likes Sally but not Alice” to a logical formula.</p>
</blockquote>
<p>Exercises like these are just a complete and utter waste of time. The only people who <em>might</em> do something like this are linguists. Even then the “linguistics” presented in introductions to formal logic is (understandably) ridiculously naive, and an introductory course on formal logic is simply not the place for it anyway. What the vast majority of consumers of formal logic <em>do</em> need is to 1) be able to understand what <em>formal</em> expressions mean, and 2) be able to present <em>their own thoughts</em> formally. Even when formalizing another person’s informal proof, the process is one of understanding what the person was saying and then writing down your understanding in formal language. Instead what’s presented is rules-of-thumb like “‘but’ means ‘and’”.</p>
<p>I could understand having an in-class discussion where the instructor presents natural language statements and asks students interactively to provide various interpretations of the statements. The ambiguity in natural language should quickly become apparent and this motivates the introduction of unambiguous formal notation. What makes no sense to me is giving students sheets of natural language statements about everyday situations that they need to “formalize” and grading them on this. Even if this was valuable, which I don’t think it is, the opportunity cost of spending time on this rather than some other topic would make it negatively valued to me.</p>
<p>What is fine is telling students how to <em>read</em> formal notation in natural language as long as it’s clear that these readings are not definitions. I have <a href="https://math.stackexchange.com/q/2596766/305738">seen people</a> claim that e.g. #A =&gt; B# is <em>defined</em> as “if #A#, then #B#”, but this is likely a misunderstanding on their part and not what their instructors or books said. On the other hand, it doesn’t seem too rare for a textbook to present a logical connective and then explicate it in terms of natural language examples shortly before or after providing a more formal definition (via rules or semantics). I don’t find it completely surprising that some students confuse these examples as definitions especially when the actual definitions, at times, don’t look terribly different from schematic phrases of natural languages.</p>
<p>I think natural language examples, and especially everyday as opposed to mathematical examples, should largely be avoided, or at the very least should be used with care.</p>
<h3 id="the-complete-conflation-of-negation-introduction-with-double-negation-elimination">The Complete Conflation of Negation Introduction with Double Negation Elimination</h3>
<p>Negation introduction is the following rule: if by assuming #P# we can produce a contradiction, then we can conclude #neg P#. You could present this as an axiom: <code class="asciimath">#(P =&gt; _|_) =&gt; neg P#</code>.</p>
<p>Double negation elimination is the following rule: if by assuming #neg P# we can produce a contradiction, then we can conclude #P#. You could present this as an axiom: <code class="asciimath">#(neg P =&gt; _|_) =&gt; P#</code> or #neg neg P =&gt; P#.</p>
<p>It seems pretty evident that many experienced mathematicians, let alone instructors and students, don’t see any difference between these. “Proof by contradiction” or “reductio ad absurdum” are often used to ambiguously refer to either of the above rules. This has <a href="http://math.andrej.com/2010/03/29/proof-of-negation-and-proof-by-contradiction/">been discussed</a> a <a href="https://existentialtype.wordpress.com/2017/03/04/a-proof-by-contradiction-is-not-a-proof-that-derives-a-contradiction/">few</a> <a href="https://blog.plover.com/math/IL-contradiction.html">times</a>. When even the likes of Timothy Gowers <a href="https://gowers.wordpress.com/2010/03/28/when-is-proof-by-contradiction-necessary/#comment-6984">does this</a>, I think it is safe to say many others do as well. Or just look at the comments to <a href="http://math.andrej.com/2010/03/29/proof-of-negation-and-proof-by-contradiction/">Andrej’s article</a> which I <em>highly</em> recommend.</p>
<p>As discussed in the referenced articles, and evidenced in the comments, there seems to be two things going on here. First, it seems that many people believe that dropping double negations, i.e. treating #neg neg P# and #P# is just a thing that you can do. They incorrectly reason as follows: substitute #neg P# into <code class="asciimath">#(P =&gt; _|_) =&gt; neg P#</code> to derive <code class="asciimath">#(neg P =&gt; _|_) =&gt; neg neg P#</code> which is the same as <code class="asciimath">#(neg P =&gt; _|_) =&gt; P#</code>. Of course, double negation elimination, the thing they are trying to show follows from negation introduction, is what lets them do that last step in the first place! It’s clear that negation introduction only adds (introduces, if you will) negations and can’t eliminate them. The second is that any informal proof of the form “Suppose #P# … hence a contradiction” is called a “proof by contradiction”.</p>
<p>As alluded to in the articles, people develop and state false beliefs about constructive logics because they are unaware of this distinction. This is an excellent area where exploring non-classical logics can be hugely clarifying. The fact that negation introduction is not only derivable but often <em>axiomatic</em> in constructive logics, while adding double negation elimination to a constructive logic makes it classical, very clearly illustrates that there is a <em>huge</em> difference between these rules. But forget about constructive logics, what’s happening here is many mathematicians are <em>deeply confused</em> about the key thing that makes classical logics classical. Further, students are being <em>taught</em> this confusion. Clearly there is <em>some</em> pedagogical failure occurring here.</p>
<h3 id="overuse-of-indirect-proof">Overuse of Indirect Proof</h3>
<p>A major cause of the above conflation of negation introduction and double negation elimination is the overuse and overemphasis of indirect proof. This causes more problems than just the aforementioned conflation. For example, it’s <a href="https://math.stackexchange.com/q/2665890/305738">relatively</a> <a href="https://math.stackexchange.com/q/1747050/305738">common</a> to see proofs by students that are of the form: “Assume #not P#. We can show that #P# holds via [proof of #P# without using the assumption #not P#]. Hence we have #P# and #not P# and thus a contradiction. Therefore #P#.” This is, of course, silly. The indirect proof by contradiction is just a wrapper around a <em>direct</em> proof of #P#. To make this completely clear, the logical structure mirrors the logical structure of the following argument: “Assume for contradiction #1+1 != 2#. Since by calculation #1+1=2# we have a contradiction. Thus #1+1=2#.”</p>
<p>It’s not always this egregious. Often the use indirect proof isn’t <em>completely</em> superfluous like the previous examples, but there is nevertheless a direct proof which is clearer, more informative, more general, and shorter. If these proofs are clearer and shorter, why don’t students find them? I see a variety of reasons for this. First, indirect proof is conceptually more complicated so more effort is spent explaining it, more examples using it are given, and its use is encouraged for practice. Second, the proof system provided is often intimately based on classical propositional logic (e.g. the “semantic tableaux” above or resolution) and often essentially <em>requires</em> the use of indirect proof to prove anything. For example, if we take <a href="https://en.wikipedia.org/wiki/Resolution_(logic)">resolution</a> as the primitive rule, i.e. if #P vv Q# and #not Q# then #P#, then doing a case analysis requires an indirect proof in general since clearly both #P# and #Q# could hold so we can only hypothetically assume #not Q#. (If you want a more formal argument, see <a href="https://math.stackexchange.com/q/2436136/305738">this question</a> and its answers.) Relying on double negation elimination to get negation introduction is another example. It’s common in less formal approaches to effectively take all classical propositional tautologies as axioms (which blurs the distinction between syntax and semantics). Many equivalences or definitions also hide uses of indirect proof (if we were to demonstrate them with respect to a constructive logic). For example, #P =&gt; Q -= not P vv Q#, #not not P -= P#, #exists x. P(x) -= not forall x.not P(x)#, and #P vv Q -= not (not P ^^ not Q)#. Third, often definitions and theorems are given in a “negative” way so that their negations or contrapositives are actually more useful. For example, defining “injective” as #f(x)=f(y) =&gt; x = y# is a little more convenient than #x != y =&gt; f(x) != f(y)# e.g. when proving an injective function has a post-inverse. Last but not least, it certainly can be harder, up to and including impossible, to prove a statement directly (i.e. constructively). That extra information doesn’t come from nowhere. As an example, there is a simple non-constructive proof that either #sqrt(2)^sqrt(2)# or <code class="asciimath">#(sqrt(2)^sqrt(2))^sqrt(2)#</code> is a rational number that’s of the form of an irrational number raised to an irrational power, but which it is is <a href="https://en.wikipedia.org/wiki/Gelfond%E2%80%93Schneider_theorem">much</a> harder to show.</p>
<p>What I’d recommend is for instructors and authors to strongly encourage direct/constructive proofs suggesting that a direct proof should be attempted before reaching for indirect proof. Perhaps even <em>require</em> direct/constructive proofs unless the exercise specifies otherwise. This, of course, requires a proof system that doesn’t force the use of indirect proofs. I’d recommend a natural deduction or sequent style presentation where each logical connective is specified in terms of rules that are independent of the other connectives. (I’m not saying that the formal sequent calculus notation needs to be used, just that there should be a rule, formal or informal, for each rule in the usual presentation of natural deduction, say.) As in the earlier section, I strongly recommend keeping syntax and semantics separate: the proof system should be able to stand on its own. Definitions and theorems should be given in forms that are constructively-friendly, which, admittedly, can take a good amount of care. To avoid students spending a lot of time looking for simple direct proofs that don’t exist, give hints where indirect proof is or is not required. While it can be helpful to motivate the significance of direct proofs and why certain choices were made, there’s no need to explicitly reference constructive logics for any of this.</p>
<h3 id="smaller-issues">Smaller Issues</h3>
<p>Finally, here are some minor issues that either seem less common or less problematic as well as some potential missed opportunities.</p>
<p>The first is a failure of abstraction I’ve occasionally seen, and I think certain books encourage. I’ve seen students who talk about the “symbols” of their logic as “including …, <code>'('</code>, and <code>')'</code>”. While certainly how to parse strings into syntax trees is a core topic of formal <em>languages</em>, it’s not that important for formal logic. The issues with parsing formulas I see have to do with conventions on omitting parentheses, not an inability to match them. There can be some benefit to driving home that syntax is just inert symbols, but the concept of a syntax tree seems more important and the process of producing a syntax tree from a string of symbols rather secondary. Working at the level of strings adds a lot of complexity and offers little insight into the <em>logical</em> concepts.</p>
<p>As a missed opportunity, there’s a <em>lot</em> of algorithmic content in formal logic. This should come as no surprise as the mechanization of reasoning has been a long time dream of mathematics and was an explicit goal during the birth of formal logic. Unfortunately, if the instructors and/or students don’t have a programming background it can be hard to really leverage this. Still, my impression is that the only indication of the mechanizability of much of logic that students get is in the mechanical drudgery of doing certain exercises. Even when it’s not possible to actually implement some of the relevant ideas, they can still be mentioned. For example, it is often taken as a requirement of a proof system that the proofs can be mechanically verified. That is, we can write a program that takes a formal proof and tells you whether it is a valid proof or not. This should be at least mentioned. Even better, for more than just this reason, it can be experienced by using proof assistants like Coq or MetaMath or simpler things like <a href="http://logitext.mit.edu/main">Logitext</a>. There are other areas of mechanization. Clearly, we can mechanize the process of checking truth tables for classical propositional logic, though there are interesting efficiency aspects to mention here. The completeness theorem of classical propositional logic is also constructively provable producing an algorithm that will take a formula with a truth table showing its validity and will produce a formal proof of that formula. As a little more open-ended topic, much of proof <em>search</em> is mechanical. I’ve often seen students stumped by problems whose proofs can be found in a completely goal-directed manner without any guessing at all.</p>
<p>Another issue is the use of Hilbert-style presentations of logics. It does seem that many logic texts thankfully use natural deduction or some other humane proof system, but Hilbert-style presentations aren’t uncommon. The <em>benefit</em> of Hilbert-style presentations of logics is that they can simplify meta-logical reasoning. But many introductions to logic do little to no meta-logical reasoning. The downsides of Hilbert-style presentations is that they don’t match informal reasoning well, they don’t provide much insight into the logical connectives, and proofs in Hilbert-style proof systems are just painful where even simple results are puzzles. Via the lens of Curry-Howard, Hilbert-style proof systems correspond to combinatory algebras, and writing programs in combinator systems (such as <a href="http://www.madore.org/~david/programs/unlambda/">unlambda</a>) is similarly unpleasant. The fact that introducing logic using a Hilbert-style proof system is formally analogous to introducing programming using an esoteric programming language designed to be obfuscatory says something…</p>]]></summary>
</entry>
<entry>
    <title>Djinn in your browser</title>
    <link href="https://derekelkins.github.io/posts/djinn.html" />
    <id>https://derekelkins.github.io/posts/djinn.html</id>
    <published>2017-04-09 05:35:18-07:00</published>
    <updated>2017-04-09T12:35:18Z</updated>
    <summary type="html"><![CDATA[<p>I couldn’t find an online version of Djinn, so I ran it through GHCJS with some tweaks, added a basic interface and hosted it <a href="/djinn/">here</a>. It runs in your browser, so go crazy. If you want changes to the default settings or environment, feel free to suggest them. Right now everything is the default settings of the Djinn tool except that multiple results is enabled.</p>]]></summary>
</entry>

</feed>
